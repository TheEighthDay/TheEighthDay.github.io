<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pytorch进阶学习]]></title>
    <url>%2F2019%2F09%2F18%2Flearn-pytorch%2F</url>
    <content type="text"><![CDATA[一 利用Variable自动求导1.1 Variable1.1.1 定义&emsp;&emsp;在pytorch中，我们需要能够构建计算图的 tensor，这就是 Variable数据结构。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身.data，对应 tensor 的梯度.grad以及这个 Variable 是通过什么方式得到的.grad_fn。 1.1.2 特性 requires_grad变量可以有梯度，求导。 volatile主要以用于inference过程中。若是某个过程，从 x 开始 都只需做预测，不需反传梯度的话，那么只需设置x.volatile=True ,那么 x 以后的运算过程的输出均为 volatile==True ,即 requires_grad==False。虽然inference 过程不必backward(),所以requires_grad 的值为False 或 True，对结果是没有影响的，但是对程序的运算效率有直接影响；所以使用volatile=True ,就不必把运算过程中所有参数都手动设一遍requires_grad = False 了，方便快捷。 detach 123456789y = A(x)temp=y.detach()z = B(temp)z.backward()def detach(self): result = NoGrad()(self) # this is needed, because it merges version counters result._grad_fn = None return result &emsp;&emsp;如果我们有两个网络 , 两个关系是这样的 现在我们想用 来为B网络的参数来求梯度，但是又不想求A网络参数的梯度。接着我们看一下detach的源码，将grad_fn设置为None，也就是说切断了变量temp与上一个网络反向传播的途径。不知道temp如何得到的。 retain_graph 123456789import torchx = torch.randn((1,4),dtype=torch.float32,requires_grad=True)y = x ** 2z = y * 4output1 = z.mean()output2 = z.sum()output1.backward() # 这个代码执行正常，但是执行完中间变量都free了，所以下一个出现了问题#正确的写法是 output1.backward(retain_graph=True) output2.backward() # 这时会引发错误 &emsp;&emsp;所以retain_graph主要用于处理，计算节点数值保存了，但是计算图x-y-z-out结构被释放了的情况。 create_graph官方的意思是对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。默认False，目前还没看到具体的应用，看到再补上。 1.2 自动求导过程1.2.1 输出为标量首先我们看一段代码：1234567891011121314151617import torch as t from torch.autograd import Variable as Va=V(t.Tensor([2,3]),requires_grad=True)b=a+3c=b*3out=c.mean()out.backward()print(&quot;a.data\n&quot;,a.data)print(&quot;a.grad\n&quot;,a.grad)print(&quot;a.grad_fn\n&quot;,a.grad_fn)print(&quot;b.data\n&quot;,b.data)print(&quot;b.grad\n&quot;,b.grad)print(&quot;b.grad_fn\n&quot;,b.grad_fn)print(&quot;out.data\n&quot;,out.data)print(&quot;out.grad\n&quot;,out.grad)print(&quot;out.grad_fn\n&quot;,out.grad_fn) 对应的函数是:$$out=\frac{3\left [ \left ( a_1 + 3 \right )+\left ( a_2 + 3 \right ) \right ]}{2}=\frac{3\left [ \mathbf{a}+3 \right ]}{2}$$所以\(\mathbf{a}\)向量的梯度为:$$\frac{\partial out}{\partial \mathbf{a}}=(\frac{3}{2},\frac{3}{2})$$真实的运行结果为：123456789101112131415161718a.data tensor([2., 3.])a.grad tensor([1.5000, 1.5000])a.grad_fn Noneb.data tensor([5., 6.])b.grad Noneb.grad_fn &lt;AddBackward0 object at 0x000002C22F441A20&gt;out.data tensor(16.5000)out.grad Noneout.grad_fn &lt;MeanBackward1 object at 0x000002C22F441160&gt; 我们得到以下结论： grad_fn表示变量通过怎样的计算方式得到，叶节点变量None grad为梯度，中间变量不存储grad，只有叶节点存储。 data为运行过程中变量的值 1.2.2 输出为向量1.2.2.1 雅可比矩阵的介绍&emsp;&emsp;Rn→Rm为一个从欧式n维空间转换到欧式m维空间的函数，并且由m个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn)。若将该函数的偏导数(若存在)组成一个m行n列的矩阵, 那么这个矩阵就是所谓的雅可比矩阵: 1.2.2.2 实例首先我们再看一段代码 12345678910111213141516171819import torch as t from torch.autograd import Variable as Va=V(t.Tensor([[2,4]]),requires_grad=True)b=t.zeros(1,2)b[0,0]=a[0,0]**2+a[0,1]b[0,1]=a[0,1]**3+a[0,0]out=2*bout.backward(t.Tensor([[1,1]]))#backward参数与out维度相同print(&quot;a.data\n&quot;,a.data)print(&quot;a.grad\n&quot;,a.grad)print(&quot;a.grad_fn\n&quot;,a.grad_fn)print(&quot;b.data\n&quot;,b.data)print(&quot;b.grad\n&quot;,b.grad)print(&quot;b.grad_fn\n&quot;,b.grad_fn)print(&quot;out.data\n&quot;,out.data)print(&quot;out.grad\n&quot;,out.grad)print(&quot;out.grad_fn\n&quot;,out.grad_fn) 对应的函数为：$$\mathbf{out}=2\mathbf{b}=2\left ( \left (a_1\right )^2+a_2,\left (a_2\right )^3+a_1 \right)$$我们求得\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵为：\begin{pmatrix}\frac{\partial out_1}{\partial a_1}=4a_1 &amp; \frac{\partial out_1}{\partial a_2}=2 \\\frac{\partial out_2}{\partial a_1}=2&amp; \frac{\partial out_2}{\partial a_2}=6{a_2}^2\end{pmatrix}也就是：\begin{pmatrix}8 &amp; 2\\ 2&amp;96\end{pmatrix}真实的运行结果为： 123456789101112131415161718a.data tensor([[2., 4.]])a.grad tensor([[10., 98.]])a.grad_fn Noneb.data tensor([[ 8., 66.]])b.grad Noneb.grad_fn &lt;CopySlices object at 0x0000020221827F60&gt;out.data tensor([[ 16., 132.]])out.grad Noneout.grad_fn &lt;MulBackward0 object at 0x0000020221827C50&gt; 我们得到以下结论： \(\mathbf{a}\)向量的梯度由\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵得出。也就是\( a.grad=(\frac{\partial out_1}{\partial a_1}+\frac{\partial out_2}{\partial a_1}=10 , \frac{\partial out_1}{\partial a_2}+\frac{\partial out_2}{\partial a_2}=98)\),那么这是为什么呢？与backward输入的[[1,1]]参数有何联系？接下来看下一节。 1.2.3 输出为矩阵：backward（）参数的意义1.2.3.1 矩阵求导的介绍&emsp;&emsp;基础是利用矩阵微分，复杂结合链式法则。矩阵微分和矩阵的迹有很大的关系，矩阵论中有详细的描述。矩阵微分的介绍:https:/www.cnblogs.compinardp/10791506.html矩阵求导常用公式：https://blog.csdn.net/WPR1991/article/details/82929843 1.2.3.2 实例首先看一段代码 123456789101112131415161718import torch as t from torch.autograd import Variable as Vx=t.Tensor([[100,200],[101,201]])w=V(t.Tensor([[0.1],[0.2]]),requires_grad=True)b=V(t.Tensor([[0.01],[0.02]]),requires_grad=True)y=t.Tensor([[1],[0]])out=(t.mm(x,w)+b)-yout.backward(t.Tensor([[1],[2]]))print(&quot;w.data\n&quot;,w.data)print(&quot;w.grad\n&quot;,w.grad)print(&quot;w.grad_fn\n&quot;,w.grad_fn)print(&quot;b.data\n&quot;,b.data)print(&quot;b.grad\n&quot;,b.grad)print(&quot;b.grad_fn\n&quot;,b.grad_fn)print(&quot;out.data\n&quot;,out.data)print(&quot;out.grad\n&quot;,out.grad)print(&quot;out.grad_fn\n&quot;,out.grad_fn) &emsp;&emsp;我们模拟一个超级简单的网络\(\mathbf{out} ={\left (\mathbf{X}\mathbf{W}+\mathbf{B}\right )}-\mathbf{Y} \),全部为矩阵。\( \mathbf{X} \)为我们输入的有2个属性的2组数据，\(\mathbf{Y}\)为2组数据的标签，所以这两个只需要Tensor封装，不需要Variable封装，\(\mathbf{out}\)为差损失。\(\mathbf{W}\)与\(\mathbf{B}\)为随机初始化的参数，需要计算梯度并通过梯度下降等方法变化，代码中没有写优化算法，我们只想看看一次反向传播后变量的梯度是多少。理论上可以得出：$$\frac{\partial \mathbf{out}}{\partial \mathbf{W}} = \mathbf{X} =\begin{pmatrix}100 &amp; 200\\101 &amp; 201\end{pmatrix}$$$$\frac{\partial \mathbf{out}}{\partial \mathbf{B}} = \mathbf{I} =\begin{pmatrix}1 &amp; 0\\0 &amp; 1\end{pmatrix}$$ 我们看一下运行结果 1234567891011121314151617181920212223w.data tensor([[0.1000], [0.2000]])w.grad tensor([[302.], [602.]])w.grad_fn Noneb.data tensor([[0.0100], [0.0200]])b.grad tensor([[1.], [2.]])b.grad_fn Noneout.data tensor([[49.0100], [50.3200]])out.grad Noneout.grad_fn &lt;SubBackward0 object at 0x000001E4B0857C18&gt; 可以看到\(\mathbf{W}\)与\(\mathbf{B}\)的梯度是理论值与backward参数运算的结果.\(\mathbf{W}\)的梯度为\begin{pmatrix}100+101\times 2 \\200+201\times 2\end{pmatrix}也就是\begin{pmatrix}302 \\602\end{pmatrix}\(\mathbf{B}\)和\(\mathbf{W}\)计算方式相同。我们可以得出以下的结论： backward的参数的实际意义是：当每个变量在反向传播计算梯度时候，不同的样本将给与不同的权重影响。这个权重就是backward的参数，当参数全部为1，表示所有的样本都一样，意味着拿到一个mini-batch的所有数据平均梯度。backward的参数与y的维度相同。 1.2.4 利用自动求导找函数的最小值原来的文章https://blog.csdn.net/weixin_42892943/article/details/94716387 首先我们看一段代码 12345678910111213141516import numpy as npimport torch as t from torch.autograd import Variable as Vdef fun(x): return (x[0]**2+x[1]-11)**2+(x[0]+x[1]**2-7)**2x = V(t.Tensor([0.,0.]),requires_grad=True)optimizer = t.optim.Adam([x],lr=1e-3)for step in range(20000): pred = fun(x) optimizer.zero_grad() pred.backward() optimizer.step() if step%2000 == 0: print(&apos;step&#123;&#125;: x = &#123;&#125;, x.grad = &#123;&#125;, f(x) = &#123;&#125;&apos;.format(step,x.toli(),x.grad,pred.item())) 我们构建了一个函数：$$z=(x^2+y-11)^2+(x+y^2-7)^2$$其图像为：我们利用adam算法优化器寻找函数的最小值。结果如下： 12345678910step0: x = [0.0009999999310821295, 0.0009999999310821295], x.grad = tensor([-14., -22.]), f(x) = 170.0step2000: x = [2.3331806659698486, 1.9540692567825317], x.grad = tensor([-35.3487, -13.8643]), f(x) = 13.730920791625977step4000: x = [2.9820079803466797, 2.0270984172821045], x.grad = tensor([-0.7803, 0.5791]), f(x) = 0.014858869835734367step6000: x = [2.999983549118042, 2.0000221729278564], x.grad = tensor([-0.0008, 0.0004]), f(x) = 1.1074007488787174e-08step8000: x = [2.9999938011169434, 2.0000083446502686], x.grad = tensor([-0.0003, 0.0002]), f(x) = 1.5572823031106964e-09step10000: x = [2.999997854232788, 2.000002861022949], x.grad = tensor([-9.5367e-05, 5.7221e-05]), f(x) = 1.8189894035458565e-10step12000: x = [2.9999992847442627, 2.0000009536743164], x.grad = tensor([-2.8610e-05, 1.7166e-05]), f(x) = 1.6370904631912708e-11step14000: x = [2.999999761581421, 2.000000238418579], x.grad = tensor([-9.5367e-06, 5.7220e-06]), f(x) = 1.8189894035458565e-12step16000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0step18000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0 我们可以看到x逐渐接近最低点，梯度在不断的减小。 二 数据集预处理的自定义构建2.1 transforms中的源码123456789101112class Normalize(object): def __init__(self, mean, std, inplace=False): self.mean = mean self.std = std self.inplace = inplace def __call__(self, tensor): return F.normalize(tensor, self.mean, self.std, self.inplace) def __repr__(self): return self.__class__.__name__ + &apos;(mean=&#123;0&#125;, std=&#123;1&#125;)&apos;.format(self.mean, self.std) 以Normalize为例子，我们看到每个处理方式由init，call，repr组成。 2.2 自定义自己的预处理方式&emsp;&emsp;torchvision.transforms的方法都是随机的，如果样本和标签都是图片，需要转动相同的随机角度。需要自己来定义预处理方式。这时候可以借用torchvision.transforms.functional来构造自己的函数，同时处理样本标签，得到新的样本标签后再分开处理。 12345678910111213141516171819202122232425import torchvision.transforms.functional as TFimport torchvision.transforms as T #自定义旋转角度：方法一，二class FixedRotation(object): def __init__(self, startangle,endangle): self.startangle = startangle self.endangle = endangle def __call__(self,img): return my_transforms1(img,startangle,endangle) # return my_transforms2(img,startangle,endangle) def __repr__(self): return self.__class__.__name__ +&apos;from&#123;&#125;to&#123;&#125;&apos;.format(startangle,endangle)def my_transforms1(img, startangle,endangle): angle = random.randint(startangle,endangle) image = img.rotate(angles[angle]) return imagedef my_transforms2(image): angle = random.randint(-30, 30) image = TF.rotate(image, angle) return image 三 神经网络的自定义构建3.1 详解nn.module&emsp;&emsp;这里先看一下nn.module类源码中重要的属性方法，这里放一段代码，等下下面会用到，我们建立了一个前传后的钩子函数，一个有纯参数组成的module，一个由submodule组成的module，并用这两个module组成个复杂的大module。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import torch as t from torch import nn from torch.nn import functional as Ffrom torch.autograd import Variable as Vdef for_hook(module, input, output): print(module) for val in input: print(&quot;input val:&quot;,val) for out_val in output: print(&quot;output val:&quot;, out_val)class Linear(nn.Module): # 继承nn.Module def __init__(self, in_features, out_features): super(Linear, self).__init__() # 等价于nn.Module.__init__(self) self.w = nn.Parameter(t.randn(in_features, out_features)) self.b = nn.Parameter(t.randn(out_features)) def forward(self, x): x = x.mm(self.w) # x.@(self.w) return x + self.b.expand_as(x)class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.subsubmodule=Linear(3,3) self.subsubmodule.register_forward_hook(for_hook) def forward(self,x): x = self.subsubmodule(x) x = F.relu(x) return xclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.register_buffer(&quot;buf1&quot;,t.ones(3,3)) #为module注册一个缓存区 self.register_parameter(&quot;param1&quot;,nn.Parameter(t.ones(3,3))) #与下一句等价，注册一个参数，本质就是一个变量 self.param2 = nn.Parameter(t.rand(3, 3)) self.add_module(&quot;submodel1&quot;,nn.Linear(3, 3)) #与下一句等价，注册一个子module self.submodel2 = nn.Linear(3, 3) self.submodel3 = MyLinear() self.modulelist1 = nn.ModuleList([nn.Linear(3,3),nn.Linear(3,3)]) self.modulelist2 = nn.Sequential(nn.Linear(3,3),nn.Linear(3,3)) self.bn = nn.BatchNorm1d(3) def forward(self, input): x = input.mm(self.param1) x = self.submodel1(x) x = self.submodel3(x) # x = self.modulelist1(x) error x = self.modulelist2(x) x = self.bn(x) return x net = Net()x=t.rand(2,3)y=net(x) 3.1.1 属性1234567891011class Module(object): def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True self._forward_pre_hooks = OrderedDict() self._state_dict_hooks = OrderedDict() self._load_state_dict_pre_hooks = OrderedDict() 解释一下重要的几个属性，我把它分为三类。 用于存储数据的： _parameters：字典，保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为’param’，value为对应parameter的item。不会查看到子module的参数。 _modules：子module，通过self.submodel = nn.Linear(3, 4)指定的子module会保存于此。同时子moudle可以自己定义。 _buffers：缓存，每个moudle都可以注册自己的缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 用于是否继续向前传播的: training：通过判断training值来决定正向传播策略。 钩子函数来实现对前传,后传,保存,回复等操作的触发,_backward_hooks,_forward_hooks,_forward_pre_hooks，_state_dict_hooks，_load_state_dict_pre_hooks这些字典主要用于存储钩子。 我们看几个属性：123456789101112131415161718192021222324252627282930net._buffers: OrderedDict([(&apos;buf1&apos;, tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]))])net._modules: OrderedDict([(&apos;submodule1&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule2&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule3&apos;, MyLinear( (subsubmodule): Linear())), (&apos;modulelist1&apos;, ModuleList( (0): Linear(in_features=3, out_features=3, bias=True) (1): Linear(in_features=3, out_features=3, bias=True))), (&apos;modulelist2&apos;, Sequential( (0): Linear(in_features=3, out_features=3, bias=True) (1): Linear(in_features=3, out_features=3, bias=True))), (&apos;bn&apos;, BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))])net._parameters: OrderedDict([(&apos;param1&apos;, Parameter containing:tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], requires_grad=True)), (&apos;param2&apos;, Parameter containing:tensor([[0.3535, 0.6803, 0.7144], [0.2985, 0.1329, 0.2111], [0.3999, 0.0395, 0.1407]], requires_grad=True))])net.training: Truenet.submodule3.subsubmodule._forward_hooks: OrderedDict([(0, &lt;function for_hook at 0x0000028C74F23E18&gt;)]) 3.1.2 方法根据属性我们来分开介绍一些重要的nn.moudle的重要方法。 3.1.2.1 缓存，子模型，参数的设置，操作，查看的方法 self.register_buffer（name[string],buf[Tensor]） self.register_parameter（name[string],param[nn.Parameter]）等价与self.name=param self.add_module（name[string],submodule[nn.Module]） 等价于self.name=submodule self.named_parameters() 生成器，产生所有参数的name与param self.named_buffers() 生成器，产生所有参数的name与buf self.named_module() 生成器，产生所有参数的name与module，包括module与子module self.named_children() 生成器，产生所有参数的name与submodule self.cuda() 转移到cuda上 self.cpu() 转移到cpu上 self.float() 参书变换 self.to() 多态性 12345.. function:: to(device=None, dtype=None, non_blocking=False).. function:: to(dtype, non_blocking=False).. function:: to(tensor, non_blocking=False) self.apply(fn) 源码 12345def apply(self, fn): for module in self.children(): module.apply(fn) fn(self) return self 我们看一些方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546net.named_parameters()所有参数名称:param1param2submodule1.weightsubmodule1.biassubmodule2.weightsubmodule2.biassubmodule3.subsubmodule.wsubmodule3.subsubmodule.bmodulelist1.0.weightmodulelist1.0.biasmodulelist1.1.weightmodulelist1.1.biasmodulelist2.0.weightmodulelist2.0.biasmodulelist2.1.weightmodulelist2.1.biasbn.weightbn.biasnet.named_modules()所有模型名称:submodule1submodule2submodule3submodule3.subsubmodulemodulelist1modulelist1.0modulelist1.1modulelist2modulelist2.0modulelist2.1bnnet.named_children()所有子模型名称:submodule1submodule2submodule3modulelist1modulelist2bnnet.named_buffers()所有缓存名称:buf1bn.running_meanbn.running_varbn.num_batches_tracked 3.2.1.2 传播的方法 self.train() #使用Dropout与BN层训练时开启 self.eval() #使用Dropout与BN层测试时开启 self.zero_grad() #清空所有参数的梯度 3.2.1.3 钩子的方法 self.register_forward_hook(hook) 设立该module前传后的钩子 self.register_backward_hook(hook) 设立该module后传后的钩子 self.register_forward_pre_hook(hook) 设立该module前传前的钩子 正向传播时候当钩子的函数监控的module发生正向传播，触发钩子函数： 12345Linear()input val: tensor([[ 0.3910, -0.9092, -0.9559], [ 0.4495, -0.9752, -1.0354]], grad_fn=&lt;AddmmBackward&gt;)output val: tensor([ 1.3830, 1.3145, -0.7691], grad_fn=&lt;SelectBackward&gt;)output val: tensor([ 1.5223, 1.4328, -0.7066], grad_fn=&lt;SelectBackward&gt;) 3.3 自定义Function实现网络层3.4 需要辨析注意的点3.4.1 nn.Parameter与Variable的关系3.4.2 nn.ModuleList与nn.Sequential()区别3.4.3 何时使用from torch.nn import functional as F中的F四 损失函数的构建4.1 xx源码五 多GPU应用与部署六 参考]]></content>
      <categories>
        <category>pytorch进阶学习</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本操作二：写作技巧]]></title>
    <url>%2F2019%2F08%2F24%2Fsecond2%2F</url>
    <content type="text"><![CDATA[写作###有关符号编译出错123&#123; -&gt; &amp;#123; — 大括号左边部分Left curly brace&#125; -&gt; &amp;#125; — 大括号右边部分Right curly brace空格 -&gt; &amp;ensp; ###有关图片在blog（hexo）目录下Git Bash Here，运行hexo n “博客名”来生成md博客时，会在_post目录下看到一个与博客同名的文件夹。按照如下格式则可以插入图片1![你想要输入的替代文字](second2/test.jpg) 测试效果图： ###有关音乐在网易云音乐中生成外链音乐播放器。如下1&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&amp;id=541326593&amp;auto=1&amp;height=66"&gt;&lt;/iframe&gt; ###有关视频在优酷中生成外链视频播放器。如下1&lt;iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=i0031n20390" allowFullScreen="true"&gt;&lt;/iframe&gt;]]></content>
      <categories>
        <category>hexo博客操作</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本操作一：部署命令]]></title>
    <url>%2F2018%2F12%2F02%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files and compress12$ hexo generate$ gulp More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo博客操作</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
