<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PRML：概率分布]]></title>
    <url>%2F2019%2F11%2F25%2FProbabilityDistributions%2F</url>
    <content type="text"><![CDATA[1 共轭的概念共轭先验（conjugate prior）：如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。所有指数家族的分布都有共轭先验。 二项分布作为似然函数，共轭先验为beta分布。 多项式分布作为似然函数，共轭先验为狄利克雷分布。 高斯分布作为似然函数，共轭先验为高斯分布。 一维情况下：如果 \( \mu \)未知，\( \Sigma^2 \)已知，那么先验概率（共轭先验）为一个高斯分布，后验概率也是高斯分布。 一维情况下：如果 \( \mu \)已知，\( \Sigma^2 \)未知，那么先验概率（共轭先验）为一个Gamma分布，后验概率也是Gamma分布。 一维情况下：如果 \( \mu \)未知，\( \Sigma^2 \)未知，那么先验概率（共轭先验）为一个Gaussian-Gamma分布，后验概率也是Gaussian-Gamma分布。 高维情况下：如果 \( \mu \)未知，\( \Lambda^-1 \)已知，那么先验概率（共轭先验）为一个高斯分布，后验概率也是高斯分布。 高维情况下：如果 \( \mu \)已知，\( \Lambda^-1 \)未知，那么先验概率（共轭先验）为一个Wishart分布，后验概率也是Wishart分布。 高维情况下：如果 \( \mu \)未知，\( \Lambda^-1 \)未知，那么先验概率（共轭先验）为一个Gaussian-Wishart分布，后验概率也是Gaussian-Wishart分布。 2 高斯分布（重要）高斯分布的贝叶斯推断主要要讲了，当似然函数为一个高斯分布。\( \Sigma \)的逆矩阵\( \Lambda \)称为精度。 2.1 马氏距离\( ({\bf x- \bf y})^T{\Sigma}^{-1}({\bf x-\bf y}]) \)称为马氏距离的平方，其中\( \Sigma \)是两个随机变量向量的协方差矩阵，\( \sigma_ij \)为\( E[(x_i-E[x_i])(y_j-E[y_j])] \)。协方差矩阵非负定。 2.2 高斯分布的几何意义如果把一个高斯分布分为两部分，指数部分和非指数部分。非指数部分用于描述概率密度衰减的速度（在二维情况下图中蓝线的下降速度），而指数部分描述了大多数样本点在一个超椭球面（在二维情况下图中红色的椭圆）的内部。最开始的一段先描述了马氏距离概念，也就是对高斯分布的指数部分。那么这个超椭球面如何得出的？先看一下高斯分布的公式$$p({\bf x})=\frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^\frac{1}{2}}exp{[-\frac{1}{2}({\bf x-\mu})^T{\Sigma}^{-1}({\bf x-\mu}])}$$ 协方差矩阵公式$$\Sigma=E[(\bf x-\bf \mu)(\bf x - \mu)^T]$$ 很明显，协方差公式是实对称矩阵，利用特征分解相似对角化，并施密特正交化可得$${\bf\Sigma}^{-1}={\bf U^{-T}\Lambda^{-1}U^{-1}}={\bf U\Lambda^{-1}U^T}=\sum_{i=1}^d\frac{1}{\lambda_i}{\bf u}_i{\bf u}_i^T$$ 因此高斯分布指数部分的公式可以写成$$({\bf x-\mu})^T{\Sigma}^{-1}({\bf x-\mu})=({\bf x-\mu})^T\left(\sum_{i=1}^d\frac{1}{\lambda_i}{\bf u}_i{\bf u}_i^T\right)({\bf x-\mu})$$ 也就是$$\sum_{i=1}^d\frac{1}{\lambda_i}({\bf x-\mu})^T{\bf u}_i{\bf u}_i^T({\bf x-\mu})$$ 我们做一个替换$$y_i=（{\bf x-\mu})^T{\bf u}_i$$ 原来的式子就变成了$$\sum_{i=1}^d \frac{y_i^2}{\lambda_i}$$ 这个形式其实已经相当明朗了，令式子为1，这是一个超椭球面，椭球面的中心为原点，而轴长为 \( \sqrt{\lambda_i} \)这是y形式下的，那么x形式下就是x在\( \bf \mu \)为原点，\( {\bf u}_i \)方向投影的轴长为\( \sqrt{\lambda_i} \)。那么高斯分布的概率就是超椭球体的的每一点的质量。 在二维情况下，绝大多数样本在红色的椭圆内，也就是超椭球面，而对应的两个\( \sqrt{\lambda_i} \)特征值就是轴的长度。在一维情况下，绝大多数样本在\( \mu - \Sigma \) 到\( \mu + \Sigma \)，这时候的超椭球面退化成了一条直线，\( \Sigma \)也就是轴的长度。多维高斯分布可以转化为一元高斯分布的乘积形式。 2.3 高斯分布的缺点高斯分布参数多，单峰的缺点。 2.4 条件高斯分布首先引入精度矩阵（precision matrix）的概念,它是协方差举证的逆。这一节主要推导，如果两节变量是联合高斯分布，那么以一组变量为条件，另外一组变量同样是高斯分布。对于联合概率密度p(Xa,Xb),只需要将Xb确定，那么便得到条件概率密度p（Xa|Xb），对比高斯分布指数分布的一般形式，我们可以得到条件概率密度p（Xa|Xb）的\( \mu \)，\( \Sigma \)。推导过程主要是分块矩阵变换，没有进行推导，有需要再来吧。 2.5 边缘高斯分布因为积分只对指数部分有用，我们单独拿出指数部分来做分析，将指数部分编程变量为Xb的一般形式，然后求积分，对比标准的高斯分布，可以得出边缘概率密度也是高斯分布，且可以的得到边缘概率密度的\( \mu \)，\( \Sigma \)。 2.6 高斯分布的极大似然估计因为数据集的每个数据变量符合高斯分布且独立，所以这些变量的概率密度函数连乘的形式可以写成联合概率密度形式，也就是高维度的高斯分布，接着取log，求导=0，寻找令联合概率密度极大值的 \( \mu \)，\( \Sigma \)。对\( \mu \)的估计是无偏的估计，对\( \Sigma \)的估计是有偏的估计 3 其他分布3.1 与高斯分布没联系的分布3.1.1 伯努利分布与二项分布伯努利分布又称二点分布或0-1分布，即一次试验只有正例和反例两种可能，以随机变量表示就是y只能取0或1，伯努利试验是只有两种可能结果的单次随机试验，假设一次试验出现正例的概率为p(0&lt;p&lt;1)，那么\( P(Y=1) = p\)，\( P(Y=0) = 1-p\)，可以统一表达为\( P(Y=k) = p^k(1-p)^{1-k},k=0,1\)，则称Y服从参数为p的伯努利分布，记为Y∼Ber(p)。参数只有1个 记每次伯努利试验正例发生的概率为p，总共试验次数为n，随机变量Y表示出现正例的次数，则记Y∼Bin(n,p)表示Y服从参数为(n,p)的二项分布，观测变量y∈[0,n]，y取k的概率，即在n次伯努利试验中，正例出现k次的概率为 \( P(x=k)=C_n^k p^k (1-p)^{n-k} \)。参数有2个 3.1.2 范畴分布与多项式分布范畴分布是伯努利分布的拓展。在单次试验中，假设一共有k种可能情况，记这k种可能发生的概率为$$\mu=[\mu_1,\dots, \mu_k]$$ 并且$$\sum_{i=1}^k \mu_i =1$$ $$ \sum_{i=1}^k y_i = 1$$ 其中$$y_i \in \{0,1\}$$ $$\sum_{i=1}^k y_i = 1$$ yi 中只有一个为1，其他均为0，也就是每次试验只有一种可能发生。yi取1的概率为ui。$$P(\textbf{y}|\mu) = \prod_{i=1}^k \mu_i^{y_i}$$ 范畴分布参数为一共有k个 多项分布是对二项分布的扩展，二项分布是单变量分布，而多项分布是多变量分布。将范畴分布的实验进行n次，记第i种可能发生的次数为mi$$\sum_{i=1}^{k} m_i = N$$ 那么多项分布表示的联合概率分布参数有k+1个。 3.1.3 beta分布与狄利克雷分布beta分布常用于二项分布的共轭先验。狄利克雷分布常用于多项式分布的共轭先验。gamma函数主要用于归一化，因为有指数项，而这两个分布概念也主要是用于共轭先验才被提出。 3.2 与高斯分布有联系的分布目前认为这些分布更多是因为计算的需要产生，实际具体联系和意义还没有研究。 3.2.1 拉普拉斯分布比高斯分布有更尖的头和更厚的尾。$$p(x) = \frac{1}{2 \lambda} e^{-\frac{\vert x –\mu \vert}{\lambda}}$$ 3.2.2 高斯混合分布高斯混合分布多了参数混合系数，极大似然估计无法直接得出解析解，可以使用数值迭代优化。 3.2.3 学生t分布比高斯分布有更厚的尾巴，具有更好的“鲁棒性”。可以看作无数的同均值不同方差的高斯混合分布。 3.2.4 Gamma分布一维情况下：如果 \( \mu \)已知，\( \Sigma^2 \)未知，那么先验概率（共轭先验）为一个Gamma分布，后验概率也是Gamma分布。 3.2.5 Gaussian-Gamma分布一维情况下：如果 \( \mu \)未知，\( \Sigma^2 \)未知，那么先验概率（共轭先验）为一个Gaussian-Gamma分布，后验概率也是Gaussian-Gamma分布。 3.2.6 Wishart分布高维情况下：如果 \( \mu \)已知，\( \Lambda^-1 \)未知，那么先验概率（共轭先验）为一个Wishart分布，后验概率也是Wishart分布。 3.2.7 Gaussian-Wishart分布高维情况下：如果 \( \mu \)未知，\( \Lambda^-1 \)未知，那么先验概率（共轭先验）为一个Gaussian-Wishart分布，后验概率也是Gaussian-Wishart分布。 4 样本集的密度函数估计方法给定一个样本集，怎么得到该样本集的密度函数，解决这一问题有两个方法： 4.1 参数估计方法简单来讲，即假定样本集符合某一概率分布，然后根据样本集拟合该分布中的参数，例如：似然估计，混合高斯等，由于参数估计方法中需要加入主观的先验知识。 4.2 非参数化的估计方法和参数估计不同，非参数估计并不加入任何先验知识，而是根据数据本身的特点、性质来拟合分布。注意，公式的成立依赖于两个相互矛盾的假设，即区域R要足够小，使得这个区域内 的概率密度近似为常数，但是也要足够大，使得落在这个区域内的数据点的数量K能够足够让 二项分布达到尖峰。我们有两种方式利用上式的结果。我们可以固定K然后从数据中确定V的值，这就是K近邻方法。我们还可以固定V然后从数据中确定K，这就是核方法。在极限N →∞的情况 下，如果V随着N而合适地收缩，并且K随着N增大，那么可以证明K近邻概率密度估计和核方法概率密度估计都会收敛到真实的概率密度。 4.2.1 核概率密度估计方法（parzen窗方法）首先采用矩形核这里使用了D维边长为h的立方体的体积公式V = h^D。如果我们选 择一个平滑的核函数，那么我们就可以得到一个更加光滑的模型：依旧是就算在直径为h的立方体中含有多少数据。不过计算多少不再是被立方体边缘阻断的叠加，而是平滑的概率叠加。参数h对平滑参数起着重要的作用。小的h会造成模型对噪声 过于敏感，而大的h会造成过度平滑，因此要进⾏一个折中。固定V然后从数据中确定K。 4.2.2 近邻方法 参考文献 共轭先验的理解：https://baike.baidu.com/item/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83/15696678?fr=aladdin 极大似然估计与贝叶斯估计的理解(对于先验，后验，似然函数的认识)：https://blog.csdn.net/liu1194397014/article/details/52766760 高斯分布理解1 https://www.zhihu.com/question/36339816/answer/67043318 高斯分布理解2 https://www.cnblogs.com/shouhuxianjian/p/9773121.html 核密度估计 https://www.jianshu.com/p/249e5ff97c04，https://blog.csdn.net/sunchao3555/article/details/84843801,https://blog.csdn.net/unixtch/article/details/78556499 gamma分布 https://blog.csdn.net/lynn0085/article/details/79338611 beta分布与狄利克雷分布 https://blog.csdn.net/jteng/article/details/60334628 拉普拉斯分布 https://www.cnblogs.com/jiaxin359/p/8872986.html]]></content>
      <categories>
        <category>PRML</category>
      </categories>
      <tags>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML：线性回归模型]]></title>
    <url>%2F2019%2F11%2F21%2FLinearModelsForRegression%2F</url>
    <content type="text"><![CDATA[1 回归问题的定义数据定义\( Data=(X,\boldsymbol{t}),X=[\boldsymbol{x_1}…\boldsymbol{x_n}]^T,\boldsymbol{x_1}\epsilon R^p,\boldsymbol{t}=[t_1…t_n],t_1 \epsilon R \)需要解决的问题\( y=\boldsymbol{w}^T\boldsymbol{x},t=y+\varepsilon ,\varepsilon \sim \mathit{N}(0,\sigma^2) \) 对于一个新的数据 \( x^{* } \),我们想要得到 \( t^{ * } \)。 2 频率派思想2.1 损失期望=偏置^2+方差+噪音首先我们知道一个损失函数的期望为如下形式：我们采用平方差损失： 我们定义h(x)为真实估计函数，y(x)为拟合函数，t为观测值。我们可以将损失函数化作如下形式：考虑对于多个数据集的情况，将第一项继续拆分可得综上可得我们可以得到如下结论： 偏置刻画的是构建的模型和真实模型之间的差异，学习算法本身的拟合能力。 方差刻画的是构建的模型自身的稳定性，多个训练集的变动所导致的学习性能的变化。 噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。 泛化能力由这三个共同组成。 损失期望的偏差方差分解需要多个数据集。 2.2 频率派求解模型这里本质上就是寻找w的值。 2.2.1 最小二乘估计 LSE求解w\( \boldsymbol{w^{*}}=argmin \quad (\boldsymbol{t}-X\boldsymbol{w})^T(\boldsymbol{t}-X\boldsymbol{w})=(X^TX)^{-1}X^T\boldsymbol{t} \)模型预测\( t^{* }=(\boldsymbol{x}^{* })^T\boldsymbol{w^{*}} \) 2.2.2 极大似然估计 MLE求解w和w有关的项与极小二乘法的损失函数一样。\( \boldsymbol{w^{*}}=argmin \quad (\boldsymbol{t}-X\boldsymbol{w})^T(\boldsymbol{t}-X\boldsymbol{w})=(X^TX)^{-1}X^T\boldsymbol{t} \)同时可以得出模型预测\( t^{* }=(\boldsymbol{x}^{* })^T\boldsymbol{w^{*}} \) 2.2.3 对于解析解的几何解释也就是t乘以投影矩阵等于y。\( (X(X^TX)^{-1}X^T)(\boldsymbol{t}) = (X\boldsymbol{w}) \) 2.2.4 带有正则项的LSE加入正则项以后变成了如下方式，q=1为lasso回归，q=2为岭回归，分别意味着增加L1正则化和L2正则化，或者说增加L1范数，L2范数，正则化系数人为设定。 正则项的作用：L1也可以防止过拟合，主要作用是用于构建稀疏权值矩阵，用于特征选择。L2正则可以防止奇异矩阵，也就是防止求伪逆时候的矩阵不可逆，防止过拟合。L2没有稀疏的作用是因为没有棱角。具体参考文献。 3 贝叶斯派的思想3.1 参数后验为点估计（不完全的贝叶斯派）3.1.1 模型推断就是MAP最大后验法（附加高斯噪音），这时候我们对于参数的后验概率只用一个值来代替。当参数先验服从高斯分布，参数的后验概率等同于最小二乘法+L2正则项的结果，当参数先验服从Lapalce分布，参数的后验概率等同于最小二乘法+L1正则项的结果。我们观察，当参数先验服从高斯分布。利用极大似然法求解下面式子可以得到参数后验的点估计\( \boldsymbol{w^{*}} = argmax \quad p(\boldsymbol{w}|\boldsymbol{t},X) \)\( \boldsymbol{w^{*}} = argmax \quad p(\boldsymbol{t}|\boldsymbol{w},X)p(\boldsymbol{w}) \)其中有\( \boldsymbol{t}|\boldsymbol{w},X \sim N(X\boldsymbol{w},\Sigma_1 ) \)等价于:\( \boldsymbol{w} \sim N(\boldsymbol{\mu},\Sigma_2)\)一般设置为：\( p(\boldsymbol{w}) \sim N(0,\alpha^{-1} I) \)极大似然函数为\( \boldsymbol{w^{*}} = argmax \quad \prod_i^{100} (N(\boldsymbol{w}^T x_i,\beta^{-1}) N(0,\alpha^{-1}))^{t_i} \) 所以有\( \boldsymbol{w^{*}} = (\alpha^{-1} I + X^TX)^{-1} X^T \boldsymbol{t} \) 3.1.2 模型预测\( t^{* } \sim N({\boldsymbol{x}^{* }}^T\boldsymbol{w^{*}},\beta^{-1})\) 3.2 参数后验完全是概率分布3.2.1 模型推导这次我们要寻找到真正的\( p(\boldsymbol{w}|\boldsymbol{t},X) \)分布，忽略分母积分常量\( p(\boldsymbol{w}|\boldsymbol{t},X) \propto p(\boldsymbol{t}|\boldsymbol{w},X)p(\boldsymbol{w})\)。其中有 \( p(\boldsymbol{t}|\boldsymbol{w},X) \)为： 其中有 \( p(\boldsymbol{w}) \) 为：\( p(\boldsymbol{w}) \sim N(0,\alpha^{-1} I) \)经过计算可得于是我们可以写出后验：\( \boldsymbol{w}|\boldsymbol{t},X \sim N(\boldsymbol{m_N},S_N) \) 3.2.2 模型预测\( p(t|\boldsymbol{t},X,\boldsymbol{x^{*}})=\int p(t|\boldsymbol{x^{*}},\boldsymbol{w},\beta)p(\boldsymbol{w}|\boldsymbol{t},X,\alpha,\beta) d\boldsymbol{w} \)也就是\( p(t|\boldsymbol{t},X,\boldsymbol{x^{*}}) = N(\boldsymbol{m_N}^T\boldsymbol{x^{*}},\beta^{-1}+{\boldsymbol{x^{*}}}^TS_N\boldsymbol{x^{*}}) \)也就是\( t^{* } \sim N(\boldsymbol{m_N}^T\boldsymbol{x^{*}},\beta^{-1}+{\boldsymbol{x^{*}}}^TS_N\boldsymbol{x^{*}}) \) 参考文献正交投影：https://www.cnblogs.com/mfrbuaa/p/5319365.htmlL1与L2正则化的区别：https://blog.csdn.net/TXBSW/article/details/79073933]]></content>
      <categories>
        <category>PRML</category>
      </categories>
      <tags>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML：线性分类模型]]></title>
    <url>%2F2019%2F11%2F18%2FLinearModelsForClassfication%2F</url>
    <content type="text"><![CDATA[1 概述本章主要从判别函数（硬分类）与概率模型（软分类）两个角度解决二分类与多分类的问题。 2 判别函数（硬分类）简单来说，硬分类就是直接寻找决策平面，二分类下判别函数就是I(f(x))，f(x)&gt;0属于正类，f(x)&lt;0属于负类，x·-&gt;{-1，1}。多分类就是寻找多个分类相互的决策平面。 2.1 二分类与多分类的决策平面性质2.1.1 二分类我们需要记得如下图此时一个D-1维的超平面经过D维的空间。我们将w0整合到w,可以得到一个D维度的超平面，经过D+1维度的空间的原点。 2.1.2 多分类 2.1 最小平方法（学习判别函数参数【寻找决策平面】的方法一）这个方法只适合简单的线性函数，如果线性函数上加了激活函数则不适用了。二分类情况下y为单变量，求最小二乘法的时候可以求得y的解析值x的伪逆乘以t。我们考虑多分类的情况。多分类时候，我们使用onehot编码，也可以得到精确的解析解。 2.2 Fisher线性判别分析法（学习判别函数参数【寻找决策平面】的方法二）Fisher线性判别函数是从降维的角度出发，同时这个方法只适合简单的线性函数。思想是最大化一个函数，能够让类均值的在低维度空间投影分开较大，同时让每个类内部的方差较小，从而减少类别的重叠。 有\( \boldsymbol{m}_k = \frac{1}{N_k} \sum \boldsymbol{x_n} \) \( \boldsymbol{m}_k \) 是第k类的均值。那么有\( m_k \)是投影值。 $$m_k=\boldsymbol{w}^T\boldsymbol{m}_k$$ 类内方差有如果是二分类的话这样我们可以构建最大化函数J(w)推广到多分类略 2.3 感知器算法（学习判别函数参数【寻找决策平面】的方法三）相比于前面两种方法，感知器算法求解判别函数的参数，从容错的角度出发，利用了一个激活函数。如下文： 3 概率模型这里将从概率的角度看分类问题。这里便是软分类，我们不直接寻找决策平面，而是对比概率值，二分类只需要看y=1的概率值，x·-&gt;（0，1）。多分类的话需要对比多个分类概率值，最大的就是我们认为的分类。这里会有两种模型概率生成式模型与概率判别式模型。 3.1 概率生成式模型VS概率判别式模型。在概率生成式模型中，我们先对类条件概率密度p(x|Ck)和类先验概率分布p(Ck)建模，后使用这两个概率密度通过贝叶斯定理计算后验概率p（Ck|x），典型的案例就是logistics回归。而概率判别式模型，我们直接对于P(Ck|x)建模，典型的模型就是朴素贝叶斯模型和高斯判别模型。 3.2 概率生成式模型概率生成模型中我们先对类条件概率密度p(x|Ck)和类先验概率分布p(Ck)建模，后使用这两个概率密度（也就是联合概率密度）通过贝叶斯定理计算后验概率p（Ck|x）。 3.2.1 sigmoid与softmax函数这里从概率生成模型的角度引出了sigmoid与softmax函数，本身这两个函数是概率生成模型转化后的函数形式，解释了logistics模型中参数，体现了概率生成模型更强的解释性。 sigmoid函数如下：其中a也等价与如下形式，a被称为logit函数，表示两类概率的比值的ln值：softmax： 3.2.2 高斯判别模型（Gaussian discriminant classifier）不要被名字迷惑，这是个概率生成模型 3.2.2.1 模型定义假设我们的类条件概率p（x|Ck）为高斯分布，且每一类的协方差矩阵相同。在二分类中类先验概率为p(Ck)服从伯努利分布，多分类中先验服从mutionoulli（catetorical）范畴分布。来看二分类的sigmoid与多分类的softmax。类条件概率密度分布如下：在二分类sigmoid中，a为 \( \boldsymbol{w}^T\boldsymbol{x}+w_0\)即 拓展到多分类的softmax。 由于我们假设类条件概率密度分布都具有相同的协方差矩阵，所以决策边界是线性的，当具有不同的协方差，边界是二次的。而先验只是决定决策边界的移动。 3.2.2.2 最大似然解一旦我们具体化了类条件概率密度和类先验概率密度的形式，利用极大似然法就可以求解类先验概率和类条件概率的参数。书上举了低维的高斯分布与伯努利分布作为类条件概率密度和先验概率密度。 3.2.3 朴素贝叶斯模型（Naive Bayes classifier）一般来说朴素贝叶斯分类器是离散的，广意的朴素贝叶斯就是连续的高斯判别模型和离散的朴素贝叶斯结合。 我们认为特征向量\( \boldsymbol{x} \)中每一个特征值 \( x_i \)都是独立的，这就是朴素贝叶斯假设。根据条件概率p(X=x|Y=c_k)的假设分布不同,分为伯努利假设是伯努利分布(其实应该是多变量伯努利分布),多项式假设是多项式分布,而高斯也就是假设是高斯分布(其实是多变量高斯分布)。 首先我们观察伯努利假设\( \boldsymbol{x} = [x_1,x_2 … x_d] \) 中\( x_i\epsilon{0，1} \)，\( x_i \sim Ber(\mu)\)。这里可以理解为以一篇文章中所有单词有和无作为特征，\( \boldsymbol{x} \)特征向量长度为文章中词汇的数量d。最终形式：\( p(C_k|\textbf{x}) = \frac{p(x_i,x_2\dots x_d|C_k)p(C_k)}{p(\boldsymbol{x})} = \frac{[\prod_{i = 1}^{d}{\mu_{ki}}^{x_i}{(1-{\mu_{ki}})}^{1-x_i}]p(C_k)}{p(\boldsymbol{x})} \) 接着我们观察多项式假设，，\( \boldsymbol{x}=[x_i,x_2 \dots x_d] \),且\( \boldsymbol{x} \sim Multi(n,\boldsymbol{\mu}) \),其中\( \mu = [\mu_1, …, \mu_d] \)。可以理解为，一篇文章中所有单词有n个，每个单词可能出现0-d种词汇（词典大小为d）。所以\( x_i,x_2 \dots x_d \)代表着大小为d的词典中每个词出现的频率。最终形式：\(p(C_k|\textbf{x})=\frac{Multi(n,\boldsymbol{\mu})p(C_k)}{p(\boldsymbol{x})}=\frac{[\frac{n!}{x_1!x_2! \dots x_d!}\prod_{i=1}^d {\mu_{ki}}^{x_i}]p(C_k)}{p(\boldsymbol{x})}\) 最后的高斯假设就是高斯判别分析。 一般情况在朴素贝叶斯中还会加入拉普拉斯平滑防止出现错误，就是为每个类计算时候+1。 3.2.4 指数族分布下的概率生成模型我们把类条件概率密度分布视作指数族分布形式，可以推导出二分类中，a和多分类中ak更加一般的形式。 3.3 概率判别式模型3.3.1 固定基函数基函数实现原空间非线性的分割到基函数空间的线性分割。 3.3.2 logistic回归（典型的频率派思想）3.3.2.1 logistic回归定义这里对于sigmoid函数，我们构建时候不再从生成式模型那样从类条件概率和类先验概率的角度出发。而是直接用\( \boldsymbol{w}^T\phi \)，这样的基函数。即需要求解的极大似然函数，或者说损失函数为： 3.2.2.2 求解参数这里涉及到求解w，这里和最小二乘法的求解区别是要是多了sigmoid函数在基函数上，但是不再有解析解了，可以利用梯度下降法和牛顿法。 梯度下降法的梯度为： 牛顿法的介绍以及应用于最小二乘法： 牛顿法应用于求解logistic回归的损失函数： 但是极大似然法会导致过拟合严重，如果使用优化算法如梯度下降法和牛顿法，无法判别哪个解更加好。通过加入参数先验把极大似然法变成最大后验法（等价于为损失函数加上正则项）来减少过拟合。 3.3.3 多分类的logistic回归多分类使用softmax函数。在生成式模型中我们通过极大似然估计法求解类条件概率和类先验概率的参数，隐形确定了模型的参数。而这里我们直接对模型的参数使用极大似然法。下一步则是取log得出损失函数然后利用优化算法求解参数。 3.3.4 probit回归和logistics回归差不多的一个回归，他的激活函数（逆probit回归函数）形式为\( f(a)=\int_{-\infty }^{a}\mathcal N(\theta|0,1)d\theta \) 3.3.5 标准链接函数标准链结函数这一章先介绍了一个概念：链结函数是激活函数的反函数。分析t有高斯噪音模型到对于t属于指数分布族情况的推广，从而得到更加一般的形式。 4 拉普拉斯近似4.1 方法介绍由于后验概率分布的计算依赖于先验概率分布函数、似然概率分布函数，当这二者共轭时，后验概率与先验概率服从相同的分布函数，从而可以推导计算出后验概率分布(posterior could be computed analytically)。但是，当这二者不共轭时，则是计算后验概率分布的近似值。计算近似值一共有三种方法: 点估计法(point estimate — MAP) 拉普拉斯近似法 Metropolis-Hastings采样法。 点估计法(point estimate — maximum a posteriori)其实就是最大后验法，而拉普拉斯近似法这里将介绍，采样法后面会介绍。点估计法是最简单的近似，简单来说点估计是：\( p(\boldsymbol{w}|\boldsymbol{t},\boldsymbol{X}) \approx max( p(\boldsymbol{t}|\boldsymbol{X},\boldsymbol{w})p(\boldsymbol{w}|\theta ) )=max( \prod_{n=1}^N p(t_n|\boldsymbol{x}_n, \boldsymbol{w}) p(\boldsymbol{w}|\boldsymbol{\theta}) ) \) 在精准计算参数后验概率时候，需要对先验和似然函数乘积归一化（就是贝叶斯公式里的分母，需要求积分），但是积分无法求出，贝叶斯推导无法进行，则需要拉普拉斯近似是：\( p(\boldsymbol{w}|\boldsymbol{t},\boldsymbol{X}) \approx N(\boldsymbol{u},\boldsymbol{A^{-1}} ) \) 那么拉普拉斯是怎么近似的呢？首先我们有数据集，明确先验和似然函数，我们把后验概率写成如下形式 \( p(\boldsymbol{w}|\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})=\frac{p(\boldsymbol{t}|\boldsymbol{w},\boldsymbol{X}) p(\boldsymbol{w}|\boldsymbol{\theta})} { p(\boldsymbol{t})} \) 把该式子先log，可以看到log后的极大似然函数的展开形式如下：\( \ln p(\boldsymbol{w}|\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})=\ln p(\boldsymbol{t}|\boldsymbol{w},\boldsymbol{X})+\ln p(\boldsymbol{w}|\boldsymbol{\theta})-\ln p(\boldsymbol{t}) \) 将极大似然函数在一个局部最大值（对于\( \boldsymbol{w} \)一阶求导等于0，二阶小于0或者负定，通常可以选择最大后验法中的\( \boldsymbol{\hat{w}}\)，也就是\（ w_{MAP}\)）处泰勒展开可以得到：$$\ln p(\boldsymbol{w};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) \approx \ln p(\boldsymbol{\hat{w}};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})+0-\frac{1}{2}(\boldsymbol{w}-\boldsymbol{\hat{w}})^T\boldsymbol{A}(\boldsymbol{w}-\boldsymbol{\hat{w}})$$其中有精度矩阵A为\( \ln p(\boldsymbol{w}|\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) \)对w二阶求导，只和该式子前两项有关，与归一化常数无关，我们可以利用\( \ln p(\boldsymbol{t}|\boldsymbol{w},\boldsymbol{X})+\ln p(\boldsymbol{w}|\boldsymbol{\theta}) \)对对w二阶求导得到精度矩阵。$$\boldsymbol{A}=-\frac{d^{2}}{dw^2} \ln p(\boldsymbol{w};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})\bigg|_{w=\hat{w}}$$用exp还原近似的式子为$$p(\boldsymbol{w};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) \approx p(\boldsymbol{\hat{w}};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) exp (-\frac{1}{2}(\boldsymbol{w}-\boldsymbol{\hat{w}})^T\boldsymbol{A}(\boldsymbol{w}-\boldsymbol{\hat{w}}))$$对比高斯分布密度函数可以得出\( \boldsymbol{A}\)为精度，而均值\(\boldsymbol{u}\) 为\( \boldsymbol{\hat{w}}\)。那么对比高斯分布密度函数非指数部分，我们也可以得到归一化常数，当然此时已经没必要求了,我们还是利用如下式子把他求出来。\( \frac{|\boldsymbol{A}|^{\frac{1}{2}}}{({2\pi})^{\frac{M}{2}}} = \ln p(\boldsymbol{t}|\boldsymbol{\hat{w}},\boldsymbol{X})+\ln p(\boldsymbol{\hat{w}}|\boldsymbol{\theta})-\ln p(\boldsymbol{t}) \) 但是拉普拉斯主要依据高斯分布来做的，这是其局限性，特别是现实中很多分布是多峰的，不同的众数会导致不同的拉普拉斯近似。 4.2 贝叶斯派logistics回归分为两个阶段，模型的定义，参数的求解，利用模型推断。 4.2.1 模型的定义与求解我们假设似然函数为logistics回归函数，先验服从高斯分布如下，数据集已知：那么利用拉普拉斯近似参数的后验概率为： 4.2.3 模型推断对于新的预测数据，我们便可以用参数后验概率来推断，但是该积分不好求，具体的求法参考书籍。 模型比较与BIC未完待续 5 参考文献牛顿法与迭代重加权最小平方：https://blog.csdn.net/xuanyuansen/article/details/41050507拉普拉斯近似：https://www.cnblogs.com/hapjin/p/8834794.html,https://www.cnblogs.com/hapjin/p/8834794.html朴素贝叶斯：https://www.jianshu.com/p/b6cadf53b8b8]]></content>
      <categories>
        <category>PRML</category>
      </categories>
      <tags>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML：绪论]]></title>
    <url>%2F2019%2F10%2F06%2FIntroduction%2F</url>
    <content type="text"><![CDATA[1 概率论相关概念理解： 概率：作为贝叶斯派的概率应该理解为：事件的随机性，而频率派的理解是：可重复事件再次发生的可能性。 概率密度：我们将概率这一概念与质量作类比。考虑一个密度分布不均匀的小球，总质量为1，概率密度就相当于这个小球某处的密度，值是可以大于1的，但是这个密度乘以体积所得的质量（也就是概率）是恒小于等于1的。然后至于概率密度越大的点，说明单位体积落在该点的质量越大（也就是发生这个点附近事件的概率越大）。 期望，方差，协方差，协方差矩阵从频率派的角度来看：期望是度量一个随机变量取值的集中位置或平均水平的最基本的数字特征，方差是表示随机变量取值的分散性的一个数字特征。协方差是度量两个变量之间的线性相关性。协方差矩阵是半正定或者正定矩阵（矩阵乘以矩阵转置必定为实对称矩阵，且半正定或者正定），用于衡量变量不同维度的线性相关性。 贝叶斯概率以及贝叶斯估计的思想 最大似然估计的局限：首先最大似然估计是根据一个数据集进行，以正态分布举例，数据集期望的极大似然值是真实分布的期望值，而数据集方差的极大似然值并不是真实分布的方差，比真实值偏小，这是导致过度拟合的主要原因。在公式中这种偏差的体现就是：每一个数据集中样本计算方差是根据当前数据集期望的极大似然值，并非是真实分布的期望值。同时表达出一种思想：数据集越大，数据集方差极大似然值越接近真实分布的方差。 2 模型选择，维度灾难 交叉验证：一种针对较小数据集的验证手段，有K折，留一等方法。 信息准则：作为一种度量不同模型的的准则，更客观表现出每个模型的优良程度如AIC，BIC等。 维度灾难：随着维度的增加，我们模型需要的参数快速增加，使得模型变得笨拙不可利用。 3 决策论 推断和决策过程：一个问题的求解我们将其分为推断和决策部分。推断部分我们可以理解为建立模型的过程，而决策部分理解为通过模型做出结论过程。 最小化错误分类率：这是做分类问题决策最直接原始的想法，就是所有的样本分类全部正确实现最小化的错误分类，以此作为决策目标。 最小化期望损失：但是考虑到不同的分类错误带来的影响不一样，我们引入了损失矩阵，并构建损失函数来表示损失，所有样本的损失期望值作为决策目标。 拒绝选项：设定一个阈值来拒绝对一些样本决策，当该样本的后验概率中最大值依旧小于这个阈值，那拒绝对这个样本决策。 分类问题中推断与决策的三种方式： 求解联合概率，从而得到后验概率，利用决策论与后验概率决策。先求联合概率再求后验概率我们称为生成式模型。 直接求解后验概率，利用决策论与后验概率决策。我们称这样的模型为判别式模型。 找到一个判别函数将输入直接映射到标签。 回归问题的损失函数：和分类问题十分相似，只是没了损失矩阵，我们通过损失函数来计算损失所有样本的损失期望值作为决策目标。 回归问题中推断与决策的三种方式：与回归问题相似。 求解后验概率的必要性：实际上我们有很多理由需要求解后验概率 最小化风险 拒绝选项：后验概率可以让我们设定拒绝的阈值，而直接映射到标签，则无法选择拒绝。 补偿先验概率 组合模型 4 信息论 信息论的基本假设：越不可能发生的事情发生了那么这件事信息量大。 自信息与香农熵：自信息是事情发生了这件事信息量，香农熵（当变量连续时候被称为微分熵）体现了一个变量的信息量。其实熵这个字的本义表示事件的混乱程度。 KL散度与交叉熵 KL散度用于表示不同分布的的相似性，书中用了jensen来证明KL散度的性质。交叉熵的意义等价于KL散度，通常我们衡量真实分布和自己建模的分布KL散度，但是真实分布无法得知其概率密度函数，此时用交叉熵来表示。 5 贝叶斯模型比较5.1 奥卡姆剃刀思想如非必需，勿增实体。 5.2 计算一个模型的模型证据对于一个数据集，可能有多种分布（模型）构成。p(D|Mi)称为模型证据，同时也被称作边缘似然函数(marginal likelihood)，之所以这么叫是因为对模型中的参数进行了marginalize，表示Mi模型生成数据的可能性。p(Mi)表示一个先验，我们通常认为所有的模型的先验相同。那么p(Mi|D)数据集来自Mi模型的可能性正比于模型证据。当多个模型需要选择，我们选择模型证据大的。 那如何计算一个模型证据呢？其实就是在给定Mi情况下，求p(D|Mi),模型Mi生成D的可能性，这里巧妙的把Mi融入p（D|w）。文章提到对参数积分进行一个简单的近似：先省略掉Mi,考虑到模型Mi有一个参数w，w的后验概率正比于p(D|w)p(w)。其中假设后验分布在最大似然值Wmap是一个尖峰，宽度为δw，就可以用被积函数的值乘以尖峰的宽度近似看作积分。如果进一步假设先验分布是平的，宽度为δw(先验)，p(w)=1/δw(先验),如下图： 就可以得到：Mi为M个参数的模型，对每一个参数进行类似的近似。假设所有参数的δw(先验)&gt;δw(后验)都相同，我们有： 此时，我们能够更加清楚的明白模型证据p(D|Mi)与参数量的关系，当参数增加，上式的第一项增加，更加精准的拟合数据，但是第二项为负数（δw(先验)&gt;δw(后验)），M增大使得负增加。其模型证据在两者之间权衡，我们尝试不同的参数寻找最大的模型证据。 5.3 模型平均或者模型选择 5.4 AIC与BIC根据以上理论产生的两个衡量模型的指标。AIC\( \ln (\mathcal D) \simeq \ln (\mathcal D | \boldsymbol{\theta}_{MAP} ) -M \)BIC 5.5 贝叶斯模型比较的缺陷以及解决方法如果先验分布是反常，那么模型证据无法定义，因为反常的先验分布有任意的缩放因子。如果考虑一个正常的先验分布，然后去一个适当的极限来获得一个反常先验，那模型证据就会趋于零。现实中使用独立的先验来解决这个问题。 6 参考文献 概率密度的理解：https://www.zhihu.com/question/263467674 贝叶斯估计的思想：https://wenku.baidu.com/view/ea06a887e45c3b3566ec8b33.html 协方差矩阵的理解：https://www.zhihu.com/question/24283387/answer/27294834 模型比较：https://blog.csdn.net/jeak2015/article/details/82228726 模型比较：https://zhuanlan.zhihu.com/p/25292425]]></content>
      <categories>
        <category>PRML</category>
      </categories>
      <tags>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch进阶学习]]></title>
    <url>%2F2019%2F09%2F18%2Flearn-pytorch%2F</url>
    <content type="text"><![CDATA[一 利用Variable自动求导1.1 Variable1.1.1 定义&emsp;&emsp;在pytorch中，我们需要能够构建计算图的 tensor，这就是 Variable数据结构。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身.data，对应 tensor 的梯度.grad以及这个 Variable 是通过什么方式得到的.grad_fn。 1.1.2 特性 requires_grad变量可以有梯度，求导。 volatile主要以用于inference过程中。若是某个过程，从 x 开始 都只需做预测，不需反传梯度的话，那么只需设置x.volatile=True ,那么 x 以后的运算过程的输出均为 volatile==True ,即 requires_grad==False。虽然inference 过程不必backward(),所以requires_grad 的值为False 或 True，对结果是没有影响的，但是对程序的运算效率有直接影响；所以使用volatile=True ,就不必把运算过程中所有参数都手动设一遍requires_grad = False 了，方便快捷。 detach 123456789y = A(x)temp=y.detach()z = B(temp)z.backward()def detach(self): result = NoGrad()(self) # this is needed, because it merges version counters result._grad_fn = None return result &emsp;&emsp;如果我们有两个网络 , 两个关系是这样的 现在我们想用 来为B网络的参数来求梯度，但是又不想求A网络参数的梯度。接着我们看一下detach的源码，将grad_fn设置为None，也就是说切断了变量temp与上一个网络反向传播的途径。不知道temp如何得到的。 retain_graph 123456789import torchx = torch.randn((1,4),dtype=torch.float32,requires_grad=True)y = x ** 2z = y * 4output1 = z.mean()output2 = z.sum()output1.backward() # 这个代码执行正常，但是执行完中间变量都free了，所以下一个出现了问题#正确的写法是 output1.backward(retain_graph=True) output2.backward() # 这时会引发错误 &emsp;&emsp;所以retain_graph主要用于处理，计算节点数值保存了，但是计算图x-y-z-out结构被释放了的情况。 create_graph官方的意思是对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。默认False，目前还没看到具体的应用，看到再补上。 1.2 自动求导过程1.2.1 输出为标量首先我们看一段代码：1234567891011121314151617import torch as t from torch.autograd import Variable as Va=V(t.Tensor([2,3]),requires_grad=True)b=a+3c=b*3out=c.mean()out.backward()print(&quot;a.data\n&quot;,a.data)print(&quot;a.grad\n&quot;,a.grad)print(&quot;a.grad_fn\n&quot;,a.grad_fn)print(&quot;b.data\n&quot;,b.data)print(&quot;b.grad\n&quot;,b.grad)print(&quot;b.grad_fn\n&quot;,b.grad_fn)print(&quot;out.data\n&quot;,out.data)print(&quot;out.grad\n&quot;,out.grad)print(&quot;out.grad_fn\n&quot;,out.grad_fn) 对应的函数是:$$out=\frac{3\left [ \left ( a_1 + 3 \right )+\left ( a_2 + 3 \right ) \right ]}{2}=\frac{3\left [ \mathbf{a}+3 \right ]}{2}$$所以\(\mathbf{a}\)向量的梯度为:$$\frac{\partial out}{\partial \mathbf{a}}=(\frac{3}{2},\frac{3}{2})$$真实的运行结果为：123456789101112131415161718a.data tensor([2., 3.])a.grad tensor([1.5000, 1.5000])a.grad_fn Noneb.data tensor([5., 6.])b.grad Noneb.grad_fn &lt;AddBackward0 object at 0x000002C22F441A20&gt;out.data tensor(16.5000)out.grad Noneout.grad_fn &lt;MeanBackward1 object at 0x000002C22F441160&gt; 我们得到以下结论： grad_fn表示变量通过怎样的计算方式得到，叶节点变量None grad为梯度，中间变量不存储grad，只有叶节点存储。 data为运行过程中变量的值 1.2.2 输出为向量1.2.2.1 雅可比矩阵的介绍&emsp;&emsp;Rn→Rm为一个从欧式n维空间转换到欧式m维空间的函数，并且由m个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn)。若将该函数的偏导数(若存在)组成一个m行n列的矩阵, 那么这个矩阵就是所谓的雅可比矩阵: 1.2.2.2 实例首先我们再看一段代码 12345678910111213141516171819import torch as t from torch.autograd import Variable as Va=V(t.Tensor([[2,4]]),requires_grad=True)b=t.zeros(1,2)b[0,0]=a[0,0]**2+a[0,1]b[0,1]=a[0,1]**3+a[0,0]out=2*bout.backward(t.Tensor([[1,1]]))#backward参数与out维度相同print(&quot;a.data\n&quot;,a.data)print(&quot;a.grad\n&quot;,a.grad)print(&quot;a.grad_fn\n&quot;,a.grad_fn)print(&quot;b.data\n&quot;,b.data)print(&quot;b.grad\n&quot;,b.grad)print(&quot;b.grad_fn\n&quot;,b.grad_fn)print(&quot;out.data\n&quot;,out.data)print(&quot;out.grad\n&quot;,out.grad)print(&quot;out.grad_fn\n&quot;,out.grad_fn) 对应的函数为：$$\mathbf{out}=2\mathbf{b}=2\left ( \left (a_1\right )^2+a_2,\left (a_2\right )^3+a_1 \right)$$我们求得\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵为：\begin{pmatrix}\frac{\partial out_1}{\partial a_1}=4a_1 &amp; \frac{\partial out_1}{\partial a_2}=2 \\\frac{\partial out_2}{\partial a_1}=2&amp; \frac{\partial out_2}{\partial a_2}=6{a_2}^2\end{pmatrix}也就是：\begin{pmatrix}8 &amp; 2\\ 2&amp;96\end{pmatrix}真实的运行结果为： 123456789101112131415161718a.data tensor([[2., 4.]])a.grad tensor([[10., 98.]])a.grad_fn Noneb.data tensor([[ 8., 66.]])b.grad Noneb.grad_fn &lt;CopySlices object at 0x0000020221827F60&gt;out.data tensor([[ 16., 132.]])out.grad Noneout.grad_fn &lt;MulBackward0 object at 0x0000020221827C50&gt; 我们得到以下结论： \(\mathbf{a}\)向量的梯度由\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵得出。也就是\( a.grad=(\frac{\partial out_1}{\partial a_1}+\frac{\partial out_2}{\partial a_1}=10 , \frac{\partial out_1}{\partial a_2}+\frac{\partial out_2}{\partial a_2}=98)\),那么这是为什么呢？与backward输入的[[1,1]]参数有何联系？接下来看下一节。 1.2.3 输出为矩阵：backward（）参数的意义1.2.3.1 矩阵求导的介绍&emsp;&emsp;基础是利用矩阵微分，复杂结合链式法则。矩阵微分和矩阵的迹有很大的关系，矩阵论中有详细的描述。矩阵微分的介绍:https:/www.cnblogs.compinardp/10791506.html矩阵求导常用公式：https://blog.csdn.net/WPR1991/article/details/82929843 1.2.3.2 实例首先看一段代码 123456789101112131415161718import torch as t from torch.autograd import Variable as Vx=t.Tensor([[100,200],[101,201]])w=V(t.Tensor([[0.1],[0.2]]),requires_grad=True)b=V(t.Tensor([[0.01],[0.02]]),requires_grad=True)y=t.Tensor([[1],[0]])out=(t.mm(x,w)+b)-yout.backward(t.Tensor([[1],[2]]))print(&quot;w.data\n&quot;,w.data)print(&quot;w.grad\n&quot;,w.grad)print(&quot;w.grad_fn\n&quot;,w.grad_fn)print(&quot;b.data\n&quot;,b.data)print(&quot;b.grad\n&quot;,b.grad)print(&quot;b.grad_fn\n&quot;,b.grad_fn)print(&quot;out.data\n&quot;,out.data)print(&quot;out.grad\n&quot;,out.grad)print(&quot;out.grad_fn\n&quot;,out.grad_fn) &emsp;&emsp;我们模拟一个超级简单的网络\(\mathbf{out} ={\left (\mathbf{X}\mathbf{W}+\mathbf{B}\right )}-\mathbf{Y} \),全部为矩阵。\( \mathbf{X} \)为我们输入的有2个属性的2组数据，\(\mathbf{Y}\)为2组数据的标签，所以这两个只需要Tensor封装，不需要Variable封装，\(\mathbf{out}\)为差损失。\(\mathbf{W}\)与\(\mathbf{B}\)为随机初始化的参数，需要计算梯度并通过梯度下降等方法变化，代码中没有写优化算法，我们只想看看一次反向传播后变量的梯度是多少。理论上可以得出：$$\frac{\partial \mathbf{out}}{\partial \mathbf{W}} = \mathbf{X} =\begin{pmatrix}100 &amp; 200\\101 &amp; 201\end{pmatrix}$$$$\frac{\partial \mathbf{out}}{\partial \mathbf{B}} = \mathbf{I} =\begin{pmatrix}1 &amp; 0\\0 &amp; 1\end{pmatrix}$$ 我们看一下运行结果 1234567891011121314151617181920212223w.data tensor([[0.1000], [0.2000]])w.grad tensor([[302.], [602.]])w.grad_fn Noneb.data tensor([[0.0100], [0.0200]])b.grad tensor([[1.], [2.]])b.grad_fn Noneout.data tensor([[49.0100], [50.3200]])out.grad Noneout.grad_fn &lt;SubBackward0 object at 0x000001E4B0857C18&gt; 可以看到\(\mathbf{W}\)与\(\mathbf{B}\)的梯度是理论值与backward参数运算的结果.\(\mathbf{W}\)的梯度为\begin{pmatrix}100+101\times 2 \\200+201\times 2\end{pmatrix}也就是\begin{pmatrix}302 \\602\end{pmatrix}\(\mathbf{B}\)和\(\mathbf{W}\)计算方式相同。我们可以得出以下的结论： backward的参数的实际意义是：当每个变量在反向传播计算梯度时候，不同的样本将给与不同的权重影响。这个权重就是backward的参数，当参数全部为1，表示所有的样本都一样，意味着拿到一个mini-batch的所有数据平均梯度。backward的参数与y的维度相同。 1.2.4 利用自动求导找函数的最小值原来的文章https://blog.csdn.net/weixin_42892943/article/details/94716387 首先我们看一段代码 12345678910111213141516import numpy as npimport torch as t from torch.autograd import Variable as Vdef fun(x): return (x[0]**2+x[1]-11)**2+(x[0]+x[1]**2-7)**2x = V(t.Tensor([0.,0.]),requires_grad=True)optimizer = t.optim.Adam([x],lr=1e-3)for step in range(20000): pred = fun(x) optimizer.zero_grad() pred.backward() optimizer.step() if step%2000 == 0: print(&apos;step&#123;&#125;: x = &#123;&#125;, x.grad = &#123;&#125;, f(x) = &#123;&#125;&apos;.format(step,x.toli(),x.grad,pred.item())) 我们构建了一个函数：$$z=(x^2+y-11)^2+(x+y^2-7)^2$$其图像为：我们利用adam算法优化器寻找函数的最小值。结果如下： 12345678910step0: x = [0.0009999999310821295, 0.0009999999310821295], x.grad = tensor([-14., -22.]), f(x) = 170.0step2000: x = [2.3331806659698486, 1.9540692567825317], x.grad = tensor([-35.3487, -13.8643]), f(x) = 13.730920791625977step4000: x = [2.9820079803466797, 2.0270984172821045], x.grad = tensor([-0.7803, 0.5791]), f(x) = 0.014858869835734367step6000: x = [2.999983549118042, 2.0000221729278564], x.grad = tensor([-0.0008, 0.0004]), f(x) = 1.1074007488787174e-08step8000: x = [2.9999938011169434, 2.0000083446502686], x.grad = tensor([-0.0003, 0.0002]), f(x) = 1.5572823031106964e-09step10000: x = [2.999997854232788, 2.000002861022949], x.grad = tensor([-9.5367e-05, 5.7221e-05]), f(x) = 1.8189894035458565e-10step12000: x = [2.9999992847442627, 2.0000009536743164], x.grad = tensor([-2.8610e-05, 1.7166e-05]), f(x) = 1.6370904631912708e-11step14000: x = [2.999999761581421, 2.000000238418579], x.grad = tensor([-9.5367e-06, 5.7220e-06]), f(x) = 1.8189894035458565e-12step16000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0step18000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0 我们可以看到x逐渐接近最低点，梯度在不断的减小。 二 数据集预处理的自定义构建2.1 transforms中的源码123456789101112class Normalize(object): def __init__(self, mean, std, inplace=False): self.mean = mean self.std = std self.inplace = inplace def __call__(self, tensor): return F.normalize(tensor, self.mean, self.std, self.inplace) def __repr__(self): return self.__class__.__name__ + &apos;(mean=&#123;0&#125;, std=&#123;1&#125;)&apos;.format(self.mean, self.std) 以Normalize为例子，我们看到每个处理方式由init，call，repr组成。 2.2 自定义自己的预处理方式&emsp;&emsp;torchvision.transforms的方法都是随机的，如果样本和标签都是图片，需要转动相同的随机角度。需要自己来定义预处理方式。这时候可以借用torchvision.transforms.functional来构造自己的函数，同时处理样本标签，得到新的样本标签后再分开处理。 12345678910111213141516171819202122232425import torchvision.transforms.functional as TFimport torchvision.transforms as T #自定义旋转角度：方法一，二class FixedRotation(object): def __init__(self, startangle,endangle): self.startangle = startangle self.endangle = endangle def __call__(self,img): return my_transforms1(img,startangle,endangle) # return my_transforms2(img,startangle,endangle) def __repr__(self): return self.__class__.__name__ +&apos;from&#123;&#125;to&#123;&#125;&apos;.format(startangle,endangle)def my_transforms1(img, startangle,endangle): angle = random.randint(startangle,endangle) image = img.rotate(angles[angle]) return imagedef my_transforms2(image): angle = random.randint(-30, 30) image = TF.rotate(image, angle) return image 2.3 自定义一个数据集预处理2.3.1 torch.utils.data.Dataset自定义一个数据集需要继承类torch.utils.data.Dataset。类中主要方法有3个： __init__(self,path)该方法用来初始化类和对数据进行加载,数据的加载就是针对不同的数据读入到内存中。 __getitem__(self, index)该方法是把读入的输出传给PyTorch（迭代器的方式）。 __len__(self)该方法是数据大小，迭代一次数据的大小。 2.3.2 torch.utils.data.DataLoader()torch.utils.data.DataLoader类主要使用torch.utils.data.sampler实现，sampler是所有采样器的基础类，提供了迭代器的迭代（iter）和长度（len）接口实现，同时sampler也是通过索引对数据进行洗牌(shuffle)等操作。因此，如果DataLoader不适用于你的数据，需要重新设计数据的分批次，可以充分使用所提供的smapler。 batch-size。样本每个batch的大小，默认为1。 shuffle。是否打乱数据，默认为False。 sampler。定义一个方法来绘制样本数据，如果定义该方法，则不能使用shuffle。 num_workers。数据分为几批处理（对于大数据）。 collate_fn。整理数据，把每个batch数据整理为tensor。（一般使用默认调用default_collate(batch)）。 pin_memory。将获取的数据张量放在固定的内存中，从而能够更快地将数据传输到支持cuda的gpu drop_last。用于处理最后一个batch的数据。因为最后一个可能不能够被整除，如果设置为True，则舍弃最后一个，为False则保留最后一个，但是最后一个可能很小。 主要讲一下sampler与collate_fn. 三 神经网络的自定义构建3.1 详解nn.module&emsp;&emsp;这里先看一下nn.module类源码中重要的属性方法，这里放一段代码，等下下面会用到，我们建立了一个前传后的钩子函数，一个有纯参数组成的module，一个由submodule组成的module，并用这两个module组成个复杂的大module。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import torch as t from torch import nn from torch.nn import functional as Ffrom torch.autograd import Variable as Vdef for_hook(module, input, output): print(module) for val in input: print(&quot;input val:&quot;,val) for out_val in output: print(&quot;output val:&quot;, out_val)class Linear(nn.Module): # 继承nn.Module def __init__(self, in_features, out_features): super(Linear, self).__init__() # 等价于nn.Module.__init__(self) self.w = nn.Parameter(t.randn(in_features, out_features)) self.b = nn.Parameter(t.randn(out_features)) def forward(self, x): x = x.mm(self.w) # x.@(self.w) return x + self.b.expand_as(x)class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.subsubmodule=Linear(3,3) self.subsubmodule.register_forward_hook(for_hook) def forward(self,x): x = self.subsubmodule(x) x = F.relu(x) return xclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.register_buffer(&quot;buf1&quot;,t.ones(3,3)) #为module注册一个缓存区 self.register_parameter(&quot;param1&quot;,nn.Parameter(t.ones(3,3))) #与下一句等价，注册一个参数，本质就是一个变量 self.param2 = nn.Parameter(t.rand(3, 3)) self.add_module(&quot;submodel1&quot;,nn.Linear(3, 3)) #与下一句等价，注册一个子module self.submodel2 = nn.Linear(3, 3) self.submodel3 = MyLinear() self.modulelist1 = nn.ModuleList([nn.Linear(3,3),nn.Linear(3,3)]) self.modulelist2 = nn.Sequential(nn.Linear(3,3),nn.Linear(3,3)) self.bn = nn.BatchNorm1d(3) def forward(self, input): x = input.mm(self.param1) x = self.submodel1(x) x = self.submodel3(x) # x = self.modulelist1(x) error x = self.modulelist2(x) x = self.bn(x) return x net = Net()x=t.rand(2,3)y=net(x) 3.1.1 属性1234567891011class Module(object): def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True self._forward_pre_hooks = OrderedDict() self._state_dict_hooks = OrderedDict() self._load_state_dict_pre_hooks = OrderedDict() 解释一下重要的几个属性，我把它分为三类。 用于存储数据的： _parameters：字典，保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为’param’，value为对应parameter的item。不会查看到子module的参数。 _modules：子module，通过self.submodel = nn.Linear(3, 4)指定的子module会保存于此。同时子moudle可以自己定义。 _buffers：缓存，每个moudle都可以注册自己的缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 用于是否继续向前传播的: training：通过判断training值来决定正向传播策略。 钩子函数来实现对前传,后传,保存,回复等操作的触发,_backward_hooks,_forward_hooks,_forward_pre_hooks，_state_dict_hooks，_load_state_dict_pre_hooks这些字典主要用于存储钩子。 我们看几个属性：123456789101112131415161718192021222324252627282930net._buffers: OrderedDict([(&apos;buf1&apos;, tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]))])net._modules: OrderedDict([(&apos;submodule1&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule2&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule3&apos;, MyLinear( (subsubmodule): Linear())), (&apos;modulelist1&apos;, ModuleList( (0): Linear(in_features=3, out_features=3, bias=True) (1): Linear(in_features=3, out_features=3, bias=True))), (&apos;modulelist2&apos;, Sequential( (0): Linear(in_features=3, out_features=3, bias=True) (1): Linear(in_features=3, out_features=3, bias=True))), (&apos;bn&apos;, BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))])net._parameters: OrderedDict([(&apos;param1&apos;, Parameter containing:tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], requires_grad=True)), (&apos;param2&apos;, Parameter containing:tensor([[0.3535, 0.6803, 0.7144], [0.2985, 0.1329, 0.2111], [0.3999, 0.0395, 0.1407]], requires_grad=True))])net.training: Truenet.submodule3.subsubmodule._forward_hooks: OrderedDict([(0, &lt;function for_hook at 0x0000028C74F23E18&gt;)]) 3.1.2 方法根据属性我们来分开介绍一些重要的nn.moudle的重要方法。 3.1.2.1 缓存，子模型，参数的设置，操作，查看的方法 self.register_buffer（name[string],buf[Tensor]） self.register_parameter（name[string],param[nn.Parameter]）等价与self.name=param self.add_module（name[string],submodule[nn.Module]） 等价于self.name=submodule self.named_parameters() 生成器，产生所有参数的name与param self.named_buffers() 生成器，产生所有参数的name与buf self.named_module() 生成器，产生所有参数的name与module，包括module与子module self.named_children() 生成器，产生所有参数的name与submodule self.cuda() 转移到cuda上 self.cpu() 转移到cpu上 self.float() 参书变换 self.to() 多态性 12345.. function:: to(device=None, dtype=None, non_blocking=False).. function:: to(dtype, non_blocking=False).. function:: to(tensor, non_blocking=False) self.apply(fn) 源码 12345def apply(self, fn): for module in self.children(): module.apply(fn) fn(self) return self 我们看一些方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546net.named_parameters()所有参数名称:param1param2submodule1.weightsubmodule1.biassubmodule2.weightsubmodule2.biassubmodule3.subsubmodule.wsubmodule3.subsubmodule.bmodulelist1.0.weightmodulelist1.0.biasmodulelist1.1.weightmodulelist1.1.biasmodulelist2.0.weightmodulelist2.0.biasmodulelist2.1.weightmodulelist2.1.biasbn.weightbn.biasnet.named_modules()所有模型名称:submodule1submodule2submodule3submodule3.subsubmodulemodulelist1modulelist1.0modulelist1.1modulelist2modulelist2.0modulelist2.1bnnet.named_children()所有子模型名称:submodule1submodule2submodule3modulelist1modulelist2bnnet.named_buffers()所有缓存名称:buf1bn.running_meanbn.running_varbn.num_batches_tracked 3.2.1.2 传播的方法 self.train() #使用Dropout与BN层训练时开启 self.eval() #使用Dropout与BN层测试时开启 self.zero_grad() #清空所有参数的梯度 3.2.1.3 钩子的方法 self.register_forward_hook(hook) 设立该module前传后的钩子 self.register_backward_hook(hook) 设立该module后传后的钩子 self.register_forward_pre_hook(hook) 设立该module前传前的钩子 正向传播时候当钩子的函数监控的module发生正向传播，触发钩子函数： 12345Linear()input val: tensor([[ 0.3910, -0.9092, -0.9559], [ 0.4495, -0.9752, -1.0354]], grad_fn=&lt;AddmmBackward&gt;)output val: tensor([ 1.3830, 1.3145, -0.7691], grad_fn=&lt;SelectBackward&gt;)output val: tensor([ 1.5223, 1.4328, -0.7066], grad_fn=&lt;SelectBackward&gt;) 3.3 自定义functional其实整个网络都是依靠与基础的functional，functional实现了卷积，损失等等一系列功能，那么我们怎么自己写一个functional呢？其实pytorch官方给出了拓展方式，用numpy与scipy（实现卷积等操作封装度高于numpy）即可，如果你的操作这些库依旧不能满足，那么需要定制C++底层，这里我们先不学习，主要学习使用python下的定制functional。https://www.cnblogs.com/hellcat/p/8453615.htmlhttps://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html 3.3.1 官方源码的解析123456789101112131415class BadFFTFunction(Function): @staticmethod def forward(ctx, input): numpy_input = input.detach().numpy() result = abs(rfft2(numpy_input)) return input.new(result) @staticmethod def backward(ctx, grad_output): numpy_go = grad_output.numpy() result = irfft2(numpy_go) return grad_output.new(result)def incorrect_fft(input): return BadFFTFunction.apply(input) 这段代码是官方源码,我们认真看一下：这段代码没有数学意义，仅仅是为了演示过程，我们看到forword与backword两个过程。 forword过程输入input，输出output，而backword过程输入grad_output，输出grad_input。 forword输入的参数量（除去ctx）等于backward输出的参数量，forword输出的参数量等于backward输入的参数量（除去ctx）。 通常我们的grad_input是\( \frac{\partial output}{\partial intput} \)与grad_output运算的结果,grad_output由人工输入，或者上一层传递而来，意义其实是不同的样本对与梯度下降的影响程度。 此外ctx本质是一个缓存区，用于正向传播的缓存在反向传播时候使用。 3.3.2 定制的函数接下来我们尝试写一个超级简单的网络。Y=XW+b,假设X维度（5，10），W维度（10，1），b维度（5，1），Y的维度（5，1）。意思是我们有一个数据集，包含5个样本，每个样本对应一个标签，每个样本10个属性。我们想要用W，b来拟合X的属性与标签Y的关系。forward很简单，backward则需要手工计算梯度。1234567891011121314151617181920212223242526272829class Test2Function(Function): @staticmethod def forward(ctx,input,w,b): numpy_input = input.detach().numpy() numpy_w = w.detach().numpy() numpy_b = b.detach().numpy() result = np.dot(numpy_input,numpy_w)+numpy_b ctx.save_for_backward(input,w,b) return input.new(result) @staticmethod def backward(ctx, grad_output): #此时的grad_output是backword()输入的参数 input,w,b= ctx.saved_tensors #这里我进行了手工求导，具体可在演算纸上进行 grad_input = np.dot( grad_output.detach().numpy(),w.detach().numpy().T) grad_w = np.dot(input.detach().numpy().T,grad_output.detach().numpy()) grad_b = grad_output.detach().numpy() return t.from_numpy(grad_input),t.from_numpy(grad_w),t.from_numpy(grad_b)class TestMoudle(nn.Module): def __init__(self): super(TestMoudle, self).__init__() self.w = nn.Parameter(t.ones(10,1)) self.b= nn.Parameter(t.ones(5, 1)) def forward(self, input): return Test2Function.apply(input, self.w, self.b) 如果是loss函数的定制，在backward过程中直接返回grad_output，或者不写backward，会继承父类的backward。 3.4 需要辨析注意的点3.4.1 nn.Parameter与Variable的关系nn.Parameter是Variable的子类，用于在nn.module中设置参数变量。 3.4.2 nn.ModuleList与nn.Sequential()区别nn.ModuleList和nn.Sequential()都可以在nn.module初始化时候设置网络层，也都可以被有关数据存储的方法，属性所识别，唯一不同的是nn.Sequential()具有call()功能，forward过程中直接处理上一层。此外如果在nn.module初始化时用了列表来包含一系列网络层，这些网络层将不会被有关数据存储的方法，属性所识别。 3.4.3 何时使用from torch.nn import functional as F中的FF通常是无参数的，所以我们不必在nn.module初始化时候声明，而在forward中直接使用。这样写的原因主要是让我们更加方便整理代码。 四 多GPU应用与部署（未完待续）因为我只有一个gpu，没机会试试。这个是中文官网的代码应该问题不大。https://www.pytorchtutorial.com/pytorch-large-batches-multi-gpu-and-distributed-training/ 1234567891011from parallel import DataParallelModel, DataParallelCriterion parallel_model = DataParallelModel(model) # 并行化modelparallel_loss = DataParallelCriterion(loss_function) # 并行化损失函数 predictions = parallel_model(inputs) # 并行前向计算 # &quot;predictions&quot;是多个gpu的结果的元组loss = parallel_loss(predictions, labels) # 并行计算损失函数loss.backward() # 计算梯度optimizer.step() # 反向传播predictions = parallel_model(inputs) 五 可视化5.1 Visdom之前一直在用的可视化工具，后来发现定制图片需要开会员。现在决定要放弃了。https://ptorch.com/news/77.html 5.2 Tensorboardx支持一下tensorboardXhttps://blog.csdn.net/wsp_1138886114/article/details/87602112#TensorBoardX_120 5.3 怎么画网络结构如果你想实时查看自己的网络模型可以用tensorboardx。如果想画一个精美的图建议ppt，此外plotneuralnethttps://www.pytorchtutorial.com/plotneuralnet/]]></content>
      <categories>
        <category>pytorch进阶学习</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本操作二：写作技巧]]></title>
    <url>%2F2019%2F08%2F24%2Fsecond2%2F</url>
    <content type="text"><![CDATA[写作###有关符号编译出错123&#123; -&gt; &amp;#123; — 大括号左边部分Left curly brace&#125; -&gt; &amp;#125; — 大括号右边部分Right curly brace空格 -&gt; &amp;ensp; ###有关图片在blog（hexo）目录下Git Bash Here，运行hexo n “博客名”来生成md博客时，会在_post目录下看到一个与博客同名的文件夹。按照如下格式则可以插入图片1![你想要输入的替代文字](second2/test.jpg) 测试效果图： ###有关音乐在网易云音乐中生成外链音乐播放器。如下1&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&amp;id=541326593&amp;auto=1&amp;height=66"&gt;&lt;/iframe&gt; ###有关视频在优酷中生成外链视频播放器。如下1&lt;iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=i0031n20390" allowFullScreen="true"&gt;&lt;/iframe&gt;]]></content>
      <categories>
        <category>hexo博客操作</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本操作一：部署命令]]></title>
    <url>%2F2018%2F12%2F02%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files and compress12$ hexo generate$ gulp More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo博客操作</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
