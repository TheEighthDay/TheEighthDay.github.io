<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>过了星期天的博客</title>
  
  <subtitle>同是海角沉溺堕落人</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-06T15:23:06.658Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>星期八</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PRML：绪论</title>
    <link href="http://yoursite.com/2019/10/06/Introduction/"/>
    <id>http://yoursite.com/2019/10/06/Introduction/</id>
    <published>2019-10-06T15:14:11.000Z</published>
    <updated>2019-10-06T15:23:06.658Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概率论相关概念的通俗理解："><a href="#1-概率论相关概念的通俗理解：" class="headerlink" title="1 概率论相关概念的通俗理解："></a>1 概率论相关概念的通俗理解：</h1><p>这里将我看到知识点作通俗的表达，可以加深自己的理解，详细的案例在书籍《PRML》。<br><a id="more"></a></p><ol><li><strong>概率</strong>：作为贝叶斯派的概率应该理解为：事件的随机性，而频率派的理解是：可重复事件再次发生的可能性。</li><li><strong>概率密度</strong>：我们将概率这一概念与质量作类比。考虑一个密度分布不均匀的小球，总质量为1，概率密度就相当于这个小球某处的密度，值是可以大于1的，但是这个密度乘以体积所得的质量（也就是概率）是恒小于等于1的。然后至于概率密度越大的点，说明单位体积落在该点的质量越大（也就是发生这个点附近事件的概率越大）。</li><li><strong>期望，方差，协方差，协方差矩阵</strong>：从频率派的角度来看：期望是度量一个随机变量取值的集中位置或平均水平的最基本的数字特征，方差是表示随机变量取值的分散性的一个数字特征。协方差是度量两个变量之间的线性相关性。协方差矩阵是半正定或者正定矩阵（矩阵乘以矩阵转置必定为实对称矩阵，且半正定或者正定），用于衡量变量不同维度的线性相关性。至于说贝叶斯派的理解一直没找到，可能是不用这样的说法。</li><li><strong>贝叶斯概率以及贝叶斯估计的思想</strong>：参考文献第二点。</li><li><strong>最大似然估计的局限</strong>：首先最大似然估计是根据一个数据集进行，数据集期望的无偏估计值是真实分布的期望，而数据集方差的无偏估计值并不是真实分布的方差，比真实值偏小，这是导致过度拟合的主要原因。在公式中这种偏差的体现就是：每一个数据集中样本计算方差是根据当前数据集的期望并非是真实分布的期望。同时表达出一种思想：数据集越大，数据集方差无偏估计值越接近真实分布的方差。</li></ol><h1 id="2-模型选择，维度灾难"><a href="#2-模型选择，维度灾难" class="headerlink" title="2 模型选择，维度灾难"></a>2 模型选择，维度灾难</h1><ol><li><strong>交叉验证</strong>：一种针对较小数据集的验证手段，有K折，留一等方法。</li><li><strong>信息准则</strong>：作为一种度量不同模型的的准则，更客观表现出每个模型的优良程度。更客观的原因是相比于直接依靠最大似然函数来评判模型好坏。AIC，BIC等对于每个模型，通过加入惩罚项来减低过拟合的风险，修正最大似然函数值，用修正的最大似然函数值来评判模型好坏。</li><li><strong>维度灾难</strong>：随着维度的增加，我们模型需要的参数快速增加，使得模型变得笨拙不可利用。</li></ol><h1 id="3-决策论"><a href="#3-决策论" class="headerlink" title="3 决策论"></a>3 决策论</h1><ol><li><strong>推断和决策过程</strong>：一个问题的建模我们将其分为推断和决策部分。</li><li>未完待续……</li></ol><h1 id="4-信息论"><a href="#4-信息论" class="headerlink" title="4 信息论"></a>4 信息论</h1><ol><li><strong>信息论的基本假设</strong>：越不可能发生的事情发生了那么这件事信息量大。</li><li><strong>自信息与香农熵</strong>：自信息是事情发生了这件事信息量，香农熵（当变量连续时候被称为微分熵）体现了一个变量的信息量。其实熵这个字的本义表示事件的混乱程度。</li><li><strong>KL散度与交叉熵</strong> KL散度用于表示不同分布的的相似性，书中用了jensen来证明KL散度的性质。交叉熵的意义等价于KL散度，通常我们衡量真实分布和自己建模的分布KL散度，但是真实分布无法得知其概率密度函数，此时用交叉熵来表示。</li></ol><h1 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5 参考文献"></a>5 参考文献</h1><ul><li>概率密度的理解：<a href="https://www.zhihu.com/question/263467674" target="_blank" rel="noopener">https://www.zhihu.com/question/263467674</a></li><li>贝叶斯估计的思想：<a href="https://wenku.baidu.com/view/ea06a887e45c3b3566ec8b33.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/ea06a887e45c3b3566ec8b33.html</a></li><li>协方差矩阵的理解：<a href="https://www.zhihu.com/question/24283387/answer/27294834" target="_blank" rel="noopener">https://www.zhihu.com/question/24283387/answer/27294834</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-概率论相关概念的通俗理解：&quot;&gt;&lt;a href=&quot;#1-概率论相关概念的通俗理解：&quot; class=&quot;headerlink&quot; title=&quot;1 概率论相关概念的通俗理解：&quot;&gt;&lt;/a&gt;1 概率论相关概念的通俗理解：&lt;/h1&gt;&lt;p&gt;这里将我看到知识点作通俗的表达，可以加深自己的理解，详细的案例在书籍《PRML》。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://yoursite.com/categories/PRML/"/>
    
    
      <category term="PRML" scheme="http://yoursite.com/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>pytorch进阶学习</title>
    <link href="http://yoursite.com/2019/09/18/learn-pytorch/"/>
    <id>http://yoursite.com/2019/09/18/learn-pytorch/</id>
    <published>2019-09-18T08:19:47.000Z</published>
    <updated>2019-09-18T09:46:16.889Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-利用Variable自动求导"><a href="#一-利用Variable自动求导" class="headerlink" title="一 利用Variable自动求导"></a>一 利用Variable自动求导</h1><h2 id="1-1-Variable"><a href="#1-1-Variable" class="headerlink" title="1.1 Variable"></a>1.1 Variable</h2><h3 id="1-1-1-定义"><a href="#1-1-1-定义" class="headerlink" title="1.1.1 定义"></a>1.1.1 定义</h3><p>&emsp;&emsp;在pytorch中，我们需要能够构建计算图的 tensor，这就是 Variable数据结构。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身.data，对应 tensor 的梯度.grad以及这个 Variable 是通过什么方式得到的.grad_fn。</p><h3 id="1-1-2-特性"><a href="#1-1-2-特性" class="headerlink" title="1.1.2 特性"></a>1.1.2 特性</h3><ul><li><p>requires_grad<br>变量可以有梯度，求导。</p></li><li><p>volatile<br>主要以用于inference过程中。若是某个过程，从 x 开始 都只需做预测，不需反传梯度的话，那么只需设置x.volatile=True ,那么 x 以后的运算过程的输出均为 volatile==True ,即 requires_grad==False。虽然inference 过程不必backward(),所以requires_grad 的值为False 或 True，对结果是没有影响的，但是对程序的运算效率有直接影响；所以使用volatile=True ,就不必把运算过程中所有参数都手动设一遍requires_grad = False 了，方便快捷。</p><a id="more"></a></li><li>detach</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">y = A(x)</span><br><span class="line">temp=y.detach()</span><br><span class="line">z = B(temp)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">def detach(self):</span><br><span class="line">  result = NoGrad()(self)  # this is needed, because it merges version counters</span><br><span class="line">  result._grad_fn = None</span><br><span class="line">  return result</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;如果我们有两个网络 , 两个关系是这样的  现在我们想用 来为B网络的参数来求梯度，但是又不想求A网络参数的梯度。接着我们看一下detach的源码，将grad_fn设置为None，也就是说切断了变量temp与上一个网络反向传播的途径。不知道temp如何得到的。</p><ul><li>retain_graph</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.randn((1,4),dtype=torch.float32,requires_grad=True)</span><br><span class="line">y = x ** 2</span><br><span class="line">z = y * 4</span><br><span class="line">output1 = z.mean()</span><br><span class="line">output2 = z.sum()</span><br><span class="line">output1.backward()    # 这个代码执行正常，但是执行完中间变量都free了，所以下一个出现了问题</span><br><span class="line">#正确的写法是  output1.backward(retain_graph=True) </span><br><span class="line">output2.backward()    # 这时会引发错误</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;所以retain_graph主要用于处理，计算节点数值保存了，但是计算图x-y-z-out结构被释放了的情况。</p><ul><li>create_graph<br>官方的意思是对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。默认False，目前还没看到具体的应用，看到再补上。</li></ul><h2 id="1-2-自动求导过程"><a href="#1-2-自动求导过程" class="headerlink" title="1.2 自动求导过程"></a>1.2 自动求导过程</h2><h3 id="1-2-1-输出为标量"><a href="#1-2-1-输出为标量" class="headerlink" title="1.2.1 输出为标量"></a>1.2.1 输出为标量</h3><p>首先我们看一段代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line">a=V(t.Tensor([2,3]),requires_grad=True)</span><br><span class="line">b=a+3</span><br><span class="line">c=b*3</span><br><span class="line">out=c.mean()</span><br><span class="line">out.backward()</span><br><span class="line"></span><br><span class="line">print(&quot;a.data\n&quot;,a.data)</span><br><span class="line">print(&quot;a.grad\n&quot;,a.grad)</span><br><span class="line">print(&quot;a.grad_fn\n&quot;,a.grad_fn)</span><br><span class="line">print(&quot;b.data\n&quot;,b.data)</span><br><span class="line">print(&quot;b.grad\n&quot;,b.grad)</span><br><span class="line">print(&quot;b.grad_fn\n&quot;,b.grad_fn)</span><br><span class="line">print(&quot;out.data\n&quot;,out.data)</span><br><span class="line">print(&quot;out.grad\n&quot;,out.grad)</span><br><span class="line">print(&quot;out.grad_fn\n&quot;,out.grad_fn)</span><br></pre></td></tr></table></figure></p><p>对应的函数是:<br>$$out=\frac{3\left [ \left ( a_1 + 3 \right )+\left ( a_2 + 3 \right ) \right ]}{2}=\frac{3\left [ \mathbf{a}+3 \right ]}{2}$$<br>所以\(\mathbf{a}\)向量的梯度为:<br>$$\frac{\partial out}{\partial \mathbf{a}}=(\frac{3}{2},\frac{3}{2})$$<br>真实的运行结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a.data</span><br><span class="line"> tensor([2., 3.])</span><br><span class="line">a.grad</span><br><span class="line"> tensor([1.5000, 1.5000])</span><br><span class="line">a.grad_fn</span><br><span class="line"> None</span><br><span class="line">b.data</span><br><span class="line"> tensor([5., 6.])</span><br><span class="line">b.grad</span><br><span class="line"> None</span><br><span class="line">b.grad_fn</span><br><span class="line"> &lt;AddBackward0 object at 0x000002C22F441A20&gt;</span><br><span class="line">out.data</span><br><span class="line"> tensor(16.5000)</span><br><span class="line">out.grad</span><br><span class="line"> None</span><br><span class="line">out.grad_fn</span><br><span class="line"> &lt;MeanBackward1 object at 0x000002C22F441160&gt;</span><br></pre></td></tr></table></figure></p><p>我们得到以下结论：</p><ul><li>grad_fn表示变量通过怎样的计算方式得到，叶节点变量None</li><li>grad为梯度，中间变量不存储grad，只有叶节点存储。</li><li>data为运行过程中变量的值</li></ul><h3 id="1-2-2-输出为向量"><a href="#1-2-2-输出为向量" class="headerlink" title="1.2.2 输出为向量"></a>1.2.2 输出为向量</h3><h4 id="1-2-2-1-雅可比矩阵的介绍"><a href="#1-2-2-1-雅可比矩阵的介绍" class="headerlink" title="1.2.2.1 雅可比矩阵的介绍"></a>1.2.2.1 雅可比矩阵的介绍</h4><p>&emsp;&emsp;Rn→Rm为一个从欧式n维空间转换到欧式m维空间的函数，并且由m个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn)。若将该函数的偏导数(若存在)组成一个m行n列的矩阵, 那么这个矩阵就是所谓的雅可比矩阵:<br><img src="/2019/09/18/learn-pytorch/yakebi.jpg" alt></p><h4 id="1-2-2-2-实例"><a href="#1-2-2-2-实例" class="headerlink" title="1.2.2.2 实例"></a>1.2.2.2 实例</h4><p>首先我们再看一段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line">a=V(t.Tensor([[2,4]]),requires_grad=True)</span><br><span class="line">b=t.zeros(1,2)</span><br><span class="line">b[0,0]=a[0,0]**2+a[0,1]</span><br><span class="line">b[0,1]=a[0,1]**3+a[0,0]</span><br><span class="line">out=2*b</span><br><span class="line">out.backward(t.Tensor([[1,1]]))</span><br><span class="line">#backward参数与out维度相同</span><br><span class="line"></span><br><span class="line">print(&quot;a.data\n&quot;,a.data)</span><br><span class="line">print(&quot;a.grad\n&quot;,a.grad)</span><br><span class="line">print(&quot;a.grad_fn\n&quot;,a.grad_fn)</span><br><span class="line">print(&quot;b.data\n&quot;,b.data)</span><br><span class="line">print(&quot;b.grad\n&quot;,b.grad)</span><br><span class="line">print(&quot;b.grad_fn\n&quot;,b.grad_fn)</span><br><span class="line">print(&quot;out.data\n&quot;,out.data)</span><br><span class="line">print(&quot;out.grad\n&quot;,out.grad)</span><br><span class="line">print(&quot;out.grad_fn\n&quot;,out.grad_fn)</span><br></pre></td></tr></table></figure><p>对应的函数为：<br>$$\mathbf{out}=2\mathbf{b}=2\left ( \left (a_1\right )^2+a_2,\left (a_2\right )^3+a_1 \right)$$<br>我们求得\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵为：<br>\begin{pmatrix}<br>\frac{\partial out_1}{\partial a_1}=4a_1 &amp; \frac{\partial out_1}{\partial a_2}=2 \\<br>\frac{\partial out_2}{\partial a_1}=2&amp; \frac{\partial out_2}{\partial a_2}=6{a_2}^2<br>\end{pmatrix}<br>也就是：<br>\begin{pmatrix}<br>8 &amp; 2\\<br> 2&amp;96<br>\end{pmatrix}<br>真实的运行结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a.data</span><br><span class="line"> tensor([[2., 4.]])</span><br><span class="line">a.grad</span><br><span class="line"> tensor([[10., 98.]])</span><br><span class="line">a.grad_fn</span><br><span class="line"> None</span><br><span class="line">b.data</span><br><span class="line"> tensor([[ 8., 66.]])</span><br><span class="line">b.grad</span><br><span class="line"> None</span><br><span class="line">b.grad_fn</span><br><span class="line"> &lt;CopySlices object at 0x0000020221827F60&gt;</span><br><span class="line">out.data</span><br><span class="line"> tensor([[ 16., 132.]])</span><br><span class="line">out.grad</span><br><span class="line"> None</span><br><span class="line">out.grad_fn</span><br><span class="line"> &lt;MulBackward0 object at 0x0000020221827C50&gt;</span><br></pre></td></tr></table></figure><p>我们得到以下结论：</p><ul><li>\(\mathbf{a}\)向量的梯度由\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵得出。也就是\( a.grad=(\frac{\partial out_1}{\partial a_1}+\frac{\partial out_2}{\partial a_1}=10 , \frac{\partial out_1}{\partial a_2}+\frac{\partial out_2}{\partial a_2}=98)\),那么这是为什么呢？与backward输入的[[1,1]]参数有何联系？接下来看下一节。</li></ul><h3 id="1-2-3-输出为矩阵：backward（）参数的意义"><a href="#1-2-3-输出为矩阵：backward（）参数的意义" class="headerlink" title="1.2.3 输出为矩阵：backward（）参数的意义"></a>1.2.3 输出为矩阵：backward（）参数的意义</h3><h4 id="1-2-3-1-矩阵求导的介绍"><a href="#1-2-3-1-矩阵求导的介绍" class="headerlink" title="1.2.3.1 矩阵求导的介绍"></a>1.2.3.1 矩阵求导的介绍</h4><p>&emsp;&emsp;基础是利用矩阵微分，复杂结合链式法则。矩阵微分和矩阵的迹有很大的关系，矩阵论中有详细的描述。<br><code>矩阵微分的介绍:https:/www.cnblogs.compinardp/10791506.html</code><br><code>矩阵求导常用公式：https://blog.csdn.net/WPR1991/article/details/82929843</code></p><h4 id="1-2-3-2-实例"><a href="#1-2-3-2-实例" class="headerlink" title="1.2.3.2 实例"></a>1.2.3.2 实例</h4><p>首先看一段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line">x=t.Tensor([[100,200],[101,201]])</span><br><span class="line">w=V(t.Tensor([[0.1],[0.2]]),requires_grad=True)</span><br><span class="line">b=V(t.Tensor([[0.01],[0.02]]),requires_grad=True)</span><br><span class="line">y=t.Tensor([[1],[0]])</span><br><span class="line">out=(t.mm(x,w)+b)-y</span><br><span class="line">out.backward(t.Tensor([[1],[2]]))</span><br><span class="line"></span><br><span class="line">print(&quot;w.data\n&quot;,w.data)</span><br><span class="line">print(&quot;w.grad\n&quot;,w.grad)</span><br><span class="line">print(&quot;w.grad_fn\n&quot;,w.grad_fn)</span><br><span class="line">print(&quot;b.data\n&quot;,b.data)</span><br><span class="line">print(&quot;b.grad\n&quot;,b.grad)</span><br><span class="line">print(&quot;b.grad_fn\n&quot;,b.grad_fn)</span><br><span class="line">print(&quot;out.data\n&quot;,out.data)</span><br><span class="line">print(&quot;out.grad\n&quot;,out.grad)</span><br><span class="line">print(&quot;out.grad_fn\n&quot;,out.grad_fn)</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;我们模拟一个超级简单的网络\(\mathbf{out} ={\left (\mathbf{X}\mathbf{W}+\mathbf{B}\right )}-\mathbf{Y}  \),全部为矩阵。\( \mathbf{X} \)为我们输入的有2个属性的2组数据，\(\mathbf{Y}\)为2组数据的标签，所以这两个只需要Tensor封装，不需要Variable封装，\(\mathbf{out}\)为差损失。\(\mathbf{W}\)与\(\mathbf{B}\)为随机初始化的参数，需要计算梯度并通过梯度下降等方法变化，代码中没有写优化算法，我们只想看看一次反向传播后变量的梯度是多少。理论上可以得出：<br>$$\frac{\partial \mathbf{out}}{\partial \mathbf{W}} = \mathbf{X} =<br>\begin{pmatrix}<br>100 &amp; 200\\<br>101 &amp; 201<br>\end{pmatrix}$$<br>$$\frac{\partial \mathbf{out}}{\partial \mathbf{B}} = \mathbf{I} =<br>\begin{pmatrix}<br>1 &amp; 0\\<br>0 &amp; 1<br>\end{pmatrix}$$</p><p>我们看一下运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">w.data</span><br><span class="line"> tensor([[0.1000],</span><br><span class="line">        [0.2000]])</span><br><span class="line">w.grad</span><br><span class="line"> tensor([[302.],</span><br><span class="line">        [602.]])</span><br><span class="line">w.grad_fn</span><br><span class="line"> None</span><br><span class="line">b.data</span><br><span class="line"> tensor([[0.0100],</span><br><span class="line">        [0.0200]])</span><br><span class="line">b.grad</span><br><span class="line"> tensor([[1.],</span><br><span class="line">        [2.]])</span><br><span class="line">b.grad_fn</span><br><span class="line"> None</span><br><span class="line">out.data</span><br><span class="line"> tensor([[49.0100],</span><br><span class="line">        [50.3200]])</span><br><span class="line">out.grad</span><br><span class="line"> None</span><br><span class="line">out.grad_fn</span><br><span class="line"> &lt;SubBackward0 object at 0x000001E4B0857C18&gt;</span><br></pre></td></tr></table></figure><p>可以看到\(\mathbf{W}\)与\(\mathbf{B}\)的梯度是理论值与backward参数运算的结果.<br>\(\mathbf{W}\)的梯度为<br>\begin{pmatrix}<br>100+101\times 2 \\<br>200+201\times 2<br>\end{pmatrix}<br>也就是<br>\begin{pmatrix}<br>302 \\<br>602<br>\end{pmatrix}<br>\(\mathbf{B}\)和\(\mathbf{W}\)计算方式相同。<br>我们可以得出以下的结论：</p><ul><li>backward的参数的实际意义是：当每个变量在反向传播计算梯度时候，不同的样本将给与不同的权重影响。这个权重就是backward的参数，当参数全部为1，表示所有的样本都一样，意味着拿到一个mini-batch的所有数据平均梯度。backward的参数与y的维度相同。</li></ul><h3 id="1-2-4-利用自动求导找函数的最小值"><a href="#1-2-4-利用自动求导找函数的最小值" class="headerlink" title="1.2.4 利用自动求导找函数的最小值"></a>1.2.4 利用自动求导找函数的最小值</h3><p><code>原来的文章https://blog.csdn.net/weixin_42892943/article/details/94716387</code></p><p>首先我们看一段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line"></span><br><span class="line">def fun(x):</span><br><span class="line">    return (x[0]**2+x[1]-11)**2+(x[0]+x[1]**2-7)**2</span><br><span class="line"></span><br><span class="line">x = V(t.Tensor([0.,0.]),requires_grad=True)</span><br><span class="line">optimizer = t.optim.Adam([x],lr=1e-3)</span><br><span class="line">for step in range(20000):</span><br><span class="line">    pred = fun(x)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    if step%2000 == 0:</span><br><span class="line">        print(&apos;step&#123;&#125;: x = &#123;&#125;, x.grad = &#123;&#125;, f(x) = &#123;&#125;&apos;.format(step,x.toli(),x.grad,pred.item()))</span><br></pre></td></tr></table></figure><p>我们构建了一个函数：<br>$$z=(x^2+y-11)^2+(x+y^2-7)^2$$<br>其图像为：<br><img src="/2019/09/18/learn-pytorch/fun.png" alt><br>我们利用adam算法优化器寻找函数的最小值。结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">step0: x = [0.0009999999310821295, 0.0009999999310821295], x.grad = tensor([-14., -22.]), f(x) = 170.0</span><br><span class="line">step2000: x = [2.3331806659698486, 1.9540692567825317], x.grad = tensor([-35.3487, -13.8643]), f(x) = 13.730920791625977</span><br><span class="line">step4000: x = [2.9820079803466797, 2.0270984172821045], x.grad = tensor([-0.7803,  0.5791]), f(x) = 0.014858869835734367</span><br><span class="line">step6000: x = [2.999983549118042, 2.0000221729278564], x.grad = tensor([-0.0008,  0.0004]), f(x) = 1.1074007488787174e-08</span><br><span class="line">step8000: x = [2.9999938011169434, 2.0000083446502686], x.grad = tensor([-0.0003,  0.0002]), f(x) = 1.5572823031106964e-09</span><br><span class="line">step10000: x = [2.999997854232788, 2.000002861022949], x.grad = tensor([-9.5367e-05,  5.7221e-05]), f(x) = 1.8189894035458565e-10</span><br><span class="line">step12000: x = [2.9999992847442627, 2.0000009536743164], x.grad = tensor([-2.8610e-05,  1.7166e-05]), f(x) = 1.6370904631912708e-11</span><br><span class="line">step14000: x = [2.999999761581421, 2.000000238418579], x.grad = tensor([-9.5367e-06,  5.7220e-06]), f(x) = 1.8189894035458565e-12</span><br><span class="line">step16000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0</span><br><span class="line">step18000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0</span><br></pre></td></tr></table></figure><p>我们可以看到x逐渐接近最低点，梯度在不断的减小。</p><h1 id="二-数据集预处理的自定义构建"><a href="#二-数据集预处理的自定义构建" class="headerlink" title="二 数据集预处理的自定义构建"></a>二 数据集预处理的自定义构建</h1><h2 id="2-1-transforms中的源码"><a href="#2-1-transforms中的源码" class="headerlink" title="2.1 transforms中的源码"></a>2.1 transforms中的源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Normalize(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, mean, std, inplace=False):</span><br><span class="line">        self.mean = mean</span><br><span class="line">        self.std = std</span><br><span class="line">        self.inplace = inplace</span><br><span class="line"></span><br><span class="line">    def __call__(self, tensor):</span><br><span class="line">        return F.normalize(tensor, self.mean, self.std, self.inplace)</span><br><span class="line"></span><br><span class="line">    def __repr__(self):</span><br><span class="line">        return self.__class__.__name__ + &apos;(mean=&#123;0&#125;, std=&#123;1&#125;)&apos;.format(self.mean, self.std)</span><br></pre></td></tr></table></figure><p>以Normalize为例子，我们看到每个处理方式由<strong>init</strong>，<strong>call</strong>，<strong>repr</strong>组成。</p><h2 id="2-2-自定义自己的预处理方式"><a href="#2-2-自定义自己的预处理方式" class="headerlink" title="2.2 自定义自己的预处理方式"></a>2.2 自定义自己的预处理方式</h2><p>&emsp;&emsp;torchvision.transforms的方法都是随机的，如果样本和标签都是图片，需要转动相同的随机角度。需要自己来定义预处理方式。这时候可以借用torchvision.transforms.functional来构造自己的函数，同时处理样本标签，得到新的样本标签后再分开处理。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms.functional as TF</span><br><span class="line">import torchvision.transforms as T </span><br><span class="line"></span><br><span class="line">#自定义旋转角度：方法一，二</span><br><span class="line">class FixedRotation(object):</span><br><span class="line">    def __init__(self, startangle,endangle):</span><br><span class="line">        self.startangle = startangle</span><br><span class="line">        self.endangle = endangle</span><br><span class="line"></span><br><span class="line">    def __call__(self,img):</span><br><span class="line">        return  my_transforms1(img,startangle,endangle)</span><br><span class="line">        # return  my_transforms2(img,startangle,endangle)</span><br><span class="line"></span><br><span class="line">    def __repr__(self):</span><br><span class="line">    return self.__class__.__name__ +&apos;from&#123;&#125;to&#123;&#125;&apos;.format(startangle,endangle)</span><br><span class="line"></span><br><span class="line">def my_transforms1(img, startangle,endangle):</span><br><span class="line">    angle = random.randint(startangle,endangle)</span><br><span class="line">    image = img.rotate(angles[angle])</span><br><span class="line">    return image</span><br><span class="line"></span><br><span class="line">def my_transforms2(image):</span><br><span class="line">    angle = random.randint(-30, 30)</span><br><span class="line">    image = TF.rotate(image, angle)</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure><h1 id="三-神经网络的自定义构建"><a href="#三-神经网络的自定义构建" class="headerlink" title="三 神经网络的自定义构建"></a>三 神经网络的自定义构建</h1><h2 id="3-1-详解nn-module"><a href="#3-1-详解nn-module" class="headerlink" title="3.1 详解nn.module"></a>3.1 详解nn.module</h2><p>&emsp;&emsp;这里先看一下nn.module类源码中重要的属性方法，这里放一段代码，等下下面会用到，我们建立了一个前传后的钩子函数，一个有纯参数组成的module，一个由submodule组成的module，并用这两个module组成个复杂的大module。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import torch as t </span><br><span class="line">from torch import nn </span><br><span class="line">from torch.nn import functional as F</span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line"></span><br><span class="line">def for_hook(module, input, output):</span><br><span class="line">    print(module)</span><br><span class="line">    for val in input:</span><br><span class="line">        print(&quot;input val:&quot;,val)</span><br><span class="line">    for out_val in output:</span><br><span class="line">        print(&quot;output val:&quot;, out_val)</span><br><span class="line"></span><br><span class="line">class Linear(nn.Module): # 继承nn.Module</span><br><span class="line">    def __init__(self, in_features, out_features):</span><br><span class="line">        super(Linear, self).__init__() # 等价于nn.Module.__init__(self)</span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x.mm(self.w) # x.@(self.w)</span><br><span class="line">        return x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line">class MyLinear(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyLinear, self).__init__()</span><br><span class="line">        self.subsubmodule=Linear(3,3)</span><br><span class="line">        self.subsubmodule.register_forward_hook(for_hook)</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.subsubmodule(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.register_buffer(&quot;buf1&quot;,t.ones(3,3)) #为module注册一个缓存区</span><br><span class="line"></span><br><span class="line">        self.register_parameter(&quot;param1&quot;,nn.Parameter(t.ones(3,3))) #与下一句等价，注册一个参数，本质就是一个变量</span><br><span class="line">        self.param2 = nn.Parameter(t.rand(3, 3))</span><br><span class="line"></span><br><span class="line">        self.add_module(&quot;submodel1&quot;,nn.Linear(3, 3)) #与下一句等价，注册一个子module</span><br><span class="line">        self.submodel2 = nn.Linear(3, 3) </span><br><span class="line"></span><br><span class="line">        self.submodel3 = MyLinear()</span><br><span class="line">        self.modulelist1 = nn.ModuleList([nn.Linear(3,3),nn.Linear(3,3)])</span><br><span class="line">        self.modulelist2 = nn.Sequential(nn.Linear(3,3),nn.Linear(3,3))</span><br><span class="line">        self.bn = nn.BatchNorm1d(3)</span><br><span class="line">    def forward(self, input):</span><br><span class="line">        x = input.mm(self.param1)</span><br><span class="line">        x = self.submodel1(x)</span><br><span class="line">        x = self.submodel3(x)</span><br><span class="line">        # x = self.modulelist1(x) error</span><br><span class="line">        x = self.modulelist2(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        return x</span><br><span class="line">        </span><br><span class="line">net = Net()</span><br><span class="line">x=t.rand(2,3)</span><br><span class="line">y=net(x)</span><br></pre></td></tr></table></figure></p><h3 id="3-1-1-属性"><a href="#3-1-1-属性" class="headerlink" title="3.1.1 属性"></a>3.1.1 属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Module(object):</span><br><span class="line">def __init__(self):</span><br><span class="line">    self._parameters = OrderedDict()</span><br><span class="line">    self._modules = OrderedDict()</span><br><span class="line">    self._buffers = OrderedDict()</span><br><span class="line">    self._backward_hooks = OrderedDict()</span><br><span class="line">    self._forward_hooks = OrderedDict()</span><br><span class="line">    self.training = True</span><br><span class="line">    self._forward_pre_hooks = OrderedDict()</span><br><span class="line">    self._state_dict_hooks = OrderedDict()</span><br><span class="line">    self._load_state_dict_pre_hooks = OrderedDict()</span><br></pre></td></tr></table></figure><p>解释一下重要的几个属性，我把它分为三类。</p><ul><li>用于存储数据的：<ul><li>_parameters：字典，保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为’param’，value为对应parameter的item。不会查看到子module的参数。</li><li>_modules：子module，通过self.submodel = nn.Linear(3, 4)指定的子module会保存于此。同时子moudle可以自己定义。</li><li>_buffers：缓存，每个moudle都可以注册自己的缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。</li></ul></li><li>用于是否继续向前传播的:<ul><li>training：通过判断training值来决定正向传播策略。</li></ul></li><li>钩子函数来实现对前传,后传,保存,回复等操作的触发,_backward_hooks,_forward_hooks,_forward_pre_hooks，_state_dict_hooks，_load_state_dict_pre_hooks这些字典主要用于存储钩子。</li></ul><p>我们看几个属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">net._buffers:</span><br><span class="line"> OrderedDict([(&apos;buf1&apos;, tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]]))])</span><br><span class="line"></span><br><span class="line">net._modules:</span><br><span class="line"> OrderedDict([(&apos;submodule1&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule2&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule3&apos;, MyLinear(</span><br><span class="line">  (subsubmodule): Linear()</span><br><span class="line">)), (&apos;modulelist1&apos;, ModuleList(</span><br><span class="line">  (0): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">  (1): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">)), (&apos;modulelist2&apos;, Sequential(</span><br><span class="line">  (0): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">  (1): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">)), (&apos;bn&apos;, BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))])</span><br><span class="line"></span><br><span class="line">net._parameters:</span><br><span class="line"> OrderedDict([(&apos;param1&apos;, Parameter containing:</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], requires_grad=True)), (&apos;param2&apos;, Parameter containing:</span><br><span class="line">tensor([[0.3535, 0.6803, 0.7144],</span><br><span class="line">        [0.2985, 0.1329, 0.2111],</span><br><span class="line">        [0.3999, 0.0395, 0.1407]], requires_grad=True))])</span><br><span class="line"></span><br><span class="line">net.training:</span><br><span class="line"> True</span><br><span class="line"></span><br><span class="line">net.submodule3.subsubmodule._forward_hooks:</span><br><span class="line"> OrderedDict([(0, &lt;function for_hook at 0x0000028C74F23E18&gt;)])</span><br></pre></td></tr></table></figure></p><h3 id="3-1-2-方法"><a href="#3-1-2-方法" class="headerlink" title="3.1.2 方法"></a>3.1.2 方法</h3><p>根据属性我们来分开介绍一些重要的nn.moudle的重要方法。</p><h4 id="3-1-2-1-缓存，子模型，参数的设置，操作，查看的方法"><a href="#3-1-2-1-缓存，子模型，参数的设置，操作，查看的方法" class="headerlink" title="3.1.2.1 缓存，子模型，参数的设置，操作，查看的方法"></a>3.1.2.1 缓存，子模型，参数的设置，操作，查看的方法</h4><ul><li>self.register_buffer（name[string],buf[Tensor]）</li><li>self.register_parameter（name[string],param[nn.Parameter]）等价与self.name=param</li><li>self.add_module（name[string],submodule[nn.Module]） 等价于self.name=submodule</li><li>self.named_parameters() 生成器，产生所有参数的name与param</li><li>self.named_buffers() 生成器，产生所有参数的name与buf</li><li>self.named_module() 生成器，产生所有参数的name与module，包括module与子module</li><li>self.named_children() 生成器，产生所有参数的name与submodule</li><li>self.cuda() 转移到cuda上</li><li>self.cpu() 转移到cpu上</li><li>self.float() 参书变换</li><li><p>self.to()  多态性</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.. function:: to(device=None, dtype=None, non_blocking=False)</span><br><span class="line"></span><br><span class="line">.. function:: to(dtype, non_blocking=False)</span><br><span class="line"></span><br><span class="line">.. function:: to(tensor, non_blocking=False)</span><br></pre></td></tr></table></figure></li><li><p>self.apply(fn) 源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def apply(self, fn):</span><br><span class="line">    for module in self.children():</span><br><span class="line">        module.apply(fn)</span><br><span class="line">    fn(self)</span><br><span class="line">    return self</span><br></pre></td></tr></table></figure></li></ul><p>我们看一些方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">net.named_parameters()所有参数名称:</span><br><span class="line">param1</span><br><span class="line">param2</span><br><span class="line">submodule1.weight</span><br><span class="line">submodule1.bias</span><br><span class="line">submodule2.weight</span><br><span class="line">submodule2.bias</span><br><span class="line">submodule3.subsubmodule.w</span><br><span class="line">submodule3.subsubmodule.b</span><br><span class="line">modulelist1.0.weight</span><br><span class="line">modulelist1.0.bias</span><br><span class="line">modulelist1.1.weight</span><br><span class="line">modulelist1.1.bias</span><br><span class="line">modulelist2.0.weight</span><br><span class="line">modulelist2.0.bias</span><br><span class="line">modulelist2.1.weight</span><br><span class="line">modulelist2.1.bias</span><br><span class="line">bn.weight</span><br><span class="line">bn.bias</span><br><span class="line"></span><br><span class="line">net.named_modules()所有模型名称:</span><br><span class="line">submodule1</span><br><span class="line">submodule2</span><br><span class="line">submodule3</span><br><span class="line">submodule3.subsubmodule</span><br><span class="line">modulelist1</span><br><span class="line">modulelist1.0</span><br><span class="line">modulelist1.1</span><br><span class="line">modulelist2</span><br><span class="line">modulelist2.0</span><br><span class="line">modulelist2.1</span><br><span class="line">bn</span><br><span class="line"></span><br><span class="line">net.named_children()所有子模型名称:</span><br><span class="line">submodule1</span><br><span class="line">submodule2</span><br><span class="line">submodule3</span><br><span class="line">modulelist1</span><br><span class="line">modulelist2</span><br><span class="line">bn</span><br><span class="line"></span><br><span class="line">net.named_buffers()所有缓存名称:</span><br><span class="line">buf1</span><br><span class="line">bn.running_mean</span><br><span class="line">bn.running_var</span><br><span class="line">bn.num_batches_tracked</span><br></pre></td></tr></table></figure><h4 id="3-2-1-2-传播的方法"><a href="#3-2-1-2-传播的方法" class="headerlink" title="3.2.1.2 传播的方法"></a>3.2.1.2 传播的方法</h4><ul><li>self.train() #使用Dropout与BN层训练时开启</li><li>self.eval() #使用Dropout与BN层测试时开启</li><li>self.zero_grad() #清空所有参数的梯度</li></ul><h4 id="3-2-1-3-钩子的方法"><a href="#3-2-1-3-钩子的方法" class="headerlink" title="3.2.1.3 钩子的方法"></a>3.2.1.3 钩子的方法</h4><ul><li>self.register_forward_hook(hook) 设立该module前传后的钩子</li><li>self.register_backward_hook(hook) 设立该module后传后的钩子</li><li>self.register_forward_pre_hook(hook) 设立该module前传前的钩子</li></ul><p>正向传播时候当钩子的函数监控的module发生正向传播，触发钩子函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Linear()</span><br><span class="line">input val: tensor([[ 0.3910, -0.9092, -0.9559],</span><br><span class="line">        [ 0.4495, -0.9752, -1.0354]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line">output val: tensor([ 1.3830,  1.3145, -0.7691], grad_fn=&lt;SelectBackward&gt;)</span><br><span class="line">output val: tensor([ 1.5223,  1.4328, -0.7066], grad_fn=&lt;SelectBackward&gt;)</span><br></pre></td></tr></table></figure><h2 id="3-3-自定义Function实现网络层"><a href="#3-3-自定义Function实现网络层" class="headerlink" title="3.3 自定义Function实现网络层"></a>3.3 自定义Function实现网络层</h2><h2 id="3-4-需要辨析注意的点"><a href="#3-4-需要辨析注意的点" class="headerlink" title="3.4 需要辨析注意的点"></a>3.4 需要辨析注意的点</h2><h3 id="3-4-1-nn-Parameter与Variable的关系"><a href="#3-4-1-nn-Parameter与Variable的关系" class="headerlink" title="3.4.1 nn.Parameter与Variable的关系"></a>3.4.1 nn.Parameter与Variable的关系</h3><h3 id="3-4-2-nn-ModuleList与nn-Sequential-区别"><a href="#3-4-2-nn-ModuleList与nn-Sequential-区别" class="headerlink" title="3.4.2 nn.ModuleList与nn.Sequential()区别"></a>3.4.2 nn.ModuleList与nn.Sequential()区别</h3><h3 id="3-4-3-何时使用from-torch-nn-import-functional-as-F中的F"><a href="#3-4-3-何时使用from-torch-nn-import-functional-as-F中的F" class="headerlink" title="3.4.3 何时使用from torch.nn import functional as F中的F"></a>3.4.3 何时使用from torch.nn import functional as F中的F</h3><h1 id="四-损失函数的构建"><a href="#四-损失函数的构建" class="headerlink" title="四 损失函数的构建"></a>四 损失函数的构建</h1><h2 id="4-1-xx源码"><a href="#4-1-xx源码" class="headerlink" title="4.1 xx源码"></a>4.1 xx源码</h2><h1 id="五-多GPU应用与部署"><a href="#五-多GPU应用与部署" class="headerlink" title="五 多GPU应用与部署"></a>五 多GPU应用与部署</h1><h1 id="六-参考"><a href="#六-参考" class="headerlink" title="六 参考"></a>六 参考</h1>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一-利用Variable自动求导&quot;&gt;&lt;a href=&quot;#一-利用Variable自动求导&quot; class=&quot;headerlink&quot; title=&quot;一 利用Variable自动求导&quot;&gt;&lt;/a&gt;一 利用Variable自动求导&lt;/h1&gt;&lt;h2 id=&quot;1-1-Variable&quot;&gt;&lt;a href=&quot;#1-1-Variable&quot; class=&quot;headerlink&quot; title=&quot;1.1 Variable&quot;&gt;&lt;/a&gt;1.1 Variable&lt;/h2&gt;&lt;h3 id=&quot;1-1-1-定义&quot;&gt;&lt;a href=&quot;#1-1-1-定义&quot; class=&quot;headerlink&quot; title=&quot;1.1.1 定义&quot;&gt;&lt;/a&gt;1.1.1 定义&lt;/h3&gt;&lt;p&gt;&amp;emsp;&amp;emsp;在pytorch中，我们需要能够构建计算图的 tensor，这就是 Variable数据结构。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身.data，对应 tensor 的梯度.grad以及这个 Variable 是通过什么方式得到的.grad_fn。&lt;/p&gt;
&lt;h3 id=&quot;1-1-2-特性&quot;&gt;&lt;a href=&quot;#1-1-2-特性&quot; class=&quot;headerlink&quot; title=&quot;1.1.2 特性&quot;&gt;&lt;/a&gt;1.1.2 特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;requires_grad&lt;br&gt;变量可以有梯度，求导。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;volatile&lt;br&gt;主要以用于inference过程中。若是某个过程，从 x 开始 都只需做预测，不需反传梯度的话，那么只需设置x.volatile=True ,那么 x 以后的运算过程的输出均为 volatile==True ,即 requires_grad==False。虽然inference 过程不必backward(),所以requires_grad 的值为False 或 True，对结果是没有影响的，但是对程序的运算效率有直接影响；所以使用volatile=True ,就不必把运算过程中所有参数都手动设一遍requires_grad = False 了，方便快捷。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="pytorch进阶学习" scheme="http://yoursite.com/categories/pytorch%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>基本操作二：写作技巧</title>
    <link href="http://yoursite.com/2019/08/24/second2/"/>
    <id>http://yoursite.com/2019/08/24/second2/</id>
    <published>2019-08-24T08:37:06.000Z</published>
    <updated>2019-08-24T10:55:41.788Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><p>###有关符号编译出错<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123; -&gt; &amp;<span class="comment">#123; — 大括号左边部分Left curly brace</span></span><br><span class="line">&#125; -&gt; &amp;<span class="comment">#125; — 大括号右边部分Right curly brace</span></span><br><span class="line">空格 -&gt; &amp;ensp;</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>###有关图片<br>在blog（hexo）目录下Git Bash Here，运行hexo n “博客名”来生成md博客时，会在_post目录下看到一个与博客同名的文件夹。按照如下格式则可以插入图片<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![你想要输入的替代文字](second2/test.jpg)</span><br></pre></td></tr></table></figure></p><p>测试效果图：<br><img src="/2019/08/24/second2/test.jpg" alt="你想要输入的替代文字"></p><p>###有关音乐<br>在网易云音乐中生成外链音乐播放器。如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=<span class="string">"no"</span> border=<span class="string">"0"</span> marginwidth=<span class="string">"0"</span> marginheight=<span class="string">"0"</span> width=330 height=86 src=<span class="string">"//music.163.com/outchain/player?type=2&amp;id=541326593&amp;auto=1&amp;height=66"</span>&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure></p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=541326593&auto=1&height=66"></iframe>  <p>###有关视频<br>在优酷中生成外链视频播放器。如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=<span class="string">"0"</span> src=<span class="string">"https://v.qq.com/txp/iframe/player.html?vid=i0031n20390"</span> allowFullScreen=<span class="string">"true"</span>&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure></p><iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=i0031n20390" allowfullscreen="true"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写作&quot;&gt;&lt;a href=&quot;#写作&quot; class=&quot;headerlink&quot; title=&quot;写作&quot;&gt;&lt;/a&gt;写作&lt;/h2&gt;&lt;p&gt;###有关符号编译出错&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; -&amp;gt; &amp;amp;&lt;span class=&quot;comment&quot;&gt;#123; — 大括号左边部分Left curly brace&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125; -&amp;gt; &amp;amp;&lt;span class=&quot;comment&quot;&gt;#125; — 大括号右边部分Right curly brace&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;空格 -&amp;gt; &amp;amp;ensp;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客操作" scheme="http://yoursite.com/categories/hexo%E5%8D%9A%E5%AE%A2%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="博客" scheme="http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>基本操作一：部署命令</title>
    <link href="http://yoursite.com/2018/12/02/hello-world/"/>
    <id>http://yoursite.com/2018/12/02/hello-world/</id>
    <published>2018-12-02T12:49:01.616Z</published>
    <updated>2019-08-24T08:33:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a><br><a id="more"></a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files-and-compress"><a href="#Generate-static-files-and-compress" class="headerlink" title="Generate static files and compress"></a>Generate static files and compress</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ gulp</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a><br><!--more--></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/writing.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Writing&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客操作" scheme="http://yoursite.com/categories/hexo%E5%8D%9A%E5%AE%A2%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="博客" scheme="http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
</feed>
