<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>过了星期天的博客</title>
  
  <subtitle>同是海角沉溺堕落人</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-21T13:27:00.113Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>星期八</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PRML：线性回归模型</title>
    <link href="http://yoursite.com/2019/11/21/LinearModelsForRegression/"/>
    <id>http://yoursite.com/2019/11/21/LinearModelsForRegression/</id>
    <published>2019-11-21T12:32:28.000Z</published>
    <updated>2019-11-21T13:27:00.113Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-回归问题的定义"><a href="#1-回归问题的定义" class="headerlink" title="1 回归问题的定义"></a>1 回归问题的定义</h1><p>数据定义<br>\( Data={X,\boldsymbol{t}},X=[\boldsymbol{x_1}…\boldsymbol{x_n}]^T,\boldsymbol{x_1}\epsilon R^p,\boldsymbol{t}=[t_1…t_n],t_1 \epsilon R \)<br><a id="more"></a><br>需要解决的问题<br>\( y=\boldsymbol{w}^T\boldsymbol{x},t=y+\varepsilon ,<br>\varepsilon \sim \mathit{N}(0,\sigma^2) \)</p><p>对于一个新的数据 \( x^{* } \),我们想要得到 \ ( y^{* } \),在频率派中得到的\ ( y^{* } \)。</p><h1 id="2-频率派思想"><a href="#2-频率派思想" class="headerlink" title="2 频率派思想"></a>2 频率派思想</h1><h2 id="2-1-损失期望-偏置-2-方差-噪音"><a href="#2-1-损失期望-偏置-2-方差-噪音" class="headerlink" title="2.1 损失期望=偏置^2+方差+噪音"></a>2.1 损失期望=偏置^2+方差+噪音</h2><p>首先我们知道一个损失函数的期望为如下形式：<br><img src="https://s2.ax1x.com/2019/10/17/KkxmZ9.png" alt><br>我们采用平方差损失：<br><img src="https://s2.ax1x.com/2019/10/17/KAdhl9.png" alt></p><p>我们定义h(x)为真实估计函数，y(x)为拟合函数，t为观测值。我们可以将损失函数化作如下形式：<br><img src="https://s2.ax1x.com/2019/10/17/KAwMkT.png" alt><br>考虑对于多个数据集的情况，将第一项继续拆分可得<br><img src="https://s2.ax1x.com/2019/10/17/KA0RM9.png" alt><br>综上可得<br><img src="https://s2.ax1x.com/2019/10/17/KA0XqI.png" alt><br>我们可以得到如下结论：</p><ul><li>偏置刻画的是构建的模型和真实模型之间的差异，学习算法本身的拟合能力。</li><li>方差刻画的是构建的模型自身的稳定性，多个训练集的变动所导致的学习性能的变化。</li><li>噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</li><li>泛化能力由这三个共同组成。</li><li>损失期望的偏差方差分解需要多个数据集。</li></ul><h2 id="2-2-频率派求解模型"><a href="#2-2-频率派求解模型" class="headerlink" title="2.2 频率派求解模型"></a>2.2 频率派求解模型</h2><p>这里本质上就是寻找w的值。</p><h3 id="2-2-1-最小二乘估计-LSE"><a href="#2-2-1-最小二乘估计-LSE" class="headerlink" title="2.2.1 最小二乘估计 LSE"></a>2.2.1 最小二乘估计 LSE</h3><p>求解w<br>\( \boldsymbol{w^{*}}=argmin \quad (\boldsymbol{t}-X\boldsymbol{w})^T(\boldsymbol{t}-X\boldsymbol{w})=(X^TX)^{-1}X^T\boldsymbol{t} \)<br>模型预测<br>\( t^{* }=(\boldsymbol{x}^{* })^T\boldsymbol{w^{*}} \)</p><h3 id="2-2-2-极大似然估计-MLE"><a href="#2-2-2-极大似然估计-MLE" class="headerlink" title="2.2.2 极大似然估计 MLE"></a>2.2.2 极大似然估计 MLE</h3><p><img src="https://s2.ax1x.com/2019/11/21/MIkebQ.png" alt="MIkebQ.png"><br>求解w<br>和w有关的项与极小二乘法的损失函数一样。<br>\( \boldsymbol{w^{*}}=argmin \quad (\boldsymbol{t}-X\boldsymbol{w})^T(\boldsymbol{t}-X\boldsymbol{w})=(X^TX)^{-1}X^T\boldsymbol{t} \)<br>同时可以得出<br><img src="https://s2.ax1x.com/2019/10/16/KiZYPH.png" alt><br>模型预测<br>\( t^{* }=(\boldsymbol{x}^{* })^T\boldsymbol{w^{*}} \)</p><h3 id="2-2-3-对于解析解的几何解释"><a href="#2-2-3-对于解析解的几何解释" class="headerlink" title="2.2.3 对于解析解的几何解释"></a>2.2.3 对于解析解的几何解释</h3><p><img src="https://s2.ax1x.com/2019/11/21/MItAkn.png" alt="MItAkn.png">也就是t乘以投影矩阵等于y。<br>\( (X(X^TX)^{-1}X^T)(\boldsymbol{t}) = (X\boldsymbol{w}) \)</p><h3 id="2-2-4-带有正则项的LSE"><a href="#2-2-4-带有正则项的LSE" class="headerlink" title="2.2.4 带有正则项的LSE"></a>2.2.4 带有正则项的LSE</h3><p>加入正则项以后变成了如下方式，q=1为lasso回归，q=2为岭回归，分别意味着增加L1正则化和L2正则化，或者说增加L1范数，L2范数，正则化系数人为设定。<br><img src="https://s2.ax1x.com/2019/10/16/KF2gG6.png" alt></p><p>正则项的作用：<br>L1也可以防止过拟合，主要作用是用于构建稀疏权值矩阵，用于特征选择。<br>L2正则可以防止奇异矩阵，也就是防止求伪逆时候的矩阵不可逆，防止过拟合。<br>L2没有稀疏的作用是因为没有棱角。具体参考文献。</p><h1 id="3-贝叶斯派的思想"><a href="#3-贝叶斯派的思想" class="headerlink" title="3 贝叶斯派的思想"></a>3 贝叶斯派的思想</h1><h2 id="3-1-参数后验为点估计（不完全的贝叶斯派）"><a href="#3-1-参数后验为点估计（不完全的贝叶斯派）" class="headerlink" title="3.1 参数后验为点估计（不完全的贝叶斯派）"></a>3.1 参数后验为点估计（不完全的贝叶斯派）</h2><h3 id="3-1-1-模型推断"><a href="#3-1-1-模型推断" class="headerlink" title="3.1.1 模型推断"></a>3.1.1 模型推断</h3><p>就是MAP最大后验法（附加高斯噪音），这时候我们对于参数的后验概率只用一个值来代替。当参数先验服从高斯分布，参数的后验概率等同于最小二乘法+L2正则项的结果，当参数先验服从Lapalce分布，参数的后验概率等同于最小二乘法+L1正则项的结果。我们观察，当参数先验服从高斯分布。利用极大似然法求解下面式子可以得到参数后验的点估计<br>\(  \boldsymbol{w^{*}} = argmax \quad p(\boldsymbol{w}|\boldsymbol{t},X) \)<br>\(  \boldsymbol{w^{*}} = argmax \quad p(\boldsymbol{t}|\boldsymbol{w},X)p(\boldsymbol{w}) \)<br>其中有<br>\( \boldsymbol{t}|\boldsymbol{w},X \sim N(X\boldsymbol{w},\Sigma_1 ) \)<br>等价于:<br><img src="https://s2.ax1x.com/2019/10/17/KEVXPU.png" alt><br>\( \boldsymbol{w} \sim N(\boldsymbol{\mu},\Sigma_2)\)<br>一般设置为：<br>\( p(\boldsymbol{w}) \sim N(0,\alpha^{-1} I) \)<br>极大似然函数为<br>\( \boldsymbol{w^{*}} = argmax \quad \prod_i^{100} (N(\boldsymbol{w}^T x_i,\beta^{-1}) N(0,\alpha^{-1}))^{t_i} \)</p><p>所以有<br>\( \boldsymbol{w^{*}} = (\alpha^{-1} I + X^TX)^{-1} X^T \boldsymbol{t} \)</p><h3 id="3-1-2-模型预测"><a href="#3-1-2-模型预测" class="headerlink" title="3.1.2 模型预测"></a>3.1.2 模型预测</h3><p>\( t^{* } \sim N({\boldsymbol{x}^{* }}^T\boldsymbol{w^{*}},\beta^{-1})\)</p><h2 id="3-2-参数后验完全是概率分布"><a href="#3-2-参数后验完全是概率分布" class="headerlink" title="3.2 参数后验完全是概率分布"></a>3.2 参数后验完全是概率分布</h2><h3 id="3-2-1-模型推导"><a href="#3-2-1-模型推导" class="headerlink" title="3.2.1 模型推导"></a>3.2.1 模型推导</h3><p>这次我们要寻找到真正的\( p(\boldsymbol{w}|\boldsymbol{t},X) \)分布，忽略分母积分常量<br>\( p(\boldsymbol{w}|\boldsymbol{t},X) \propto p(\boldsymbol{t}|\boldsymbol{w},X)p(\boldsymbol{w})\)。<br>其中有 \( p(\boldsymbol{t}|\boldsymbol{w},X) \)为：</p><p><img src="https://s2.ax1x.com/2019/10/17/KEVXPU.png" alt><br>其中有 \( p(\boldsymbol{w}) \) 为：<br>\( p(\boldsymbol{w}) \sim N(0,\alpha^{-1} I) \)<br>经过计算可得<br>于是我们可以写出后验：<br>\( \boldsymbol{w}|\boldsymbol{t},X \sim N(\boldsymbol{m_N},S_N) \)<br><img src="https://s2.ax1x.com/2019/11/21/MIXyex.png" alt="MIXyex.png"></p><h3 id="3-2-2-模型预测"><a href="#3-2-2-模型预测" class="headerlink" title="3.2.2 模型预测"></a>3.2.2 模型预测</h3><p>\( p(t|\boldsymbol{t},X,\boldsymbol{x^{*}})=\int p(t|\boldsymbol{x^{*}},\boldsymbol{w},\beta)p(\boldsymbol{w}|\boldsymbol{t},X,\alpha,\beta) d\boldsymbol{w} \)<br>也就是<br>\( p(t|\boldsymbol{t},X,\boldsymbol{x^{*}}) = N(\boldsymbol{m_N}^T\boldsymbol{x^{*}},\beta^{-1}+{\boldsymbol{x^{*}}}^TS_N\boldsymbol{x^{*}}) \)<br>也就是<br>\( t^{* } \sim N(\boldsymbol{m_N}^T\boldsymbol{x^{*}},\beta^{-1}+{\boldsymbol{x^{*}}}^TS_N\boldsymbol{x^{*}}) \)</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>正交投影：<code>https://www.cnblogs.com/mfrbuaa/p/5319365.html</code><br>L1与L2正则化的区别：<code>https://blog.csdn.net/TXBSW/article/details/79073933</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-回归问题的定义&quot;&gt;&lt;a href=&quot;#1-回归问题的定义&quot; class=&quot;headerlink&quot; title=&quot;1 回归问题的定义&quot;&gt;&lt;/a&gt;1 回归问题的定义&lt;/h1&gt;&lt;p&gt;数据定义&lt;br&gt;\( Data={X,\boldsymbol{t}},X=[\boldsymbol{x_1}…\boldsymbol{x_n}]^T,\boldsymbol{x_1}\epsilon R^p,\boldsymbol{t}=[t_1…t_n],t_1 \epsilon R \)&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://yoursite.com/categories/PRML/"/>
    
    
      <category term="PRML" scheme="http://yoursite.com/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML：线性分类模型</title>
    <link href="http://yoursite.com/2019/11/18/LinearModelsForClassfication/"/>
    <id>http://yoursite.com/2019/11/18/LinearModelsForClassfication/</id>
    <published>2019-11-18T07:55:28.000Z</published>
    <updated>2019-11-18T09:28:04.906Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h1><p>本章主要从判别函数（硬分类）与概率模型（软分类）两个角度解决二分类与多分类的问题。<br><a id="more"></a></p><h1 id="2-判别函数（硬分类）"><a href="#2-判别函数（硬分类）" class="headerlink" title="2 判别函数（硬分类）"></a>2 判别函数（硬分类）</h1><p>简单来说，硬分类就是直接寻找决策平面，二分类下判别函数就是I(f(x))，f(x)&gt;0属于正类，f(x)&lt;0属于负类，x·-&gt;{-1，1}。多分类就是寻找多个分类相互的决策平面。</p><h2 id="2-1-二分类与多分类的决策平面性质"><a href="#2-1-二分类与多分类的决策平面性质" class="headerlink" title="2.1 二分类与多分类的决策平面性质"></a>2.1 二分类与多分类的决策平面性质</h2><h3 id="2-1-1-二分类"><a href="#2-1-1-二分类" class="headerlink" title="2.1.1 二分类"></a>2.1.1 二分类</h3><p><img src="https://s2.ax1x.com/2019/11/05/MSJRAg.png" alt="MSJRAg.png"><br>我们需要记得如下图<br><img src="https://s2.ax1x.com/2019/11/05/MSJ6nf.png" alt="MSJ6nf.png"><br>此时一个D-1维的超平面经过D维的空间。<br>我们将w0整合到w,可以得到一个D维度的超平面，经过D+1维度的空间的原点。<br><img src="https://s2.ax1x.com/2019/11/05/MStUWd.png" alt="MStUWd.png"></p><h3 id="2-1-2-多分类"><a href="#2-1-2-多分类" class="headerlink" title="2.1.2 多分类"></a>2.1.2 多分类</h3><p><img src="https://s2.ax1x.com/2019/11/05/MSadSJ.png" alt="MSadSJ.png"></p><h2 id="2-1-最小平方法（学习判别函数参数【寻找决策平面】的方法一）"><a href="#2-1-最小平方法（学习判别函数参数【寻找决策平面】的方法一）" class="headerlink" title="2.1 最小平方法（学习判别函数参数【寻找决策平面】的方法一）"></a>2.1 最小平方法（学习判别函数参数【寻找决策平面】的方法一）</h2><p>这个方法只适合简单的线性函数，如果线性函数上加了激活函数则不适用了。二分类情况下y为单变量，求最小二乘法的时候可以求得y的解析值x的伪逆乘以t。我们考虑多分类的情况。多分类时候，我们使用onehot编码，也可以得到精确的解析解。</p><p><a href="https://imgchr.com/i/MCUqBR" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/06/MCUqBR.md.png" alt="MCUqBR.md.png"></a></p><h2 id="2-2-Fisher线性判别分析法（学习判别函数参数【寻找决策平面】的方法二）"><a href="#2-2-Fisher线性判别分析法（学习判别函数参数【寻找决策平面】的方法二）" class="headerlink" title="2.2 Fisher线性判别分析法（学习判别函数参数【寻找决策平面】的方法二）"></a>2.2 Fisher线性判别分析法（学习判别函数参数【寻找决策平面】的方法二）</h2><p>Fisher线性判别函数是从降维的角度出发，同时这个方法只适合简单的线性函数。思想是最大化一个函数，能够让类均值的在低维度空间投影分开较大，同时让每个类内部的方差较小，从而减少类别的重叠。<br><a href="https://imgchr.com/i/MCgfED" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/06/MCgfED.md.png" alt="MCgfED.md.png"></a></p><p>有<br>\( \boldsymbol{m}_k = \frac{1}{N_k} \sum \boldsymbol{x_n} \)</p><p>\( \boldsymbol{m}_k \) 是第k类的均值。那么有\( m_k \)是投影值。</p><p>$$<br>m_k=\boldsymbol{w}^T\boldsymbol{m}_k<br>$$</p><p>类内方差有<br><img src="https://s2.ax1x.com/2019/11/06/MCguB8.png" alt="MCguB8.png"><br>如果是二分类的话这样我们可以构建最大化函数J(w)<br><img src="https://s2.ax1x.com/2019/11/06/MC2n2R.png" alt="MC2n2R.png"><br>推广到多分类略</p><h2 id="2-3-感知器算法（学习判别函数参数【寻找决策平面】的方法三）"><a href="#2-3-感知器算法（学习判别函数参数【寻找决策平面】的方法三）" class="headerlink" title="2.3 感知器算法（学习判别函数参数【寻找决策平面】的方法三）"></a>2.3 感知器算法（学习判别函数参数【寻找决策平面】的方法三）</h2><p>相比于前面两种方法，感知器算法求解判别函数的参数，从容错的角度出发，利用了一个激活函数。如下文：<br><a href="https://imgchr.com/i/MVnH00" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/08/MVnH00.md.png" alt="MVnH00.md.png"></a><br><a href="https://imgchr.com/i/MVnLkT" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/08/MVnLkT.md.png" alt="MVnLkT.md.png"></a></p><h1 id="3-概率模型"><a href="#3-概率模型" class="headerlink" title="3 概率模型"></a>3 概率模型</h1><p>这里将从概率的角度看分类问题。这里便是软分类，我们不直接寻找决策平面，而是对比概率值，二分类只需要看y=1的概率值，x·-&gt;（0，1）。多分类的话需要对比多个分类概率值，最大的就是我们认为的分类。这里会有两种模型概率生成式模型与概率判别式模型。</p><h2 id="3-1-概率生成式模型VS概率判别式模型。"><a href="#3-1-概率生成式模型VS概率判别式模型。" class="headerlink" title="3.1 概率生成式模型VS概率判别式模型。"></a>3.1 概率生成式模型VS概率判别式模型。</h2><p>在概率生成式模型中，我们先对类条件概率密度p(x|Ck)和类先验概率分布p(Ck)建模，后使用这两个概率密度通过贝叶斯定理计算后验概率p（Ck|x），典型的案例就是logistics回归。而概率判别式模型，我们直接对于P(Ck|x)建模，典型的模型就是朴素贝叶斯模型和高斯判别模型。</p><h2 id="3-2-概率生成式模型"><a href="#3-2-概率生成式模型" class="headerlink" title="3.2 概率生成式模型"></a>3.2 概率生成式模型</h2><p>概率生成模型中我们先对类条件概率密度p(x|Ck)和类先验概率分布p(Ck)建模，后使用这两个概率密度（也就是联合概率密度）通过贝叶斯定理计算后验概率p（Ck|x）。</p><h3 id="3-2-1-sigmoid与softmax函数"><a href="#3-2-1-sigmoid与softmax函数" class="headerlink" title="3.2.1 sigmoid与softmax函数"></a>3.2.1 sigmoid与softmax函数</h3><p>这里从概率生成模型的角度引出了sigmoid与softmax函数，本身这两个函数是概率生成模型转化后的函数形式，解释了logistics模型中参数，体现了概率生成模型更强的解释性。</p><p>sigmoid函数如下：<br><a href="https://imgchr.com/i/MeruNV" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/09/MeruNV.md.png" alt="MeruNV.md.png"></a><br>其中a也等价与如下形式，a被称为logit函数，表示两类概率的比值的ln值：<br><img src="https://s2.ax1x.com/2019/11/09/MerNAx.png" alt="MerNAx.png"><br>softmax：<br><a href="https://imgchr.com/i/MesQat" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/09/MesQat.md.png" alt="MesQat.md.png"></a></p><h3 id="3-2-2-高斯判别模型（Gaussian-discriminant-classifier）"><a href="#3-2-2-高斯判别模型（Gaussian-discriminant-classifier）" class="headerlink" title="3.2.2 高斯判别模型（Gaussian discriminant classifier）"></a>3.2.2 高斯判别模型（Gaussian discriminant classifier）</h3><p>不要被名字迷惑，这是个概率生成模型</p><h4 id="3-2-2-1-模型定义"><a href="#3-2-2-1-模型定义" class="headerlink" title="3.2.2.1 模型定义"></a>3.2.2.1 模型定义</h4><p>假设我们的类条件概率p（x|Ck）为高斯分布，且每一类的协方差矩阵相同。在二分类中类先验概率为p(Ck)服从伯努利分布，多分类中先验服从mutionoulli（catetorical）范畴分布。来看二分类的sigmoid与多分类的softmax。类条件概率密度分布如下：<br><img src="https://s2.ax1x.com/2019/11/09/Me6Aje.png" alt="Me6Aje.png"><br>在二分类sigmoid中，a为 \( \boldsymbol{w}^T\boldsymbol{x}+w_0\)<br>即<br><a href="https://imgchr.com/i/Me6su4" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/09/Me6su4.md.png" alt="Me6su4.md.png"></a></p><p>拓展到多分类的softmax。<br><a href="https://imgchr.com/i/MecEGV" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/09/MecEGV.md.png" alt="MecEGV.md.png"></a></p><p>由于我们假设类条件概率密度分布都具有相同的协方差矩阵，所以决策边界是线性的，当具有不同的协方差，边界是二次的。而先验只是决定决策边界的移动。</p><h4 id="3-2-2-2-最大似然解"><a href="#3-2-2-2-最大似然解" class="headerlink" title="3.2.2.2 最大似然解"></a>3.2.2.2 最大似然解</h4><p>一旦我们具体化了类条件概率密度和类先验概率密度的形式，利用极大似然法就可以求解类先验概率和类条件概率的参数。书上举了低维的高斯分布与伯努利分布作为类条件概率密度和先验概率密度。</p><h3 id="3-2-3-朴素贝叶斯模型（Naive-Bayes-classifier）"><a href="#3-2-3-朴素贝叶斯模型（Naive-Bayes-classifier）" class="headerlink" title="3.2.3 朴素贝叶斯模型（Naive Bayes classifier）"></a>3.2.3 朴素贝叶斯模型（Naive Bayes classifier）</h3><p>一般来说朴素贝叶斯分类器是离散的，广意的朴素贝叶斯就是连续的高斯判别模型和离散的朴素贝叶斯结合。</p><p>我们认为特征向量\( \boldsymbol{x} \)中每一个特征值 \( x_i \)都是独立的，这就是朴素贝叶斯假设。根据条件概率p(X=x|Y=c_k)的假设分布不同,分为伯努利假设是伯努利分布(其实应该是多变量伯努利分布),多项式假设是多项式分布,而高斯也就是假设是高斯分布(其实是多变量高斯分布)。</p><p>首先我们观察伯努利假设\( \boldsymbol{x} = [x_1,x_2 … x_d] \) 中\( x_i\epsilon{0，1} \)，\( x_i \sim Ber(\mu)\)。这里可以理解为以一篇文章中所有单词有和无作为特征，\( \boldsymbol{x} \)特征向量长度为文章中词汇的数量d。最终形式：<br><img src="https://s2.ax1x.com/2019/11/18/M6NDAg.gif" alt="M6NDAg.gif"><br>\( p(C_k|\textbf{x}) = \frac{p(x_i,x_2\dots x_d|C_k)p(C_k)}{p(\boldsymbol{x})} = \frac{[\prod_{i = 1}^{d}{\mu_{ki}}^{x_i}{(1-{\mu_{ki}})}^{1-x_i}]p(C_k)}{p(\boldsymbol{x})} \)</p><p>接着我们观察多项式假设，，\( \boldsymbol{x}=[x_i,x_2 \dots x_d] \),且\( \boldsymbol{x} \sim Multi(n,\boldsymbol{\mu}) \),其中\( \mu = [\mu_1, …, \mu_d] \)。可以理解为，一篇文章中所有单词有n个，每个单词可能出现0-d种词汇（词典大小为d）。所以\( x_i,x_2 \dots x_d \)代表着大小为d的词典中每个词出现的频率。最终形式：<br><img src="https://s2.ax1x.com/2019/11/18/M6N7C9.gif" alt="M6N7C9.gif"><br>\(p(C_k|\textbf{x})=\frac{Multi(n,\boldsymbol{\mu})p(C_k)}{p(\boldsymbol{x})}=\frac{[\frac{n!}{x_1!x_2! \dots x_d!}\prod_{i=1}^d {\mu_{ki}}^{x_i}]p(C_k)}{p(\boldsymbol{x})}\)</p><p>最后的高斯假设就是高斯判别分析。</p><p>一般情况在朴素贝叶斯中还会加入拉普拉斯平滑防止出现错误，就是为每个类计算时候+1。</p><h3 id="3-2-4-指数族分布下的概率生成模型"><a href="#3-2-4-指数族分布下的概率生成模型" class="headerlink" title="3.2.4 指数族分布下的概率生成模型"></a>3.2.4 指数族分布下的概率生成模型</h3><p>我们把类条件概率密度分布视作指数族分布形式，可以推导出二分类中，a和多分类中ak更加一般的形式。</p><h2 id="3-3-概率判别式模型"><a href="#3-3-概率判别式模型" class="headerlink" title="3.3 概率判别式模型"></a>3.3 概率判别式模型</h2><h3 id="3-3-1-固定基函数"><a href="#3-3-1-固定基函数" class="headerlink" title="3.3.1 固定基函数"></a>3.3.1 固定基函数</h3><p>基函数实现原空间非线性的分割到基函数空间的线性分割。</p><h3 id="3-3-2-logistic回归（典型的频率派思想）"><a href="#3-3-2-logistic回归（典型的频率派思想）" class="headerlink" title="3.3.2 logistic回归（典型的频率派思想）"></a>3.3.2 logistic回归（典型的频率派思想）</h3><h4 id="3-3-2-1-logistic回归定义"><a href="#3-3-2-1-logistic回归定义" class="headerlink" title="3.3.2.1 logistic回归定义"></a>3.3.2.1 logistic回归定义</h4><p>这里对于sigmoid函数，我们构建时候不再从生成式模型那样从类条件概率和类先验概率的角度出发。而是直接用\( \boldsymbol{w}^T\phi  \)，这样的基函数。即<br><img src="https://s2.ax1x.com/2019/11/11/MMLpsU.png" alt="MMLpsU.png"><br>需要求解的极大似然函数，或者说损失函数为：</p><p><a href="https://imgchr.com/i/MMLof1" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/11/MMLof1.md.png" alt="MMLof1.md.png"></a></p><h4 id="3-2-2-2-求解参数"><a href="#3-2-2-2-求解参数" class="headerlink" title="3.2.2.2 求解参数"></a>3.2.2.2 求解参数</h4><p>这里涉及到求解w，这里和最小二乘法的求解区别是要是多了sigmoid函数在基函数上，但是不再有解析解了，可以利用梯度下降法和牛顿法。</p><p>梯度下降法的梯度为：<br><img src="https://s2.ax1x.com/2019/11/11/MMzj78.png" alt="MMzj78.png"></p><p>牛顿法的介绍以及应用于最小二乘法：<br><img src="https://s2.ax1x.com/2019/11/11/MQSu9J.png" alt="MQSu9J.png"></p><p>牛顿法应用于求解logistic回归的损失函数：</p><p><img src="https://s2.ax1x.com/2019/11/11/MQ99Lq.png" alt="MQ99Lq.png"><br><img src="https://s2.ax1x.com/2019/11/11/MQ9nyR.png" alt="MQ9nyR.png"></p><p>但是极大似然法会导致过拟合严重，如果使用优化算法如梯度下降法和牛顿法，无法判别哪个解更加好。通过加入参数先验把极大似然法变成最大后验法（等价于为损失函数加上正则项）来减少过拟合。</p><h3 id="3-3-3-多分类的logistic回归"><a href="#3-3-3-多分类的logistic回归" class="headerlink" title="3.3.3 多分类的logistic回归"></a>3.3.3 多分类的logistic回归</h3><p>多分类使用softmax函数。<br><img src="https://s2.ax1x.com/2019/11/11/MQFJEt.png" alt="MQFJEt.png"><br>在生成式模型中我们通过极大似然估计法求解类条件概率和类先验概率的参数，隐形确定了模型的参数。而这里我们直接对模型的参数使用极大似然法。<br><img src="https://s2.ax1x.com/2019/11/11/MQiuOs.png" alt="MQiuOs.png"><br>下一步则是取log得出损失函数然后利用优化算法求解参数。<br><img src="https://s2.ax1x.com/2019/11/11/MQiXBn.png" alt="MQiXBn.png"></p><h3 id="3-3-4-probit回归"><a href="#3-3-4-probit回归" class="headerlink" title="3.3.4 probit回归"></a>3.3.4 probit回归</h3><p>和logistics回归差不多的一个回归，他的激活函数（逆probit回归函数）形式为<br>\( f(a)=\int_{-\infty }^{a}\mathcal N(\theta|0,1)d\theta \)</p><h3 id="3-3-5-标准链接函数"><a href="#3-3-5-标准链接函数" class="headerlink" title="3.3.5 标准链接函数"></a>3.3.5 标准链接函数</h3><p>标准链结函数这一章先介绍了一个概念：链结函数是激活函数的反函数。分析t有高斯噪音模型到对于t属于指数分布族情况的推广，从而得到更加一般的形式。</p><h1 id="4-拉普拉斯近似"><a href="#4-拉普拉斯近似" class="headerlink" title="4 拉普拉斯近似"></a>4 拉普拉斯近似</h1><h2 id="4-1-方法介绍"><a href="#4-1-方法介绍" class="headerlink" title="4.1 方法介绍"></a>4.1 方法介绍</h2><p>由于后验概率分布的计算依赖于先验概率分布函数、似然概率分布函数，当这二者共轭时，后验概率与先验概率服从相同的分布函数，从而可以推导计算出后验概率分布(posterior could be computed analytically)。但是，当这二者不共轭时，则是计算后验概率分布的近似值。计算近似值一共有三种方法:</p><ul><li>点估计法(point estimate — MAP)</li><li>拉普拉斯近似法</li><li>Metropolis-Hastings采样法。</li></ul><p>点估计法(point estimate — maximum a posteriori)其实就是最大后验法，而拉普拉斯近似法这里将介绍，采样法后面会介绍。<br>点估计法是最简单的近似，简单来说点估计是：<br>\( p(\boldsymbol{w}|\boldsymbol{t},\boldsymbol{X}) \approx max( p(\boldsymbol{t}|\boldsymbol{X},\boldsymbol{w})p(\boldsymbol{w}|\theta ) )=max( \prod_{n=1}^N p(t_n|\boldsymbol{x}_n, \boldsymbol{w}) p(\boldsymbol{w}|\boldsymbol{\theta}) ) \)</p><p>在精准计算参数后验概率时候，需要对先验和似然函数乘积归一化（就是贝叶斯公式里的分母，需要求积分），但是积分无法求出，贝叶斯推导无法进行，则需要拉普拉斯近似是：<br>\( p(\boldsymbol{w}|\boldsymbol{t},\boldsymbol{X}) \approx N(\boldsymbol{u},\boldsymbol{A^{-1}} ) \)</p><p>那么拉普拉斯是怎么近似的呢？首先我们有数据集，明确先验和似然函数，我们把后验概率写成如下形式</p><p>\(  p(\boldsymbol{w}|\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})=\frac{p(\boldsymbol{t}|\boldsymbol{w},\boldsymbol{X}) p(\boldsymbol{w}|\boldsymbol{\theta})} { p(\boldsymbol{t})} \)</p><p>把该式子先log，可以看到log后的极大似然函数的展开形式如下：<br>\( \ln p(\boldsymbol{w}|\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})=\ln p(\boldsymbol{t}|\boldsymbol{w},\boldsymbol{X})+\ln p(\boldsymbol{w}|\boldsymbol{\theta})-\ln p(\boldsymbol{t}) \)</p><p>将极大似然函数在一个局部最大值（对于\( \boldsymbol{w} \)一阶求导等于0，二阶小于0或者负定，通常可以选择最大后验法中的\( \boldsymbol{\hat{w}}\)，也就是\（ w_{MAP}\)）处泰勒展开可以得到：<br>$$<br>\ln p(\boldsymbol{w};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) \approx \ln p(\boldsymbol{\hat{w}};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})+0-<br>\frac{1}{2}(\boldsymbol{w}-\boldsymbol{\hat{w}})^T\boldsymbol{A}(\boldsymbol{w}-\boldsymbol{\hat{w}})<br>$$<br>其中有精度矩阵A为\( \ln p(\boldsymbol{w}|\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) \)对w二阶求导，只和该式子前两项有关，与归一化常数无关，我们可以利用\( \ln p(\boldsymbol{t}|\boldsymbol{w},\boldsymbol{X})+\ln p(\boldsymbol{w}|\boldsymbol{\theta}) \)对对w二阶求导得到精度矩阵。<br>$$<br>\boldsymbol{A}=-\frac{d^{2}}{dw^2} \ln p(\boldsymbol{w};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta})\bigg|_{w=\hat{w}}<br>$$<br>用exp还原近似的式子为<br>$$<br>p(\boldsymbol{w};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) \approx p(\boldsymbol{\hat{w}};\boldsymbol{X},\boldsymbol{t},\boldsymbol{\theta}) exp (-\frac{1}{2}(\boldsymbol{w}-\boldsymbol{\hat{w}})^T\boldsymbol{A}(\boldsymbol{w}-\boldsymbol{\hat{w}}))<br>$$<br>对比高斯分布密度函数可以得出\( \boldsymbol{A}\)为精度，而均值\(\boldsymbol{u}\) 为\( \boldsymbol{\hat{w}}\)。那么对比高斯分布密度函数非指数部分，我们也可以得到归一化常数，当然此时已经没必要求了,我们还是利用如下式子把他求出来。<br>\( \frac{|\boldsymbol{A}|^{\frac{1}{2}}}{({2\pi})^{\frac{M}{2}}} = \ln p(\boldsymbol{t}|\boldsymbol{\hat{w}},\boldsymbol{X})+\ln p(\boldsymbol{\hat{w}}|\boldsymbol{\theta})-\ln p(\boldsymbol{t}) \)</p><p>但是拉普拉斯主要依据高斯分布来做的，这是其局限性，特别是现实中很多分布是多峰的，不同的众数会导致不同的拉普拉斯近似。</p><h2 id="4-2-贝叶斯派logistics回归"><a href="#4-2-贝叶斯派logistics回归" class="headerlink" title="4.2 贝叶斯派logistics回归"></a>4.2 贝叶斯派logistics回归</h2><p>分为两个阶段，模型的定义，参数的求解，利用模型推断。</p><h3 id="4-2-1-模型的定义与求解"><a href="#4-2-1-模型的定义与求解" class="headerlink" title="4.2.1  模型的定义与求解"></a>4.2.1  模型的定义与求解</h3><p>我们假设似然函数为logistics回归函数，先验服从高斯分布如下，数据集已知：<br><img src="https://s2.ax1x.com/2019/11/18/M63uXF.png" alt="M63uXF.png"><br>那么利用拉普拉斯近似参数的后验概率为：<br><img src="https://s2.ax1x.com/2019/11/18/M68BKU.png" alt="M68BKU.png"><br><img src="https://s2.ax1x.com/2019/11/18/M68yVJ.png" alt="M68yVJ.png"></p><h3 id="4-2-3-模型推断"><a href="#4-2-3-模型推断" class="headerlink" title="4.2.3 模型推断"></a>4.2.3 模型推断</h3><p>对于新的预测数据，我们便可以用参数后验概率来推断，但是该积分不好求，具体的求法参考书籍。<br><img src="https://s2.ax1x.com/2019/11/18/M6GQiR.png" alt="M6GQiR.png"></p><h2 id="模型比较与BIC"><a href="#模型比较与BIC" class="headerlink" title="模型比较与BIC"></a>模型比较与BIC</h2><p>未完待续</p><h1 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5 参考文献"></a>5 参考文献</h1><p>牛顿法与迭代重加权最小平方：<code>https://blog.csdn.net/xuanyuansen/article/details/41050507</code><br>拉普拉斯近似：<code>https://www.cnblogs.com/hapjin/p/8834794.html</code>,<code>https://www.cnblogs.com/hapjin/p/8834794.html</code><br>朴素贝叶斯：<code>https://www.jianshu.com/p/b6cadf53b8b8</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1 概述&quot;&gt;&lt;/a&gt;1 概述&lt;/h1&gt;&lt;p&gt;本章主要从判别函数（硬分类）与概率模型（软分类）两个角度解决二分类与多分类的问题。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://yoursite.com/categories/PRML/"/>
    
    
      <category term="PRML" scheme="http://yoursite.com/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML：概率分布</title>
    <link href="http://yoursite.com/2019/10/22/ProbabilityDistributions/"/>
    <id>http://yoursite.com/2019/10/22/ProbabilityDistributions/</id>
    <published>2019-10-22T12:35:00.000Z</published>
    <updated>2019-10-22T12:40:38.844Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h1><ul><li><strong>共轭先验（conjugate prior）</strong>：如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。所有指数家族的分布都有共轭先验。</li><li><a id="more"></a></li><li>这章节其实就是在讲似然函数有哪些分布，对应的共轭先验分布。这些似然函数分布都有参数，但是分为两钟参数，一种控制些似然函数分布的参数，一种控制模型的复杂度。此外详细介绍了高斯分布。</li></ul><h1 id="2-似然函数分布及其共轭先验分布"><a href="#2-似然函数分布及其共轭先验分布" class="headerlink" title="2 似然函数分布及其共轭先验分布"></a>2 似然函数分布及其共轭先验分布</h1><ul><li>二项分布作为似然函数，共轭先验为beta分布。</li><li>多项式分布作为似然函数，共轭先验为狄利克雷分布。</li><li>高斯分布作为似然函数，共轭先验为高斯分布。</li></ul><h1 id="3-高斯分布（重要）"><a href="#3-高斯分布（重要）" class="headerlink" title="3 高斯分布（重要）"></a>3 高斯分布（重要）</h1><h2 id="3-1-什么是高斯分布？"><a href="#3-1-什么是高斯分布？" class="headerlink" title="3.1 什么是高斯分布？"></a>3.1 什么是高斯分布？</h2><p>如果把一个高斯分布分为两部分，指数部分和非指数部分。非指数部分用于描述<strong>概率密度衰减的速度</strong>（在二维情况下图中蓝线的下降速度），而指数部分描述了<strong>大多数样本点</strong>在一个<strong>超椭球面</strong>（在二维情况下图中红色的椭圆）的内。最开始的一段先描述了马氏距离概念，也就是对高斯分布的指数部分。<br><img src="https://s2.ax1x.com/2019/10/10/uTRl4O.png" alt><br>那么这个<strong>超椭球面</strong>如何得出的？<br>先看一下高斯分布的公式<br>$$<br>p({\bf x})=\frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^\frac{1}{2}}exp{-\frac{1}{2}({\bf x-\mu})^T{\Sigma}^{-1}({\bf x-\mu})}<br>$$</p><p>协方差矩阵公式<br>$$<br>\Sigma=E{(\bf x-\bf \mu)(\bf x - \mu)^T}<br>$$<br>很明显，协方差公式是实对称矩阵，利用特征分解相似对角化，并施密特正交化可得：<br>$$<br>{\bf\Sigma}^{-1}={\bf U^{-T}\Lambda^{-1}U^{-1}}={\bf U\Lambda^{-1}U^T}=\sum_{i=1}^d\frac{1}{\lambda_i}{\bf u}_i{\bf u}_i^T<br>$$</p><p>因此高斯分布指数部分的公式可以写成<br>$$<br>({\bf x-\mu})^T{\Sigma}^{-1}({\bf x-\mu})=({\bf x-\mu})^T\left(\sum_{i=1}^d\frac{1}{\lambda_i}{\bf u}_i{\bf u}_i^T\right)({\bf x-\mu})\<br>$$</p><p>也就是<br>$$<br>\sum_{i=1}^d\frac{1}{\lambda_i}({\bf x-\mu})^T{\bf u}_i{\bf u}_i^T({\bf x-\mu})\<br>$$<br>这个形式其实已经相当明朗了，这是一个超椭球面，椭球面的中心为\( \bf \mu \),而轴有d条，每条轴的方向为\( \bf u_i \),大小为\( \lambda_i \)<br>在二维情况下，绝大多数样本在红色的椭圆内，也就是超椭球面，而对应的两个\( \lambda \)特征值就是轴的长度。<br>在一维情况下，绝大多数样本在\( \mu - \Sigma \) 到\( \mu + \Sigma \)，这时候的超椭球面退化成了一条直线，\( \Sigma \)也就是轴的长度。<br>接着描述了多维高斯分布可以转化为一元高斯分布的乘积形式。<br>文章最后观察了\( \mu \)，\( \Sigma \)的矩，以及高斯分布参数多，单峰的缺点。</p><h2 id="3-2-条件高斯分布"><a href="#3-2-条件高斯分布" class="headerlink" title="3.2 条件高斯分布"></a>3.2 条件高斯分布</h2><p>首先引入精度矩阵（precision matrix）的概念,它是协方差举证的逆。这一节主要推导，如果两节变量是联合高斯分布，那么以一组变量为条件，另外一组变量同样是高斯分布。对于联合概率密度p(Xa,Xb),只需要将Xb确定，那么便得到条件概率密度p（Xa|Xb），对比高斯分布指数分布的一般形式，我们可以得到条件概率密度p（Xa|Xb）的\( \mu \)，\( \Sigma \)。推导过程主要是分块矩阵变换，没有进行推导，有需要再来吧。</p><h2 id="3-3-边缘高斯分布"><a href="#3-3-边缘高斯分布" class="headerlink" title="3.3 边缘高斯分布"></a>3.3 边缘高斯分布</h2><p>因为积分只对指数部分有用，我们单独拿出指数部分来做分析，将指数部分编程变量为Xb的一般形式，然后求积分，对比标准的高斯分布，可以得出边缘概率密度也是高斯分布，且可以的得到边缘概率密度的\( \mu \)，\( \Sigma \)。<br><img src="https://s2.ax1x.com/2019/10/11/uqqXYF.png" alt></p><h2 id="3-4-高斯分布的极大似然估计"><a href="#3-4-高斯分布的极大似然估计" class="headerlink" title="3.4 高斯分布的极大似然估计"></a>3.4 高斯分布的极大似然估计</h2><p>因为数据集的每个数据变量符合高斯分布且独立，所以这些变量的概率密度函数连乘的形式可以写成联合概率密度形式，也就是高维度的高斯分布，接着取log，求导=0，寻找令联合概率密度极大值的 \( \mu \)，\( \Sigma \)。</p><h2 id="3-5-高斯分布的贝叶斯推断"><a href="#3-5-高斯分布的贝叶斯推断" class="headerlink" title="3.5 高斯分布的贝叶斯推断"></a>3.5 高斯分布的贝叶斯推断</h2><p>高斯分布的贝叶斯推断主要要讲了，当似然函数为一个高斯分布。\( \Sigma^2 \)的逆矩阵\( \Lambda \)称为精度。</p><ul><li>一维情况下：如果 \( \mu \)未知，\( \Sigma^2 \)已知，那么先验概率（共轭先验）为一个高斯分布，后验概率也是高斯分布。</li><li>一维情况下：如果 \( \mu \)已知，\( \Sigma^2 \)未知，那么先验概率（共轭先验）为一个Gamma分布，后验概率也是Gamma分布。</li><li>一维情况下：如果 \( \mu \)未知，\( \Sigma^2 \)未知，那么先验概率（共轭先验）为一个Gaussian-Gamma分布，后验概率也是Gaussian-Gamma分布。</li><li>高维情况下：如果 \( \mu \)未知，\( \Lambda^-1 \)已知，那么先验概率（共轭先验）为一个高斯分布，后验概率也是高斯分布。</li><li>高维情况下：如果 \( \mu \)已知，\( \Lambda^-1 \)未知，那么先验概率（共轭先验）为一个Wishart分布，后验概率也是Wishart分布。</li><li>高维情况下：如果 \( \mu \)未知，\( \Lambda^-1 \)未知，那么先验概率（共轭先验）为一个Gaussian-Wishart分布，后验概率也是Gaussian-Wishart分布。</li></ul><h2 id="3-6-学生t分布"><a href="#3-6-学生t分布" class="headerlink" title="3.6 学生t分布"></a>3.6 学生t分布</h2><p>比高斯分布有更长的尾巴，具有更好的“鲁棒性”。没有进行推导，有需要再来吧。</p><h2 id="3-7-高斯混合分布"><a href="#3-7-高斯混合分布" class="headerlink" title="3.7 高斯混合分布"></a>3.7 高斯混合分布</h2><p>高斯混合分布多了一个参数混合系数，极大似然估计无法直接得出解析解，可以使用数值迭代优化。</p><h1 id="4-指数簇分布"><a href="#4-指数簇分布" class="headerlink" title="4 指数簇分布"></a>4 指数簇分布</h1><ul><li><strong>指数簇分布</strong>： 无论是二项分布，多项式分布，高斯分布都可以转化为固定的形式，我们称为指数簇分布。</li><li><strong>最大似然与充分统计量</strong>：我们可以根据指数簇分布来进行极大似然估计，得到参数的固定求解形式。求解的结果只对数据的固定形式产生依赖，我们将固定的数据形式称为充分统计量。</li><li><strong>共轭先验</strong>：通过指数簇分布的似然函数，我们可以看到共轭先验以及后验形式相同。</li><li><strong>无信息先验</strong>：很直接，我们不希望先验给后验提供信息帮助。这里提及了常数先验和反常先验都可以实现无信息的先验。</li></ul><h1 id="5-非参数化模型（参数用于控制模型的复杂度）"><a href="#5-非参数化模型（参数用于控制模型的复杂度）" class="headerlink" title="5 非参数化模型（参数用于控制模型的复杂度）"></a>5 非参数化模型（参数用于控制模型的复杂度）</h1><p>这时候的模型角度从频率派出发，不利用有关数据分布的先验知识，对数据分布不附加任何假定，包括直方图，核方法和最邻近等，是一种从数据样本本身出发研究数据分布特征的方法。这里讨论的随机变量在0-1之间。</p><ul><li><strong>直方图</strong>：简单，缺点是不适合高纬度且不连续。</li><li><strong>核方法</strong>：核方法是直方图的更高阶形式，我们把每个数据点看作一个以其本身为原点的区域（超立方体）。当预测新的数据点时候x_new，我们计算该数据点距离原来数据集上每一点x_old的距离（不平滑的话就是是否在x_old超立方体范围内），并将这些值平均作为x_new的概率密度。距离的计算可以看作是核函数。</li><li><strong>最邻近</strong>：计算x_new与各个x_old距离，确定K值，K个最小的距离的x_old标签决定x_new取值。</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>共轭先验的理解：<code>https://baike.baidu.com/item/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83/15696678?fr=aladdin</code></li><li>极大似然估计与贝叶斯估计的理解(对于先验，后验，似然函数的认识)：<code>https://blog.csdn.net/liu1194397014/article/details/52766760</code></li><li>高斯分布理解1 <code>https://www.zhihu.com/question/36339816/answer/67043318</code></li><li>高斯分布理解2 <code>https://www.cnblogs.com/shouhuxianjian/p/9773121.html</code></li><li>核密度估计 <code>https://www.jianshu.com/p/249e5ff97c04</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1 概述&quot;&gt;&lt;/a&gt;1 概述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;共轭先验（conjugate prior）&lt;/strong&gt;：如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。所有指数家族的分布都有共轭先验。&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://yoursite.com/categories/PRML/"/>
    
    
      <category term="PRML" scheme="http://yoursite.com/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>PRML：绪论</title>
    <link href="http://yoursite.com/2019/10/06/Introduction/"/>
    <id>http://yoursite.com/2019/10/06/Introduction/</id>
    <published>2019-10-06T15:14:11.000Z</published>
    <updated>2019-10-07T12:49:20.212Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概率论相关概念的通俗理解："><a href="#1-概率论相关概念的通俗理解：" class="headerlink" title="1 概率论相关概念的通俗理解："></a>1 概率论相关概念的通俗理解：</h1><p>这里将我看到知识点作通俗的表达，可以加深自己的理解，详细的案例在书籍《PRML》。<br><a id="more"></a></p><ol><li><strong>概率</strong>：作为贝叶斯派的概率应该理解为：事件的随机性，而频率派的理解是：可重复事件再次发生的可能性。</li><li><strong>概率密度</strong>：我们将概率这一概念与质量作类比。考虑一个密度分布不均匀的小球，总质量为1，概率密度就相当于这个小球某处的密度，值是可以大于1的，但是这个密度乘以体积所得的质量（也就是概率）是恒小于等于1的。然后至于概率密度越大的点，说明单位体积落在该点的质量越大（也就是发生这个点附近事件的概率越大）。</li><li><strong>期望，方差，协方差，协方差矩阵</strong>：从频率派的角度来看：期望是度量一个随机变量取值的集中位置或平均水平的最基本的数字特征，方差是表示随机变量取值的分散性的一个数字特征。协方差是度量两个变量之间的线性相关性。协方差矩阵是半正定或者正定矩阵（矩阵乘以矩阵转置必定为实对称矩阵，且半正定或者正定），用于衡量变量不同维度的线性相关性。至于说贝叶斯派的理解一直没找到，可能是不用这样的说法。</li><li><strong>贝叶斯概率以及贝叶斯估计的思想</strong>：参考文献第二点。</li><li><strong>最大似然估计的局限</strong>：首先最大似然估计是根据一个数据集进行，数据集期望的无偏估计值是真实分布的期望，而数据集方差的无偏估计值并不是真实分布的方差，比真实值偏小，这是导致过度拟合的主要原因。在公式中这种偏差的体现就是：每一个数据集中样本计算方差是根据当前数据集的期望并非是真实分布的期望。同时表达出一种思想：数据集越大，数据集方差无偏估计值越接近真实分布的方差。</li></ol><h1 id="2-模型选择，维度灾难"><a href="#2-模型选择，维度灾难" class="headerlink" title="2 模型选择，维度灾难"></a>2 模型选择，维度灾难</h1><ol><li><strong>交叉验证</strong>：一种针对较小数据集的验证手段，有K折，留一等方法。</li><li><strong>信息准则</strong>：作为一种度量不同模型的的准则，更客观表现出每个模型的优良程度。更客观的原因是相比于直接依靠最大似然函数来评判模型好坏。AIC，BIC等对于每个模型，通过加入惩罚项来减低过拟合的风险，修正最大似然函数值，用修正的最大似然函数值来评判模型好坏。</li><li><strong>维度灾难</strong>：随着维度的增加，我们模型需要的参数快速增加，使得模型变得笨拙不可利用。</li></ol><h1 id="3-决策论"><a href="#3-决策论" class="headerlink" title="3 决策论"></a>3 决策论</h1><ol><li><strong>推断和决策过程</strong>：一个问题的求解我们将其分为推断和决策部分。推断部分我们可以理解为建立模型的过程，而决策部分理解为通过模型做出结论过程。</li><li><strong>最小化错误分类率</strong>：这是做分类问题决策最直接原始的想法，就是所有的样本分类全部正确实现最小化的错误分类，以此作为决策目标。</li><li><strong>最小化期望损失</strong>：但是考虑到不同的分类错误带来的影响不一样，我们引入了损失矩阵，并构建损失函数来表示损失，所有样本的损失期望值作为决策目标。</li><li><strong>拒绝选项</strong>：设定一个阈值来拒绝对一些样本决策，当该样本的后验概率中最大值依旧小于这个阈值，那拒绝对这个样本决策。</li><li><strong>分类问题中推断与决策的三种方式</strong>：<ul><li>求解联合概率，从而得到后验概率，利用决策论与后验概率决策。先求联合概率再求后验概率我们称为生成式模型。</li><li>直接求解后验概率，利用决策论与后验概率决策。我们称这样的模型为判别式模型。</li><li>找到一个判别函数将输入直接映射到标签。</li></ul></li><li><strong>回归问题的损失函数</strong>：和分类问题十分相似，只是没了损失矩阵，我们通过损失函数来计算损失所有样本的损失期望值作为决策目标。</li><li><strong>回归问题中推断与决策的三种方式</strong>：与回归问题相似。</li><li><strong>求解后验概率的必要性</strong>：实际上我们有很多理由需要求解后验概率<ul><li>最小化风险</li><li>拒绝选项：后验概率可以让我们设定拒绝的阈值，而直接映射到标签，则无法选择拒绝。</li><li>补偿先验概率</li><li>组合模型</li></ul></li></ol><h1 id="4-信息论"><a href="#4-信息论" class="headerlink" title="4 信息论"></a>4 信息论</h1><ol><li><strong>信息论的基本假设</strong>：越不可能发生的事情发生了那么这件事信息量大。</li><li><strong>自信息与香农熵</strong>：自信息是事情发生了这件事信息量，香农熵（当变量连续时候被称为微分熵）体现了一个变量的信息量。其实熵这个字的本义表示事件的混乱程度。</li><li><strong>KL散度与交叉熵</strong> KL散度用于表示不同分布的的相似性，书中用了jensen来证明KL散度的性质。交叉熵的意义等价于KL散度，通常我们衡量真实分布和自己建模的分布KL散度，但是真实分布无法得知其概率密度函数，此时用交叉熵来表示。</li></ol><h1 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5 参考文献"></a>5 参考文献</h1><ul><li>概率密度的理解：<a href="https://www.zhihu.com/question/263467674" target="_blank" rel="noopener">https://www.zhihu.com/question/263467674</a></li><li>贝叶斯估计的思想：<a href="https://wenku.baidu.com/view/ea06a887e45c3b3566ec8b33.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/ea06a887e45c3b3566ec8b33.html</a></li><li>协方差矩阵的理解：<a href="https://www.zhihu.com/question/24283387/answer/27294834" target="_blank" rel="noopener">https://www.zhihu.com/question/24283387/answer/27294834</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-概率论相关概念的通俗理解：&quot;&gt;&lt;a href=&quot;#1-概率论相关概念的通俗理解：&quot; class=&quot;headerlink&quot; title=&quot;1 概率论相关概念的通俗理解：&quot;&gt;&lt;/a&gt;1 概率论相关概念的通俗理解：&lt;/h1&gt;&lt;p&gt;这里将我看到知识点作通俗的表达，可以加深自己的理解，详细的案例在书籍《PRML》。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://yoursite.com/categories/PRML/"/>
    
    
      <category term="PRML" scheme="http://yoursite.com/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>pytorch进阶学习</title>
    <link href="http://yoursite.com/2019/09/18/learn-pytorch/"/>
    <id>http://yoursite.com/2019/09/18/learn-pytorch/</id>
    <published>2019-09-18T08:19:47.000Z</published>
    <updated>2019-10-10T03:34:28.238Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-利用Variable自动求导"><a href="#一-利用Variable自动求导" class="headerlink" title="一 利用Variable自动求导"></a>一 利用Variable自动求导</h1><h2 id="1-1-Variable"><a href="#1-1-Variable" class="headerlink" title="1.1 Variable"></a>1.1 Variable</h2><h3 id="1-1-1-定义"><a href="#1-1-1-定义" class="headerlink" title="1.1.1 定义"></a>1.1.1 定义</h3><p>&emsp;&emsp;在pytorch中，我们需要能够构建计算图的 tensor，这就是 Variable数据结构。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身.data，对应 tensor 的梯度.grad以及这个 Variable 是通过什么方式得到的.grad_fn。<br><a id="more"></a></p><h3 id="1-1-2-特性"><a href="#1-1-2-特性" class="headerlink" title="1.1.2 特性"></a>1.1.2 特性</h3><ul><li><p>requires_grad<br>变量可以有梯度，求导。</p></li><li><p>volatile<br>主要以用于inference过程中。若是某个过程，从 x 开始 都只需做预测，不需反传梯度的话，那么只需设置x.volatile=True ,那么 x 以后的运算过程的输出均为 volatile==True ,即 requires_grad==False。虽然inference 过程不必backward(),所以requires_grad 的值为False 或 True，对结果是没有影响的，但是对程序的运算效率有直接影响；所以使用volatile=True ,就不必把运算过程中所有参数都手动设一遍requires_grad = False 了，方便快捷。</p></li><li><p>detach</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">y = A(x)</span><br><span class="line">temp=y.detach()</span><br><span class="line">z = B(temp)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">def detach(self):</span><br><span class="line">  result = NoGrad()(self)  # this is needed, because it merges version counters</span><br><span class="line">  result._grad_fn = None</span><br><span class="line">  return result</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;如果我们有两个网络 , 两个关系是这样的  现在我们想用 来为B网络的参数来求梯度，但是又不想求A网络参数的梯度。接着我们看一下detach的源码，将grad_fn设置为None，也就是说切断了变量temp与上一个网络反向传播的途径。不知道temp如何得到的。</p><ul><li>retain_graph</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.randn((1,4),dtype=torch.float32,requires_grad=True)</span><br><span class="line">y = x ** 2</span><br><span class="line">z = y * 4</span><br><span class="line">output1 = z.mean()</span><br><span class="line">output2 = z.sum()</span><br><span class="line">output1.backward()    # 这个代码执行正常，但是执行完中间变量都free了，所以下一个出现了问题</span><br><span class="line">#正确的写法是  output1.backward(retain_graph=True) </span><br><span class="line">output2.backward()    # 这时会引发错误</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;所以retain_graph主要用于处理，计算节点数值保存了，但是计算图x-y-z-out结构被释放了的情况。</p><ul><li>create_graph<br>官方的意思是对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。默认False，目前还没看到具体的应用，看到再补上。</li></ul><h2 id="1-2-自动求导过程"><a href="#1-2-自动求导过程" class="headerlink" title="1.2 自动求导过程"></a>1.2 自动求导过程</h2><h3 id="1-2-1-输出为标量"><a href="#1-2-1-输出为标量" class="headerlink" title="1.2.1 输出为标量"></a>1.2.1 输出为标量</h3><p>首先我们看一段代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line">a=V(t.Tensor([2,3]),requires_grad=True)</span><br><span class="line">b=a+3</span><br><span class="line">c=b*3</span><br><span class="line">out=c.mean()</span><br><span class="line">out.backward()</span><br><span class="line"></span><br><span class="line">print(&quot;a.data\n&quot;,a.data)</span><br><span class="line">print(&quot;a.grad\n&quot;,a.grad)</span><br><span class="line">print(&quot;a.grad_fn\n&quot;,a.grad_fn)</span><br><span class="line">print(&quot;b.data\n&quot;,b.data)</span><br><span class="line">print(&quot;b.grad\n&quot;,b.grad)</span><br><span class="line">print(&quot;b.grad_fn\n&quot;,b.grad_fn)</span><br><span class="line">print(&quot;out.data\n&quot;,out.data)</span><br><span class="line">print(&quot;out.grad\n&quot;,out.grad)</span><br><span class="line">print(&quot;out.grad_fn\n&quot;,out.grad_fn)</span><br></pre></td></tr></table></figure></p><p>对应的函数是:<br>$$out=\frac{3\left [ \left ( a_1 + 3 \right )+\left ( a_2 + 3 \right ) \right ]}{2}=\frac{3\left [ \mathbf{a}+3 \right ]}{2}$$<br>所以\(\mathbf{a}\)向量的梯度为:<br>$$\frac{\partial out}{\partial \mathbf{a}}=(\frac{3}{2},\frac{3}{2})$$<br>真实的运行结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a.data</span><br><span class="line"> tensor([2., 3.])</span><br><span class="line">a.grad</span><br><span class="line"> tensor([1.5000, 1.5000])</span><br><span class="line">a.grad_fn</span><br><span class="line"> None</span><br><span class="line">b.data</span><br><span class="line"> tensor([5., 6.])</span><br><span class="line">b.grad</span><br><span class="line"> None</span><br><span class="line">b.grad_fn</span><br><span class="line"> &lt;AddBackward0 object at 0x000002C22F441A20&gt;</span><br><span class="line">out.data</span><br><span class="line"> tensor(16.5000)</span><br><span class="line">out.grad</span><br><span class="line"> None</span><br><span class="line">out.grad_fn</span><br><span class="line"> &lt;MeanBackward1 object at 0x000002C22F441160&gt;</span><br></pre></td></tr></table></figure></p><p>我们得到以下结论：</p><ul><li>grad_fn表示变量通过怎样的计算方式得到，叶节点变量None</li><li>grad为梯度，中间变量不存储grad，只有叶节点存储。</li><li>data为运行过程中变量的值</li></ul><h3 id="1-2-2-输出为向量"><a href="#1-2-2-输出为向量" class="headerlink" title="1.2.2 输出为向量"></a>1.2.2 输出为向量</h3><h4 id="1-2-2-1-雅可比矩阵的介绍"><a href="#1-2-2-1-雅可比矩阵的介绍" class="headerlink" title="1.2.2.1 雅可比矩阵的介绍"></a>1.2.2.1 雅可比矩阵的介绍</h4><p>&emsp;&emsp;Rn→Rm为一个从欧式n维空间转换到欧式m维空间的函数，并且由m个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn)。若将该函数的偏导数(若存在)组成一个m行n列的矩阵, 那么这个矩阵就是所谓的雅可比矩阵:<br><img src="/2019/09/18/learn-pytorch/yakebi.jpg" alt></p><h4 id="1-2-2-2-实例"><a href="#1-2-2-2-实例" class="headerlink" title="1.2.2.2 实例"></a>1.2.2.2 实例</h4><p>首先我们再看一段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line">a=V(t.Tensor([[2,4]]),requires_grad=True)</span><br><span class="line">b=t.zeros(1,2)</span><br><span class="line">b[0,0]=a[0,0]**2+a[0,1]</span><br><span class="line">b[0,1]=a[0,1]**3+a[0,0]</span><br><span class="line">out=2*b</span><br><span class="line">out.backward(t.Tensor([[1,1]]))</span><br><span class="line">#backward参数与out维度相同</span><br><span class="line"></span><br><span class="line">print(&quot;a.data\n&quot;,a.data)</span><br><span class="line">print(&quot;a.grad\n&quot;,a.grad)</span><br><span class="line">print(&quot;a.grad_fn\n&quot;,a.grad_fn)</span><br><span class="line">print(&quot;b.data\n&quot;,b.data)</span><br><span class="line">print(&quot;b.grad\n&quot;,b.grad)</span><br><span class="line">print(&quot;b.grad_fn\n&quot;,b.grad_fn)</span><br><span class="line">print(&quot;out.data\n&quot;,out.data)</span><br><span class="line">print(&quot;out.grad\n&quot;,out.grad)</span><br><span class="line">print(&quot;out.grad_fn\n&quot;,out.grad_fn)</span><br></pre></td></tr></table></figure><p>对应的函数为：<br>$$\mathbf{out}=2\mathbf{b}=2\left ( \left (a_1\right )^2+a_2,\left (a_2\right )^3+a_1 \right)$$<br>我们求得\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵为：<br>\begin{pmatrix}<br>\frac{\partial out_1}{\partial a_1}=4a_1 &amp; \frac{\partial out_1}{\partial a_2}=2 \\<br>\frac{\partial out_2}{\partial a_1}=2&amp; \frac{\partial out_2}{\partial a_2}=6{a_2}^2<br>\end{pmatrix}<br>也就是：<br>\begin{pmatrix}<br>8 &amp; 2\\<br> 2&amp;96<br>\end{pmatrix}<br>真实的运行结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a.data</span><br><span class="line"> tensor([[2., 4.]])</span><br><span class="line">a.grad</span><br><span class="line"> tensor([[10., 98.]])</span><br><span class="line">a.grad_fn</span><br><span class="line"> None</span><br><span class="line">b.data</span><br><span class="line"> tensor([[ 8., 66.]])</span><br><span class="line">b.grad</span><br><span class="line"> None</span><br><span class="line">b.grad_fn</span><br><span class="line"> &lt;CopySlices object at 0x0000020221827F60&gt;</span><br><span class="line">out.data</span><br><span class="line"> tensor([[ 16., 132.]])</span><br><span class="line">out.grad</span><br><span class="line"> None</span><br><span class="line">out.grad_fn</span><br><span class="line"> &lt;MulBackward0 object at 0x0000020221827C50&gt;</span><br></pre></td></tr></table></figure><p>我们得到以下结论：</p><ul><li>\(\mathbf{a}\)向量的梯度由\(\mathbf{out}\)对\(\mathbf{a}\)雅可比矩阵得出。也就是\( a.grad=(\frac{\partial out_1}{\partial a_1}+\frac{\partial out_2}{\partial a_1}=10 , \frac{\partial out_1}{\partial a_2}+\frac{\partial out_2}{\partial a_2}=98)\),那么这是为什么呢？与backward输入的[[1,1]]参数有何联系？接下来看下一节。</li></ul><h3 id="1-2-3-输出为矩阵：backward（）参数的意义"><a href="#1-2-3-输出为矩阵：backward（）参数的意义" class="headerlink" title="1.2.3 输出为矩阵：backward（）参数的意义"></a>1.2.3 输出为矩阵：backward（）参数的意义</h3><h4 id="1-2-3-1-矩阵求导的介绍"><a href="#1-2-3-1-矩阵求导的介绍" class="headerlink" title="1.2.3.1 矩阵求导的介绍"></a>1.2.3.1 矩阵求导的介绍</h4><p>&emsp;&emsp;基础是利用矩阵微分，复杂结合链式法则。矩阵微分和矩阵的迹有很大的关系，矩阵论中有详细的描述。<br><code>矩阵微分的介绍:https:/www.cnblogs.compinardp/10791506.html</code><br><code>矩阵求导常用公式：https://blog.csdn.net/WPR1991/article/details/82929843</code></p><h4 id="1-2-3-2-实例"><a href="#1-2-3-2-实例" class="headerlink" title="1.2.3.2 实例"></a>1.2.3.2 实例</h4><p>首先看一段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line">x=t.Tensor([[100,200],[101,201]])</span><br><span class="line">w=V(t.Tensor([[0.1],[0.2]]),requires_grad=True)</span><br><span class="line">b=V(t.Tensor([[0.01],[0.02]]),requires_grad=True)</span><br><span class="line">y=t.Tensor([[1],[0]])</span><br><span class="line">out=(t.mm(x,w)+b)-y</span><br><span class="line">out.backward(t.Tensor([[1],[2]]))</span><br><span class="line"></span><br><span class="line">print(&quot;w.data\n&quot;,w.data)</span><br><span class="line">print(&quot;w.grad\n&quot;,w.grad)</span><br><span class="line">print(&quot;w.grad_fn\n&quot;,w.grad_fn)</span><br><span class="line">print(&quot;b.data\n&quot;,b.data)</span><br><span class="line">print(&quot;b.grad\n&quot;,b.grad)</span><br><span class="line">print(&quot;b.grad_fn\n&quot;,b.grad_fn)</span><br><span class="line">print(&quot;out.data\n&quot;,out.data)</span><br><span class="line">print(&quot;out.grad\n&quot;,out.grad)</span><br><span class="line">print(&quot;out.grad_fn\n&quot;,out.grad_fn)</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;我们模拟一个超级简单的网络\(\mathbf{out} ={\left (\mathbf{X}\mathbf{W}+\mathbf{B}\right )}-\mathbf{Y}  \),全部为矩阵。\( \mathbf{X} \)为我们输入的有2个属性的2组数据，\(\mathbf{Y}\)为2组数据的标签，所以这两个只需要Tensor封装，不需要Variable封装，\(\mathbf{out}\)为差损失。\(\mathbf{W}\)与\(\mathbf{B}\)为随机初始化的参数，需要计算梯度并通过梯度下降等方法变化，代码中没有写优化算法，我们只想看看一次反向传播后变量的梯度是多少。理论上可以得出：<br>$$\frac{\partial \mathbf{out}}{\partial \mathbf{W}} = \mathbf{X} =<br>\begin{pmatrix}<br>100 &amp; 200\\<br>101 &amp; 201<br>\end{pmatrix}$$<br>$$\frac{\partial \mathbf{out}}{\partial \mathbf{B}} = \mathbf{I} =<br>\begin{pmatrix}<br>1 &amp; 0\\<br>0 &amp; 1<br>\end{pmatrix}$$</p><p>我们看一下运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">w.data</span><br><span class="line"> tensor([[0.1000],</span><br><span class="line">        [0.2000]])</span><br><span class="line">w.grad</span><br><span class="line"> tensor([[302.],</span><br><span class="line">        [602.]])</span><br><span class="line">w.grad_fn</span><br><span class="line"> None</span><br><span class="line">b.data</span><br><span class="line"> tensor([[0.0100],</span><br><span class="line">        [0.0200]])</span><br><span class="line">b.grad</span><br><span class="line"> tensor([[1.],</span><br><span class="line">        [2.]])</span><br><span class="line">b.grad_fn</span><br><span class="line"> None</span><br><span class="line">out.data</span><br><span class="line"> tensor([[49.0100],</span><br><span class="line">        [50.3200]])</span><br><span class="line">out.grad</span><br><span class="line"> None</span><br><span class="line">out.grad_fn</span><br><span class="line"> &lt;SubBackward0 object at 0x000001E4B0857C18&gt;</span><br></pre></td></tr></table></figure><p>可以看到\(\mathbf{W}\)与\(\mathbf{B}\)的梯度是理论值与backward参数运算的结果.<br>\(\mathbf{W}\)的梯度为<br>\begin{pmatrix}<br>100+101\times 2 \\<br>200+201\times 2<br>\end{pmatrix}<br>也就是<br>\begin{pmatrix}<br>302 \\<br>602<br>\end{pmatrix}<br>\(\mathbf{B}\)和\(\mathbf{W}\)计算方式相同。<br>我们可以得出以下的结论：</p><ul><li>backward的参数的实际意义是：当每个变量在反向传播计算梯度时候，不同的样本将给与不同的权重影响。这个权重就是backward的参数，当参数全部为1，表示所有的样本都一样，意味着拿到一个mini-batch的所有数据平均梯度。backward的参数与y的维度相同。</li></ul><h3 id="1-2-4-利用自动求导找函数的最小值"><a href="#1-2-4-利用自动求导找函数的最小值" class="headerlink" title="1.2.4 利用自动求导找函数的最小值"></a>1.2.4 利用自动求导找函数的最小值</h3><p><code>原来的文章https://blog.csdn.net/weixin_42892943/article/details/94716387</code></p><p>首先我们看一段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch as t </span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line"></span><br><span class="line">def fun(x):</span><br><span class="line">    return (x[0]**2+x[1]-11)**2+(x[0]+x[1]**2-7)**2</span><br><span class="line"></span><br><span class="line">x = V(t.Tensor([0.,0.]),requires_grad=True)</span><br><span class="line">optimizer = t.optim.Adam([x],lr=1e-3)</span><br><span class="line">for step in range(20000):</span><br><span class="line">    pred = fun(x)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    if step%2000 == 0:</span><br><span class="line">        print(&apos;step&#123;&#125;: x = &#123;&#125;, x.grad = &#123;&#125;, f(x) = &#123;&#125;&apos;.format(step,x.toli(),x.grad,pred.item()))</span><br></pre></td></tr></table></figure><p>我们构建了一个函数：<br>$$z=(x^2+y-11)^2+(x+y^2-7)^2$$<br>其图像为：<br><img src="/2019/09/18/learn-pytorch/fun.png" alt><br>我们利用adam算法优化器寻找函数的最小值。结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">step0: x = [0.0009999999310821295, 0.0009999999310821295], x.grad = tensor([-14., -22.]), f(x) = 170.0</span><br><span class="line">step2000: x = [2.3331806659698486, 1.9540692567825317], x.grad = tensor([-35.3487, -13.8643]), f(x) = 13.730920791625977</span><br><span class="line">step4000: x = [2.9820079803466797, 2.0270984172821045], x.grad = tensor([-0.7803,  0.5791]), f(x) = 0.014858869835734367</span><br><span class="line">step6000: x = [2.999983549118042, 2.0000221729278564], x.grad = tensor([-0.0008,  0.0004]), f(x) = 1.1074007488787174e-08</span><br><span class="line">step8000: x = [2.9999938011169434, 2.0000083446502686], x.grad = tensor([-0.0003,  0.0002]), f(x) = 1.5572823031106964e-09</span><br><span class="line">step10000: x = [2.999997854232788, 2.000002861022949], x.grad = tensor([-9.5367e-05,  5.7221e-05]), f(x) = 1.8189894035458565e-10</span><br><span class="line">step12000: x = [2.9999992847442627, 2.0000009536743164], x.grad = tensor([-2.8610e-05,  1.7166e-05]), f(x) = 1.6370904631912708e-11</span><br><span class="line">step14000: x = [2.999999761581421, 2.000000238418579], x.grad = tensor([-9.5367e-06,  5.7220e-06]), f(x) = 1.8189894035458565e-12</span><br><span class="line">step16000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0</span><br><span class="line">step18000: x = [3.0, 2.0], x.grad = tensor([0., 0.]), f(x) = 0.0</span><br></pre></td></tr></table></figure><p>我们可以看到x逐渐接近最低点，梯度在不断的减小。</p><h1 id="二-数据集预处理的自定义构建"><a href="#二-数据集预处理的自定义构建" class="headerlink" title="二 数据集预处理的自定义构建"></a>二 数据集预处理的自定义构建</h1><h2 id="2-1-transforms中的源码"><a href="#2-1-transforms中的源码" class="headerlink" title="2.1 transforms中的源码"></a>2.1 transforms中的源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Normalize(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, mean, std, inplace=False):</span><br><span class="line">        self.mean = mean</span><br><span class="line">        self.std = std</span><br><span class="line">        self.inplace = inplace</span><br><span class="line"></span><br><span class="line">    def __call__(self, tensor):</span><br><span class="line">        return F.normalize(tensor, self.mean, self.std, self.inplace)</span><br><span class="line"></span><br><span class="line">    def __repr__(self):</span><br><span class="line">        return self.__class__.__name__ + &apos;(mean=&#123;0&#125;, std=&#123;1&#125;)&apos;.format(self.mean, self.std)</span><br></pre></td></tr></table></figure><p>以Normalize为例子，我们看到每个处理方式由<strong>init</strong>，<strong>call</strong>，<strong>repr</strong>组成。</p><h2 id="2-2-自定义自己的预处理方式"><a href="#2-2-自定义自己的预处理方式" class="headerlink" title="2.2 自定义自己的预处理方式"></a>2.2 自定义自己的预处理方式</h2><p>&emsp;&emsp;torchvision.transforms的方法都是随机的，如果样本和标签都是图片，需要转动相同的随机角度。需要自己来定义预处理方式。这时候可以借用torchvision.transforms.functional来构造自己的函数，同时处理样本标签，得到新的样本标签后再分开处理。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms.functional as TF</span><br><span class="line">import torchvision.transforms as T </span><br><span class="line"></span><br><span class="line">#自定义旋转角度：方法一，二</span><br><span class="line">class FixedRotation(object):</span><br><span class="line">    def __init__(self, startangle,endangle):</span><br><span class="line">        self.startangle = startangle</span><br><span class="line">        self.endangle = endangle</span><br><span class="line"></span><br><span class="line">    def __call__(self,img):</span><br><span class="line">        return  my_transforms1(img,startangle,endangle)</span><br><span class="line">        # return  my_transforms2(img,startangle,endangle)</span><br><span class="line"></span><br><span class="line">    def __repr__(self):</span><br><span class="line">    return self.__class__.__name__ +&apos;from&#123;&#125;to&#123;&#125;&apos;.format(startangle,endangle)</span><br><span class="line"></span><br><span class="line">def my_transforms1(img, startangle,endangle):</span><br><span class="line">    angle = random.randint(startangle,endangle)</span><br><span class="line">    image = img.rotate(angles[angle])</span><br><span class="line">    return image</span><br><span class="line"></span><br><span class="line">def my_transforms2(image):</span><br><span class="line">    angle = random.randint(-30, 30)</span><br><span class="line">    image = TF.rotate(image, angle)</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure><h2 id="2-3-自定义一个数据集预处理"><a href="#2-3-自定义一个数据集预处理" class="headerlink" title="2.3 自定义一个数据集预处理"></a>2.3 自定义一个数据集预处理</h2><h3 id="2-3-1-torch-utils-data-Dataset"><a href="#2-3-1-torch-utils-data-Dataset" class="headerlink" title="2.3.1 torch.utils.data.Dataset"></a>2.3.1 torch.utils.data.Dataset</h3><p>自定义一个数据集需要继承类torch.utils.data.Dataset。类中主要方法有3个：</p><ul><li><p><code>__init__(self,path)</code>该方法用来初始化类和对数据进行加载,数据的加载就是针对不同的数据读入到内存中。</p></li><li><p><code>__getitem__(self, index)</code>该方法是把读入的输出传给PyTorch（迭代器的方式）。</p></li><li><p><code>__len__(self)</code>该方法是数据大小，迭代一次数据的大小。</p></li></ul><h3 id="2-3-2-torch-utils-data-DataLoader"><a href="#2-3-2-torch-utils-data-DataLoader" class="headerlink" title="2.3.2 torch.utils.data.DataLoader()"></a>2.3.2 torch.utils.data.DataLoader()</h3><p>torch.utils.data.DataLoader类主要使用torch.utils.data.sampler实现，sampler是所有采样器的基础类，提供了迭代器的迭代（<strong>iter</strong>）和长度（<strong>len</strong>）接口实现，同时sampler也是通过索引对数据进行洗牌(shuffle)等操作。因此，如果DataLoader不适用于你的数据，需要重新设计数据的分批次，可以充分使用所提供的smapler。</p><ul><li>batch-size。样本每个batch的大小，默认为1。</li><li>shuffle。是否打乱数据，默认为False。</li><li>sampler。定义一个方法来绘制样本数据，如果定义该方法，则不能使用shuffle。</li><li>num_workers。数据分为几批处理（对于大数据）。</li><li>collate_fn。整理数据，把每个batch数据整理为tensor。（一般使用默认调用default_collate(batch)）。</li><li>pin_memory。将获取的数据张量放在固定的内存中，从而能够更快地将数据传输到支持cuda的gpu</li><li>drop_last。用于处理最后一个batch的数据。因为最后一个可能不能够被整除，如果设置为True，则舍弃最后一个，为False则保留最后一个，但是最后一个可能很小。</li></ul><p>主要讲一下sampler与collate_fn.</p><h1 id="三-神经网络的自定义构建"><a href="#三-神经网络的自定义构建" class="headerlink" title="三 神经网络的自定义构建"></a>三 神经网络的自定义构建</h1><h2 id="3-1-详解nn-module"><a href="#3-1-详解nn-module" class="headerlink" title="3.1 详解nn.module"></a>3.1 详解nn.module</h2><p>&emsp;&emsp;这里先看一下nn.module类源码中重要的属性方法，这里放一段代码，等下下面会用到，我们建立了一个前传后的钩子函数，一个有纯参数组成的module，一个由submodule组成的module，并用这两个module组成个复杂的大module。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import torch as t </span><br><span class="line">from torch import nn </span><br><span class="line">from torch.nn import functional as F</span><br><span class="line">from torch.autograd import Variable as V</span><br><span class="line"></span><br><span class="line">def for_hook(module, input, output):</span><br><span class="line">    print(module)</span><br><span class="line">    for val in input:</span><br><span class="line">        print(&quot;input val:&quot;,val)</span><br><span class="line">    for out_val in output:</span><br><span class="line">        print(&quot;output val:&quot;, out_val)</span><br><span class="line"></span><br><span class="line">class Linear(nn.Module): # 继承nn.Module</span><br><span class="line">    def __init__(self, in_features, out_features):</span><br><span class="line">        super(Linear, self).__init__() # 等价于nn.Module.__init__(self)</span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x.mm(self.w) # x.@(self.w)</span><br><span class="line">        return x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line">class MyLinear(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyLinear, self).__init__()</span><br><span class="line">        self.subsubmodule=Linear(3,3)</span><br><span class="line">        self.subsubmodule.register_forward_hook(for_hook)</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.subsubmodule(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.register_buffer(&quot;buf1&quot;,t.ones(3,3)) #为module注册一个缓存区</span><br><span class="line"></span><br><span class="line">        self.register_parameter(&quot;param1&quot;,nn.Parameter(t.ones(3,3))) #与下一句等价，注册一个参数，本质就是一个变量</span><br><span class="line">        self.param2 = nn.Parameter(t.rand(3, 3))</span><br><span class="line"></span><br><span class="line">        self.add_module(&quot;submodel1&quot;,nn.Linear(3, 3)) #与下一句等价，注册一个子module</span><br><span class="line">        self.submodel2 = nn.Linear(3, 3) </span><br><span class="line"></span><br><span class="line">        self.submodel3 = MyLinear()</span><br><span class="line">        self.modulelist1 = nn.ModuleList([nn.Linear(3,3),nn.Linear(3,3)])</span><br><span class="line">        self.modulelist2 = nn.Sequential(nn.Linear(3,3),nn.Linear(3,3))</span><br><span class="line">        self.bn = nn.BatchNorm1d(3)</span><br><span class="line">    def forward(self, input):</span><br><span class="line">        x = input.mm(self.param1)</span><br><span class="line">        x = self.submodel1(x)</span><br><span class="line">        x = self.submodel3(x)</span><br><span class="line">        # x = self.modulelist1(x) error</span><br><span class="line">        x = self.modulelist2(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        return x</span><br><span class="line">        </span><br><span class="line">net = Net()</span><br><span class="line">x=t.rand(2,3)</span><br><span class="line">y=net(x)</span><br></pre></td></tr></table></figure></p><h3 id="3-1-1-属性"><a href="#3-1-1-属性" class="headerlink" title="3.1.1 属性"></a>3.1.1 属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Module(object):</span><br><span class="line">def __init__(self):</span><br><span class="line">    self._parameters = OrderedDict()</span><br><span class="line">    self._modules = OrderedDict()</span><br><span class="line">    self._buffers = OrderedDict()</span><br><span class="line">    self._backward_hooks = OrderedDict()</span><br><span class="line">    self._forward_hooks = OrderedDict()</span><br><span class="line">    self.training = True</span><br><span class="line">    self._forward_pre_hooks = OrderedDict()</span><br><span class="line">    self._state_dict_hooks = OrderedDict()</span><br><span class="line">    self._load_state_dict_pre_hooks = OrderedDict()</span><br></pre></td></tr></table></figure><p>解释一下重要的几个属性，我把它分为三类。</p><ul><li>用于存储数据的：<ul><li>_parameters：字典，保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为’param’，value为对应parameter的item。不会查看到子module的参数。</li><li>_modules：子module，通过self.submodel = nn.Linear(3, 4)指定的子module会保存于此。同时子moudle可以自己定义。</li><li>_buffers：缓存，每个moudle都可以注册自己的缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。</li></ul></li><li>用于是否继续向前传播的:<ul><li>training：通过判断training值来决定正向传播策略。</li></ul></li><li>钩子函数来实现对前传,后传,保存,回复等操作的触发,_backward_hooks,_forward_hooks,_forward_pre_hooks，_state_dict_hooks，_load_state_dict_pre_hooks这些字典主要用于存储钩子。</li></ul><p>我们看几个属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">net._buffers:</span><br><span class="line"> OrderedDict([(&apos;buf1&apos;, tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]]))])</span><br><span class="line"></span><br><span class="line">net._modules:</span><br><span class="line"> OrderedDict([(&apos;submodule1&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule2&apos;, Linear(in_features=3, out_features=3, bias=True)), (&apos;submodule3&apos;, MyLinear(</span><br><span class="line">  (subsubmodule): Linear()</span><br><span class="line">)), (&apos;modulelist1&apos;, ModuleList(</span><br><span class="line">  (0): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">  (1): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">)), (&apos;modulelist2&apos;, Sequential(</span><br><span class="line">  (0): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">  (1): Linear(in_features=3, out_features=3, bias=True)</span><br><span class="line">)), (&apos;bn&apos;, BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))])</span><br><span class="line"></span><br><span class="line">net._parameters:</span><br><span class="line"> OrderedDict([(&apos;param1&apos;, Parameter containing:</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], requires_grad=True)), (&apos;param2&apos;, Parameter containing:</span><br><span class="line">tensor([[0.3535, 0.6803, 0.7144],</span><br><span class="line">        [0.2985, 0.1329, 0.2111],</span><br><span class="line">        [0.3999, 0.0395, 0.1407]], requires_grad=True))])</span><br><span class="line"></span><br><span class="line">net.training:</span><br><span class="line"> True</span><br><span class="line"></span><br><span class="line">net.submodule3.subsubmodule._forward_hooks:</span><br><span class="line"> OrderedDict([(0, &lt;function for_hook at 0x0000028C74F23E18&gt;)])</span><br></pre></td></tr></table></figure></p><h3 id="3-1-2-方法"><a href="#3-1-2-方法" class="headerlink" title="3.1.2 方法"></a>3.1.2 方法</h3><p>根据属性我们来分开介绍一些重要的nn.moudle的重要方法。</p><h4 id="3-1-2-1-缓存，子模型，参数的设置，操作，查看的方法"><a href="#3-1-2-1-缓存，子模型，参数的设置，操作，查看的方法" class="headerlink" title="3.1.2.1 缓存，子模型，参数的设置，操作，查看的方法"></a>3.1.2.1 缓存，子模型，参数的设置，操作，查看的方法</h4><ul><li>self.register_buffer（name[string],buf[Tensor]）</li><li>self.register_parameter（name[string],param[nn.Parameter]）等价与self.name=param</li><li>self.add_module（name[string],submodule[nn.Module]） 等价于self.name=submodule</li><li>self.named_parameters() 生成器，产生所有参数的name与param</li><li>self.named_buffers() 生成器，产生所有参数的name与buf</li><li>self.named_module() 生成器，产生所有参数的name与module，包括module与子module</li><li>self.named_children() 生成器，产生所有参数的name与submodule</li><li>self.cuda() 转移到cuda上</li><li>self.cpu() 转移到cpu上</li><li>self.float() 参书变换</li><li><p>self.to()  多态性</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.. function:: to(device=None, dtype=None, non_blocking=False)</span><br><span class="line"></span><br><span class="line">.. function:: to(dtype, non_blocking=False)</span><br><span class="line"></span><br><span class="line">.. function:: to(tensor, non_blocking=False)</span><br></pre></td></tr></table></figure></li><li><p>self.apply(fn) 源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def apply(self, fn):</span><br><span class="line">    for module in self.children():</span><br><span class="line">        module.apply(fn)</span><br><span class="line">    fn(self)</span><br><span class="line">    return self</span><br></pre></td></tr></table></figure></li></ul><p>我们看一些方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">net.named_parameters()所有参数名称:</span><br><span class="line">param1</span><br><span class="line">param2</span><br><span class="line">submodule1.weight</span><br><span class="line">submodule1.bias</span><br><span class="line">submodule2.weight</span><br><span class="line">submodule2.bias</span><br><span class="line">submodule3.subsubmodule.w</span><br><span class="line">submodule3.subsubmodule.b</span><br><span class="line">modulelist1.0.weight</span><br><span class="line">modulelist1.0.bias</span><br><span class="line">modulelist1.1.weight</span><br><span class="line">modulelist1.1.bias</span><br><span class="line">modulelist2.0.weight</span><br><span class="line">modulelist2.0.bias</span><br><span class="line">modulelist2.1.weight</span><br><span class="line">modulelist2.1.bias</span><br><span class="line">bn.weight</span><br><span class="line">bn.bias</span><br><span class="line"></span><br><span class="line">net.named_modules()所有模型名称:</span><br><span class="line">submodule1</span><br><span class="line">submodule2</span><br><span class="line">submodule3</span><br><span class="line">submodule3.subsubmodule</span><br><span class="line">modulelist1</span><br><span class="line">modulelist1.0</span><br><span class="line">modulelist1.1</span><br><span class="line">modulelist2</span><br><span class="line">modulelist2.0</span><br><span class="line">modulelist2.1</span><br><span class="line">bn</span><br><span class="line"></span><br><span class="line">net.named_children()所有子模型名称:</span><br><span class="line">submodule1</span><br><span class="line">submodule2</span><br><span class="line">submodule3</span><br><span class="line">modulelist1</span><br><span class="line">modulelist2</span><br><span class="line">bn</span><br><span class="line"></span><br><span class="line">net.named_buffers()所有缓存名称:</span><br><span class="line">buf1</span><br><span class="line">bn.running_mean</span><br><span class="line">bn.running_var</span><br><span class="line">bn.num_batches_tracked</span><br></pre></td></tr></table></figure><h4 id="3-2-1-2-传播的方法"><a href="#3-2-1-2-传播的方法" class="headerlink" title="3.2.1.2 传播的方法"></a>3.2.1.2 传播的方法</h4><ul><li>self.train() #使用Dropout与BN层训练时开启</li><li>self.eval() #使用Dropout与BN层测试时开启</li><li>self.zero_grad() #清空所有参数的梯度</li></ul><h4 id="3-2-1-3-钩子的方法"><a href="#3-2-1-3-钩子的方法" class="headerlink" title="3.2.1.3 钩子的方法"></a>3.2.1.3 钩子的方法</h4><ul><li>self.register_forward_hook(hook) 设立该module前传后的钩子</li><li>self.register_backward_hook(hook) 设立该module后传后的钩子</li><li>self.register_forward_pre_hook(hook) 设立该module前传前的钩子</li></ul><p>正向传播时候当钩子的函数监控的module发生正向传播，触发钩子函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Linear()</span><br><span class="line">input val: tensor([[ 0.3910, -0.9092, -0.9559],</span><br><span class="line">        [ 0.4495, -0.9752, -1.0354]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line">output val: tensor([ 1.3830,  1.3145, -0.7691], grad_fn=&lt;SelectBackward&gt;)</span><br><span class="line">output val: tensor([ 1.5223,  1.4328, -0.7066], grad_fn=&lt;SelectBackward&gt;)</span><br></pre></td></tr></table></figure><h2 id="3-3-自定义functional"><a href="#3-3-自定义functional" class="headerlink" title="3.3 自定义functional"></a>3.3 自定义functional</h2><p>其实整个网络都是依靠与基础的functional，functional实现了卷积，损失等等一系列功能，那么我们怎么自己写一个functional呢？其实pytorch官方给出了拓展方式，用numpy与scipy（实现卷积等操作封装度高于numpy）即可，如果你的操作这些库依旧不能满足，那么需要定制C++底层，这里我们先不学习，主要学习使用python下的定制functional。<br><code>https://www.cnblogs.com/hellcat/p/8453615.html</code><br><code>https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html</code></p><h3 id="3-3-1-官方源码的解析"><a href="#3-3-1-官方源码的解析" class="headerlink" title="3.3.1 官方源码的解析"></a>3.3.1 官方源码的解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class BadFFTFunction(Function):</span><br><span class="line">    @staticmethod</span><br><span class="line">    def forward(ctx, input):</span><br><span class="line">        numpy_input = input.detach().numpy() </span><br><span class="line">        result = abs(rfft2(numpy_input))</span><br><span class="line">        return input.new(result)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def backward(ctx, grad_output):</span><br><span class="line">        numpy_go = grad_output.numpy()</span><br><span class="line">        result = irfft2(numpy_go)</span><br><span class="line">        return grad_output.new(result)</span><br><span class="line"></span><br><span class="line">def incorrect_fft(input):</span><br><span class="line">    return BadFFTFunction.apply(input)</span><br></pre></td></tr></table></figure><p>这段代码是官方源码,我们认真看一下：这段代码没有数学意义，仅仅是为了演示过程，我们看到forword与backword两个过程。</p><ul><li>forword过程输入input，输出output，而backword过程输入grad_output，输出grad_input。</li><li>forword输入的参数量（除去ctx）等于backward输出的参数量，forword输出的参数量等于backward输入的参数量（除去ctx）。</li><li>通常我们的grad_input是\( \frac{\partial output}{\partial intput} \)与grad_output运算的结果,grad_output由人工输入，或者上一层传递而来，意义其实是不同的样本对与梯度下降的影响程度。</li><li>此外ctx本质是一个缓存区，用于正向传播的缓存在反向传播时候使用。</li></ul><h3 id="3-3-2-定制的函数"><a href="#3-3-2-定制的函数" class="headerlink" title="3.3.2 定制的函数"></a>3.3.2 定制的函数</h3><p>接下来我们尝试写一个超级简单的网络。Y=XW+b,假设X维度（5，10），W维度（10，1），b维度（5，1），Y的维度（5，1）。意思是我们有一个数据集，包含5个样本，每个样本对应一个标签，每个样本10个属性。我们想要用W，b来拟合X的属性与标签Y的关系。forward很简单，backward则需要手工计算梯度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class Test2Function(Function):</span><br><span class="line">    @staticmethod</span><br><span class="line">    def forward(ctx,input,w,b):</span><br><span class="line">        numpy_input = input.detach().numpy() </span><br><span class="line">        numpy_w = w.detach().numpy() </span><br><span class="line">        numpy_b = b.detach().numpy() </span><br><span class="line">        result = np.dot(numpy_input,numpy_w)+numpy_b</span><br><span class="line">        ctx.save_for_backward(input,w,b)</span><br><span class="line">        return input.new(result)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def backward(ctx, grad_output):</span><br><span class="line">        #此时的grad_output是backword()输入的参数</span><br><span class="line">        input,w,b= ctx.saved_tensors</span><br><span class="line">        #这里我进行了手工求导，具体可在演算纸上进行</span><br><span class="line">        grad_input = np.dot( grad_output.detach().numpy(),w.detach().numpy().T)</span><br><span class="line">        grad_w =  np.dot(input.detach().numpy().T,grad_output.detach().numpy())</span><br><span class="line">        grad_b = grad_output.detach().numpy()</span><br><span class="line">        return t.from_numpy(grad_input),t.from_numpy(grad_w),t.from_numpy(grad_b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TestMoudle(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(TestMoudle, self).__init__()</span><br><span class="line">        self.w = nn.Parameter(t.ones(10,1))</span><br><span class="line">        self.b= nn.Parameter(t.ones(5, 1))</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        return Test2Function.apply(input, self.w, self.b)</span><br></pre></td></tr></table></figure></p><p>如果是loss函数的定制，在backward过程中直接返回grad_output，或者不写backward，会继承父类的backward。</p><h2 id="3-4-需要辨析注意的点"><a href="#3-4-需要辨析注意的点" class="headerlink" title="3.4 需要辨析注意的点"></a>3.4 需要辨析注意的点</h2><h3 id="3-4-1-nn-Parameter与Variable的关系"><a href="#3-4-1-nn-Parameter与Variable的关系" class="headerlink" title="3.4.1 nn.Parameter与Variable的关系"></a>3.4.1 nn.Parameter与Variable的关系</h3><p>nn.Parameter是Variable的子类，用于在nn.module中设置参数变量。</p><h3 id="3-4-2-nn-ModuleList与nn-Sequential-区别"><a href="#3-4-2-nn-ModuleList与nn-Sequential-区别" class="headerlink" title="3.4.2 nn.ModuleList与nn.Sequential()区别"></a>3.4.2 nn.ModuleList与nn.Sequential()区别</h3><p>nn.ModuleList和nn.Sequential()都可以在nn.module初始化时候设置网络层，也都可以被有关数据存储的方法，属性所识别，唯一不同的是nn.Sequential()具有<strong>call</strong>()功能，forward过程中直接处理上一层。此外如果在nn.module初始化时用了列表来包含一系列网络层，这些网络层将不会被有关数据存储的方法，属性所识别。</p><h3 id="3-4-3-何时使用from-torch-nn-import-functional-as-F中的F"><a href="#3-4-3-何时使用from-torch-nn-import-functional-as-F中的F" class="headerlink" title="3.4.3 何时使用from torch.nn import functional as F中的F"></a>3.4.3 何时使用from torch.nn import functional as F中的F</h3><p>F通常是无参数的，所以我们不必在nn.module初始化时候声明，而在forward中直接使用。这样写的原因主要是让我们更加方便整理代码。</p><h1 id="四-多GPU应用与部署（未完待续）"><a href="#四-多GPU应用与部署（未完待续）" class="headerlink" title="四 多GPU应用与部署（未完待续）"></a>四 多GPU应用与部署（未完待续）</h1><p>因为我只有一个gpu，没机会试试。这个是中文官网的代码应该问题不大。<br><code>https://www.pytorchtutorial.com/pytorch-large-batches-multi-gpu-and-distributed-training/</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from parallel import DataParallelModel, DataParallelCriterion</span><br><span class="line"> </span><br><span class="line">parallel_model = DataParallelModel(model)             # 并行化model</span><br><span class="line">parallel_loss  = DataParallelCriterion(loss_function) # 并行化损失函数</span><br><span class="line"> </span><br><span class="line">predictions = parallel_model(inputs)      # 并行前向计算</span><br><span class="line">                                          # &quot;predictions&quot;是多个gpu的结果的元组</span><br><span class="line">loss = parallel_loss(predictions, labels) # 并行计算损失函数</span><br><span class="line">loss.backward()                           # 计算梯度</span><br><span class="line">optimizer.step()                          # 反向传播</span><br><span class="line">predictions = parallel_model(inputs)</span><br></pre></td></tr></table></figure><h1 id="五-可视化"><a href="#五-可视化" class="headerlink" title="五 可视化"></a>五 可视化</h1><h2 id="5-1-Visdom"><a href="#5-1-Visdom" class="headerlink" title="5.1 Visdom"></a>5.1 Visdom</h2><p>之前一直在用的可视化工具，后来发现定制图片需要开会员。现在决定要放弃了。<br><code>https://ptorch.com/news/77.html</code></p><h2 id="5-2-Tensorboardx"><a href="#5-2-Tensorboardx" class="headerlink" title="5.2 Tensorboardx"></a>5.2 Tensorboardx</h2><p>支持一下tensorboardX<br><code>https://blog.csdn.net/wsp_1138886114/article/details/87602112#TensorBoardX_120</code></p><h2 id="5-3-怎么画网络结构"><a href="#5-3-怎么画网络结构" class="headerlink" title="5.3 怎么画网络结构"></a>5.3 怎么画网络结构</h2><p>如果你想实时查看自己的网络模型可以用tensorboardx。<br>如果想画一个精美的图建议ppt，此外plotneuralnet<br><code>https://www.pytorchtutorial.com/plotneuralnet/</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一-利用Variable自动求导&quot;&gt;&lt;a href=&quot;#一-利用Variable自动求导&quot; class=&quot;headerlink&quot; title=&quot;一 利用Variable自动求导&quot;&gt;&lt;/a&gt;一 利用Variable自动求导&lt;/h1&gt;&lt;h2 id=&quot;1-1-Variable&quot;&gt;&lt;a href=&quot;#1-1-Variable&quot; class=&quot;headerlink&quot; title=&quot;1.1 Variable&quot;&gt;&lt;/a&gt;1.1 Variable&lt;/h2&gt;&lt;h3 id=&quot;1-1-1-定义&quot;&gt;&lt;a href=&quot;#1-1-1-定义&quot; class=&quot;headerlink&quot; title=&quot;1.1.1 定义&quot;&gt;&lt;/a&gt;1.1.1 定义&lt;/h3&gt;&lt;p&gt;&amp;emsp;&amp;emsp;在pytorch中，我们需要能够构建计算图的 tensor，这就是 Variable数据结构。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身.data，对应 tensor 的梯度.grad以及这个 Variable 是通过什么方式得到的.grad_fn。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="pytorch进阶学习" scheme="http://yoursite.com/categories/pytorch%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>基本操作二：写作技巧</title>
    <link href="http://yoursite.com/2019/08/24/second2/"/>
    <id>http://yoursite.com/2019/08/24/second2/</id>
    <published>2019-08-24T08:37:06.000Z</published>
    <updated>2019-08-24T10:55:41.788Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><p>###有关符号编译出错<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123; -&gt; &amp;<span class="comment">#123; — 大括号左边部分Left curly brace</span></span><br><span class="line">&#125; -&gt; &amp;<span class="comment">#125; — 大括号右边部分Right curly brace</span></span><br><span class="line">空格 -&gt; &amp;ensp;</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>###有关图片<br>在blog（hexo）目录下Git Bash Here，运行hexo n “博客名”来生成md博客时，会在_post目录下看到一个与博客同名的文件夹。按照如下格式则可以插入图片<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![你想要输入的替代文字](second2/test.jpg)</span><br></pre></td></tr></table></figure></p><p>测试效果图：<br><img src="/2019/08/24/second2/test.jpg" alt="你想要输入的替代文字"></p><p>###有关音乐<br>在网易云音乐中生成外链音乐播放器。如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=<span class="string">"no"</span> border=<span class="string">"0"</span> marginwidth=<span class="string">"0"</span> marginheight=<span class="string">"0"</span> width=330 height=86 src=<span class="string">"//music.163.com/outchain/player?type=2&amp;id=541326593&amp;auto=1&amp;height=66"</span>&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure></p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=541326593&auto=1&height=66"></iframe>  <p>###有关视频<br>在优酷中生成外链视频播放器。如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=<span class="string">"0"</span> src=<span class="string">"https://v.qq.com/txp/iframe/player.html?vid=i0031n20390"</span> allowFullScreen=<span class="string">"true"</span>&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure></p><iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=i0031n20390" allowfullscreen="true"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写作&quot;&gt;&lt;a href=&quot;#写作&quot; class=&quot;headerlink&quot; title=&quot;写作&quot;&gt;&lt;/a&gt;写作&lt;/h2&gt;&lt;p&gt;###有关符号编译出错&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; -&amp;gt; &amp;amp;&lt;span class=&quot;comment&quot;&gt;#123; — 大括号左边部分Left curly brace&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125; -&amp;gt; &amp;amp;&lt;span class=&quot;comment&quot;&gt;#125; — 大括号右边部分Right curly brace&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;空格 -&amp;gt; &amp;amp;ensp;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客操作" scheme="http://yoursite.com/categories/hexo%E5%8D%9A%E5%AE%A2%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="博客" scheme="http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>基本操作一：部署命令</title>
    <link href="http://yoursite.com/2018/12/02/hello-world/"/>
    <id>http://yoursite.com/2018/12/02/hello-world/</id>
    <published>2018-12-02T12:49:01.616Z</published>
    <updated>2019-08-24T08:33:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a><br><a id="more"></a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files-and-compress"><a href="#Generate-static-files-and-compress" class="headerlink" title="Generate static files and compress"></a>Generate static files and compress</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ gulp</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a><br><!--more--></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/writing.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Writing&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客操作" scheme="http://yoursite.com/categories/hexo%E5%8D%9A%E5%AE%A2%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="博客" scheme="http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
</feed>
